{"cells":[{"cell_type":"markdown","metadata":{"id":"peUcVKtbGPfD"},"source":["# LSTM Classification Model\n","\n","This notebook demonstrates how to load data, preprocess it, define an LSTM model, train the model, and evaluate its performance. The data is assumed to be in CSV format and stored in a directory.\n","\n","## Setup\n","\n","First, we need to install the necessary libraries. Run the following cell to install them."]},{"cell_type":"code","execution_count":69,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14646,"status":"ok","timestamp":1730218056629,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"aAZ2n50rGS-C","outputId":"664f5275-bf30-43d7-ce5b-92249c92a4e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n","Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n"]}],"source":["%pip install torch torchvision torchaudio\n","%pip install pandas scikit-learn\n","%pip install wandb onnx -Uq\n","%pip install joblib"]},{"cell_type":"markdown","metadata":{"id":"l4d3KxhxGML8"},"source":["## Import Libraries and seed\n","Import the necessary libraries for data processing, model building, training, and evaluation. Adding a seed ensures reproducibility by making sure that the random number generation is consistent across different runs."]},{"cell_type":"code","execution_count":127,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":268,"status":"ok","timestamp":1730220620264,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"9a9HvzrNG8iw","outputId":"08ae1cb2-d4ec-4905-8209-757004580d96"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}],"source":["import os\n","import random\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import joblib\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","import wandb\n","\n","def set_seed(seed):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    random.seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":118940,"status":"ok","timestamp":1730215008290,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"SUQtUPxSBzq4","outputId":"bbc2e051-b049-49ce-c0d8-10244408de45"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"elapsed":16861,"status":"ok","timestamp":1730215025147,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"AoCyZSTt7qqS","outputId":"456be2dc-9a25-4994-ca99-aa280d659ab2"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["wandb.login()\n","#94b4debef3cc9601df4d91995649548f8ab3a097"]},{"cell_type":"markdown","metadata":{"id":"S1-6Xv6LHHmZ"},"source":["## Load Data from Github Repository\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1730215025148,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"eI7hrAaVHL44"},"outputs":[],"source":["## Remove PIC-PAPER-01 folder:\n","!rm -rf PIC-PAPER-01\n","\n","# # Download Github Repo (Private) https://stackoverflow.com/questions/74532852/clone-github-repo-with-fine-grained-token/78280453#78280453\n","# !git clone --no-checkout https://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@github.com/danimp94/PIC-PAPER-01.git\n","\n","# # To clone data folder only:\n","# %cd PIC-PAPER-01 # Navigate to the repository directory\n","# !git sparse-checkout init --cone # Initialize sparse-checkout\n","# !git sparse-checkout set data # Set the sparse-checkout to include only the data/ folder\n","# !git checkout # Checkout the specified folder"]},{"cell_type":"code","execution_count":71,"metadata":{"executionInfo":{"elapsed":260,"status":"ok","timestamp":1730218066180,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"hAY8BfVX9XCK"},"outputs":[],"source":["def load_data_from_directory(input_path):\n","    data_frames = []\n","    for file in os.listdir(input_path):\n","        if file.endswith('.csv'):\n","            df = pd.read_csv(os.path.join(input_path, file), delimiter=';', header=0)\n","            data_frames.append(df)\n","    data = pd.concat(data_frames, ignore_index=True)\n","\n","    print(data)\n","    print(data.shape)\n","\n","    return data"]},{"cell_type":"markdown","metadata":{"id":"wiCTw-qcKXhn"},"source":["## Preprocessing Data\n","Define a function to preprocess the data. This includes encoding categorical labels and standardizing the features."]},{"cell_type":"code","execution_count":72,"metadata":{"executionInfo":{"elapsed":294,"status":"ok","timestamp":1730218068569,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"zJoZqi5LDPl8"},"outputs":[],"source":["def calculate_averages_and_dispersion(data, data_percentage):\n","    df = data\n","    results = []\n","    for (sample, freq), group in df.groupby(['Sample', 'Frequency (GHz)']):\n","        window_size = max(1, int(len(group) * data_percentage / 100))\n","        # print(f\"Processing sample: {sample}, frequency: {freq} with window size: {window_size}\")\n","        for start in range(0, len(group), window_size):\n","            window_data = group.iloc[start:start + window_size]\n","            mean_values = window_data[['LG (mV)', 'HG (mV)']].mean()\n","            std_deviation_values = window_data[['LG (mV)', 'HG (mV)']].std()\n","            results.append({\n","                'Frequency (GHz)': freq,\n","                'LG (mV) mean': mean_values['LG (mV)'],\n","                'HG (mV) mean': mean_values['HG (mV)'],\n","                'LG (mV) std deviation': std_deviation_values['LG (mV)'],\n","                'HG (mV) std deviation': std_deviation_values['HG (mV)'],\n","                'Thickness (mm)': window_data['Thickness (mm)'].iloc[0],\n","                'Sample': sample,\n","            })\n","    results_df = pd.DataFrame(results)\n","    # results_df.to_csv(output_file, sep=';', index=False)\n","    # print(f\"Processed {input_file} and saved to {output_file}\")\n","    print(results_df)\n","    return results_df"]},{"cell_type":"code","execution_count":136,"metadata":{"executionInfo":{"elapsed":258,"status":"ok","timestamp":1730220832304,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"KZPdMwkkKmm7"},"outputs":[],"source":["def preprocess_data(data, data_percentage):\n","    # Windowing the data\n","    data = calculate_averages_and_dispersion(data, data_percentage)\n","    print(data.shape)\n","\n","    # Assuming the last column is the target\n","    X = data.iloc[:, :-1].values\n","    y = data.iloc[:, -1].values\n","\n","    # Encode the target variable if it's categorical\n","    if y.dtype == 'object':\n","        le = LabelEncoder()\n","        y = le.fit_transform(y)\n","\n","    # le is the fitted LabelEncoder\n","    joblib.dump(le, 'label_encoder.pkl')\n","\n","    # Get the original labels and their encoded values\n","    original_labels = le.classes_\n","    encoded_values = le.transform(original_labels)\n","\n","    # Create a DataFrame to display the mapping\n","    label_mapping_df = pd.DataFrame({\n","        'Original Label': original_labels,\n","        'Encoded Value': encoded_values\n","    })\n","\n","    # Display the DataFrame\n","    print(label_mapping_df)\n","\n","    # Standardize the features\n","    scaler = StandardScaler()\n","    X = scaler.fit_transform(X)\n","\n","    # Convert to PyTorch tensors\n","    X = torch.tensor(X, dtype=torch.float32)\n","    y = torch.tensor(y, dtype=torch.long)\n","\n","    return X, y"]},{"cell_type":"code","execution_count":137,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46246,"status":"ok","timestamp":1730220881747,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"UTXvxCzu6MBA","outputId":"369f2efd-7d61-42a3-e209-ff381f6df97d"},"outputs":[{"output_type":"stream","name":"stdout","text":["        Sample  Frequency (GHz)     LG (mV)    HG (mV)  Thickness (mm)\n","0           A1            100.0   -7.080942  -0.854611             0.2\n","1           A1            100.0   67.024785   0.244141             0.2\n","2           A1            100.0  124.893178  -1.098776             0.2\n","3           A1            100.0   91.075571   0.000000             0.2\n","4           A1            100.0   48.956174   0.122094             0.2\n","...        ...              ...         ...        ...             ...\n","2737958    REF            600.0    0.366256  16.237333             0.0\n","2737959    REF            600.0    0.000000  -7.080942             0.0\n","2737960    REF            600.0   -0.244170  15.260652             0.0\n","2737961    REF            600.0    0.366256  20.021975             0.0\n","2737962    REF            600.0    0.122085  13.185203             0.0\n","\n","[2737963 rows x 5 columns]\n","(2737963, 5)\n","       Frequency (GHz)  LG (mV) mean  HG (mV) mean  LG (mV) std deviation  \\\n","0                100.0     54.879155     -0.022198              29.958659   \n","1                100.0     54.511665      0.048093              28.096155   \n","2                100.0     55.099894     -0.118380              26.833871   \n","3                100.0     48.387674      0.103588              28.843498   \n","4                100.0     49.932853     -0.071525              23.093397   \n","...                ...           ...           ...                    ...   \n","24271            600.0     -0.006143     10.237498               0.890999   \n","24272            600.0     -0.006910     10.949278               0.858148   \n","24273            600.0      0.029178     10.547702               0.842082   \n","24274            600.0      0.065266     10.051683               0.891632   \n","24275            600.0     -0.228910      9.766817               0.856929   \n","\n","       HG (mV) std deviation  Thickness (mm) Sample  \n","0                   0.644211             0.2     A1  \n","1                   0.704742             0.2     A1  \n","2                   0.755493             0.2     A1  \n","3                   0.854655             0.2     A1  \n","4                   0.650231             0.2     A1  \n","...                      ...             ...    ...  \n","24271               8.205224             0.0    REF  \n","24272               8.679982             0.0    REF  \n","24273               8.802972             0.0    REF  \n","24274               8.365464             0.0    REF  \n","24275               9.725526             0.0    REF  \n","\n","[24276 rows x 7 columns]\n","(24276, 7)\n","   Original Label  Encoded Value\n","0              A1              0\n","1              B1              1\n","2              C1              2\n","3              D1              3\n","4              E1              4\n","5              E2              5\n","6              E3              6\n","7              F1              7\n","8              G1              8\n","9              H1              9\n","10             I1             10\n","11             J1             11\n","12             K1             12\n","13             L1             13\n","14             M1             14\n","15             N1             15\n","16            REF             16\n"]}],"source":["input_path = '/content/drive/MyDrive/PhD/Colab Notebooks/training_data/'\n","data = load_data_from_directory(input_path)\n","\n","# Load and preprocess data\n","X, y = preprocess_data(data, data_percentage=3.7) # 1s window size\n","\n","\n","# print(le.classes_)"]},{"cell_type":"markdown","metadata":{"id":"BZjha9Cx6MBB"},"source":["## Config"]},{"cell_type":"code","execution_count":75,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":267,"status":"ok","timestamp":1730218139404,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"oLRhwd8cKKJH","outputId":"dbd79215-ba30-4e61-fc13-d2540a83c4ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'epochs': 100, 'seed': 40, 'classes': 17, 'k_folds': 4, 'batch_size': 128, 'learning_rate': 0.001, 'dataset': 'experiment_1', 'architecture': 'LSTM', 'hidden_dim': 64}\n"]}],"source":["config = dict(\n","    epochs=100,\n","    seed = 40,\n","    classes = data['Sample'].nunique(), # Each different sample is a different class\n","    k_folds = 4,  # Number of folds for cross-validation\n","    batch_size=128,\n","    learning_rate=0.001,\n","    dataset=\"experiment_1\",\n","    architecture=\"LSTM\",\n","    hidden_dim = 64\n",")\n","\n","print(config)"]},{"cell_type":"markdown","metadata":{"id":"RI96M_V6KpyH"},"source":["## Define Model\n","Define the LSTM model architecture"]},{"cell_type":"code","execution_count":76,"metadata":{"executionInfo":{"elapsed":283,"status":"ok","timestamp":1730218141600,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"h6j-_U_RKxlR"},"outputs":[],"source":["class LSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(LSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n","        self.dropout = nn.Dropout(p=0.2)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.dropout(out[:, -1, :])\n","        out = self.fc(out)\n","        return out\n"]},{"cell_type":"markdown","metadata":{"id":"afvxx1KzKzYp"},"source":["## Train Model\n","Define a function to train the model"]},{"cell_type":"code","execution_count":77,"metadata":{"executionInfo":{"elapsed":310,"status":"ok","timestamp":1730218144030,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"-3a9IuRVK3Wq"},"outputs":[],"source":["def train_model(model, train_loader, val_loader, criterion, optimizer, device, config):\n","    num_epochs = config.epochs\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        for X_batch, y_batch in train_loader:\n","              X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","\n","              outputs = model(X_batch)\n","              loss = criterion(outputs, y_batch)\n","\n","              optimizer.zero_grad()\n","              loss.backward()\n","              optimizer.step()\n","\n","              running_loss += loss.item()\n","\n","        val_loss = 0.0\n","        model.eval()\n","        with torch.no_grad():\n","            for X_batch, y_batch in val_loader:\n","                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","                outputs = model(X_batch)\n","                loss = criterion(outputs, y_batch)\n","                val_loss += loss.item()\n","\n","        # Log metrics to W&B\n","        wandb.log({\"epoch\": epoch, \"train_loss\": running_loss / len(train_loader), \"val_loss\": val_loss / len(val_loader)})\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"G1bDpP8JK5jp"},"source":["## Evaluate Model\n"]},{"cell_type":"code","execution_count":78,"metadata":{"executionInfo":{"elapsed":302,"status":"ok","timestamp":1730218145852,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"TIDNXJGRK9af"},"outputs":[],"source":["def evaluate_model(model, test_loader, device):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for X_batch, y_batch in test_loader:\n","            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","            outputs = model(X_batch)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += y_batch.size(0)\n","            correct += (predicted == y_batch).sum().item()\n","\n","    print(f'Accuracy of the model on the test set: {100 * correct / total:.2f}%')"]},{"cell_type":"code","execution_count":79,"metadata":{"executionInfo":{"elapsed":269,"status":"ok","timestamp":1730218148309,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"KBOELYWD6MBB"},"outputs":[],"source":["def make(config, X, y):\n","    # K-Fold Cross-Validation\n","    kfold = KFold(n_splits=config.k_folds, shuffle=True, random_state=config.seed)\n","\n","    for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n","        print(f'Fold {fold+1}/{config.k_folds}')\n","\n","        # Create DataLoader for training and validation sets\n","        X_train, X_val = X[train_idx], X[val_idx]\n","        y_train, y_val = y[train_idx], y[val_idx]\n","\n","        # Convert data to tensors and add sequence length dimension\n","        X_train = torch.tensor(X_train).float().unsqueeze(1)\n","        X_val = torch.tensor(X_val).float().unsqueeze(1)\n","        y_train = torch.tensor(y_train).long()\n","        y_val = torch.tensor(y_val).long()\n","\n","        train_dataset = TensorDataset(X_train, y_train)\n","        val_dataset = TensorDataset(X_val, y_val)\n","        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n","        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n","\n","        # Initialize the model, loss function, and optimizer\n","        input_dim = X_train.shape[2]\n","        hidden_dim = config.hidden_dim\n","        output_dim = config.classes\n","        model = LSTMModel(input_dim, hidden_dim, output_dim).to(device)\n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n","\n","        yield model, train_loader, val_loader, criterion, optimizer"]},{"cell_type":"code","execution_count":80,"metadata":{"executionInfo":{"elapsed":261,"status":"ok","timestamp":1730218149937,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"SlnfXqu-6MBB"},"outputs":[],"source":["def model_pipeline(hyperparameters):\n","    with wandb.init(project=\"PIC-PAPER-01-exp-1\", config=hyperparameters):\n","        config = wandb.config\n","\n","        # Set seed for reproducibility\n","        set_seed(config.seed)\n","\n","        # Split the data into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=config.seed)\n","\n","        # K-Fold Cross-Validation\n","        for model, train_loader, val_loader, criterion, optimizer in make(config, X_train, y_train):\n","            print(model)\n","\n","            # Train the model\n","            train_model(model, train_loader, val_loader, criterion, optimizer, device, config)\n","\n","            # Evaluate the model on the validation set\n","            evaluate_model(model, val_loader, device)\n","\n","        # Evaluate the final model on the test set\n","        X_test = torch.tensor(X_test).float().unsqueeze(1)  # Add sequence length dimension\n","        y_test = torch.tensor(y_test).long()\n","        test_dataset = TensorDataset(X_test, y_test)\n","        test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n","        evaluate_model(model, test_loader, device)\n","\n","    return model\n"]},{"cell_type":"markdown","metadata":{"id":"WEtZPi2fLQGL"},"source":["## Run Training"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":438359,"status":"ok","timestamp":1730215513418,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"tScgXvxA6MBC","outputId":"d2178619-e711-4843-d5b9-6c61f339f6f8"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanimp94\u001b[0m (\u001b[33mdanimp94-university-carlos-iii-of-madrid\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.18.5"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20241029_151758-0ui9c9wh</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-exp-1/runs/0ui9c9wh' target=\"_blank\">honest-meadow-23</a></strong> to <a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-exp-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-exp-1' target=\"_blank\">https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-exp-1</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-exp-1/runs/0ui9c9wh' target=\"_blank\">https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-exp-1/runs/0ui9c9wh</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Fold 1/5\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-14-50cf231ae070>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  X_train = torch.tensor(X_train).float().unsqueeze(1)\n","<ipython-input-14-50cf231ae070>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  X_val = torch.tensor(X_val).float().unsqueeze(1)\n","<ipython-input-14-50cf231ae070>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train = torch.tensor(y_train).long()\n","<ipython-input-14-50cf231ae070>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_val = torch.tensor(y_val).long()\n"]},{"output_type":"stream","name":"stdout","text":["LSTMModel(\n","  (lstm): LSTM(6, 64, batch_first=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n","  (fc): Linear(in_features=64, out_features=17, bias=True)\n",")\n","Epoch [1/100], Train Loss: 2.7165, Val Loss: 2.4939\n","Epoch [2/100], Train Loss: 2.3578, Val Loss: 2.2630\n","Epoch [3/100], Train Loss: 2.2268, Val Loss: 2.1543\n","Epoch [4/100], Train Loss: 2.0991, Val Loss: 1.9984\n","Epoch [5/100], Train Loss: 1.9357, Val Loss: 1.8339\n","Epoch [6/100], Train Loss: 1.7893, Val Loss: 1.7010\n","Epoch [7/100], Train Loss: 1.6713, Val Loss: 1.5944\n","Epoch [8/100], Train Loss: 1.5775, Val Loss: 1.5061\n","Epoch [9/100], Train Loss: 1.4983, Val Loss: 1.4322\n","Epoch [10/100], Train Loss: 1.4295, Val Loss: 1.3656\n","Epoch [11/100], Train Loss: 1.3674, Val Loss: 1.3079\n","Epoch [12/100], Train Loss: 1.3132, Val Loss: 1.2539\n","Epoch [13/100], Train Loss: 1.2678, Val Loss: 1.2077\n","Epoch [14/100], Train Loss: 1.2218, Val Loss: 1.1647\n","Epoch [15/100], Train Loss: 1.1808, Val Loss: 1.1277\n","Epoch [16/100], Train Loss: 1.1422, Val Loss: 1.0913\n","Epoch [17/100], Train Loss: 1.1082, Val Loss: 1.0559\n","Epoch [18/100], Train Loss: 1.0767, Val Loss: 1.0267\n","Epoch [19/100], Train Loss: 1.0474, Val Loss: 0.9959\n","Epoch [20/100], Train Loss: 1.0201, Val Loss: 0.9685\n","Epoch [21/100], Train Loss: 0.9928, Val Loss: 0.9440\n","Epoch [22/100], Train Loss: 0.9659, Val Loss: 0.9207\n","Epoch [23/100], Train Loss: 0.9448, Val Loss: 0.8992\n","Epoch [24/100], Train Loss: 0.9240, Val Loss: 0.8773\n","Epoch [25/100], Train Loss: 0.9046, Val Loss: 0.8584\n","Epoch [26/100], Train Loss: 0.8853, Val Loss: 0.8383\n","Epoch [27/100], Train Loss: 0.8636, Val Loss: 0.8202\n","Epoch [28/100], Train Loss: 0.8484, Val Loss: 0.8035\n","Epoch [29/100], Train Loss: 0.8316, Val Loss: 0.7881\n","Epoch [30/100], Train Loss: 0.8172, Val Loss: 0.7731\n","Epoch [31/100], Train Loss: 0.8004, Val Loss: 0.7587\n","Epoch [32/100], Train Loss: 0.7860, Val Loss: 0.7441\n","Epoch [33/100], Train Loss: 0.7727, Val Loss: 0.7308\n","Epoch [34/100], Train Loss: 0.7588, Val Loss: 0.7184\n","Epoch [35/100], Train Loss: 0.7484, Val Loss: 0.7072\n","Epoch [36/100], Train Loss: 0.7353, Val Loss: 0.6968\n","Epoch [37/100], Train Loss: 0.7249, Val Loss: 0.6831\n","Epoch [38/100], Train Loss: 0.7117, Val Loss: 0.6736\n","Epoch [39/100], Train Loss: 0.7034, Val Loss: 0.6630\n","Epoch [40/100], Train Loss: 0.6929, Val Loss: 0.6533\n","Epoch [41/100], Train Loss: 0.6831, Val Loss: 0.6437\n","Epoch [42/100], Train Loss: 0.6746, Val Loss: 0.6348\n","Epoch [43/100], Train Loss: 0.6650, Val Loss: 0.6261\n","Epoch [44/100], Train Loss: 0.6577, Val Loss: 0.6172\n","Epoch [45/100], Train Loss: 0.6482, Val Loss: 0.6094\n","Epoch [46/100], Train Loss: 0.6412, Val Loss: 0.6017\n","Epoch [47/100], Train Loss: 0.6343, Val Loss: 0.5940\n","Epoch [48/100], Train Loss: 0.6238, Val Loss: 0.5864\n","Epoch [49/100], Train Loss: 0.6189, Val Loss: 0.5807\n","Epoch [50/100], Train Loss: 0.6117, Val Loss: 0.5740\n","Epoch [51/100], Train Loss: 0.6050, Val Loss: 0.5686\n","Epoch [52/100], Train Loss: 0.5980, Val Loss: 0.5617\n","Epoch [53/100], Train Loss: 0.5925, Val Loss: 0.5558\n","Epoch [54/100], Train Loss: 0.5866, Val Loss: 0.5497\n","Epoch [55/100], Train Loss: 0.5808, Val Loss: 0.5442\n","Epoch [56/100], Train Loss: 0.5760, Val Loss: 0.5386\n","Epoch [57/100], Train Loss: 0.5703, Val Loss: 0.5346\n","Epoch [58/100], Train Loss: 0.5651, Val Loss: 0.5285\n","Epoch [59/100], Train Loss: 0.5607, Val Loss: 0.5246\n","Epoch [60/100], Train Loss: 0.5552, Val Loss: 0.5200\n","Epoch [61/100], Train Loss: 0.5498, Val Loss: 0.5152\n","Epoch [62/100], Train Loss: 0.5454, Val Loss: 0.5112\n","Epoch [63/100], Train Loss: 0.5427, Val Loss: 0.5071\n","Epoch [64/100], Train Loss: 0.5368, Val Loss: 0.5019\n","Epoch [65/100], Train Loss: 0.5336, Val Loss: 0.4991\n","Epoch [66/100], Train Loss: 0.5295, Val Loss: 0.4964\n","Epoch [67/100], Train Loss: 0.5244, Val Loss: 0.4929\n","Epoch [68/100], Train Loss: 0.5220, Val Loss: 0.4889\n","Epoch [69/100], Train Loss: 0.5183, Val Loss: 0.4851\n","Epoch [70/100], Train Loss: 0.5128, Val Loss: 0.4811\n","Epoch [71/100], Train Loss: 0.5100, Val Loss: 0.4784\n","Epoch [72/100], Train Loss: 0.5093, Val Loss: 0.4743\n","Epoch [73/100], Train Loss: 0.5056, Val Loss: 0.4726\n","Epoch [74/100], Train Loss: 0.5025, Val Loss: 0.4695\n","Epoch [75/100], Train Loss: 0.4990, Val Loss: 0.4670\n","Epoch [76/100], Train Loss: 0.4974, Val Loss: 0.4649\n","Epoch [77/100], Train Loss: 0.4941, Val Loss: 0.4616\n","Epoch [78/100], Train Loss: 0.4915, Val Loss: 0.4605\n","Epoch [79/100], Train Loss: 0.4882, Val Loss: 0.4567\n","Epoch [80/100], Train Loss: 0.4874, Val Loss: 0.4563\n","Epoch [81/100], Train Loss: 0.4843, Val Loss: 0.4535\n","Epoch [82/100], Train Loss: 0.4815, Val Loss: 0.4510\n","Epoch [83/100], Train Loss: 0.4805, Val Loss: 0.4484\n","Epoch [84/100], Train Loss: 0.4759, Val Loss: 0.4477\n","Epoch [85/100], Train Loss: 0.4752, Val Loss: 0.4451\n","Epoch [86/100], Train Loss: 0.4743, Val Loss: 0.4430\n","Epoch [87/100], Train Loss: 0.4714, Val Loss: 0.4418\n","Epoch [88/100], Train Loss: 0.4681, Val Loss: 0.4395\n","Epoch [89/100], Train Loss: 0.4678, Val Loss: 0.4376\n","Epoch [90/100], Train Loss: 0.4651, Val Loss: 0.4364\n","Epoch [91/100], Train Loss: 0.4637, Val Loss: 0.4345\n","Epoch [92/100], Train Loss: 0.4608, Val Loss: 0.4326\n","Epoch [93/100], Train Loss: 0.4596, Val Loss: 0.4328\n","Epoch [94/100], Train Loss: 0.4595, Val Loss: 0.4306\n","Epoch [95/100], Train Loss: 0.4555, Val Loss: 0.4295\n","Epoch [96/100], Train Loss: 0.4565, Val Loss: 0.4280\n","Epoch [97/100], Train Loss: 0.4541, Val Loss: 0.4263\n","Epoch [98/100], Train Loss: 0.4543, Val Loss: 0.4261\n","Epoch [99/100], Train Loss: 0.4517, Val Loss: 0.4235\n","Epoch [100/100], Train Loss: 0.4521, Val Loss: 0.4233\n","Accuracy of the model on the test set: 77.71%\n","Fold 2/5\n","LSTMModel(\n","  (lstm): LSTM(6, 64, batch_first=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n","  (fc): Linear(in_features=64, out_features=17, bias=True)\n",")\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-14-50cf231ae070>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  X_train = torch.tensor(X_train).float().unsqueeze(1)\n","<ipython-input-14-50cf231ae070>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  X_val = torch.tensor(X_val).float().unsqueeze(1)\n","<ipython-input-14-50cf231ae070>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_train = torch.tensor(y_train).long()\n","<ipython-input-14-50cf231ae070>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_val = torch.tensor(y_val).long()\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100], Train Loss: 2.7291, Val Loss: 2.5225\n","Epoch [2/100], Train Loss: 2.3592, Val Loss: 2.2838\n","Epoch [3/100], Train Loss: 2.2194, Val Loss: 2.1711\n","Epoch [4/100], Train Loss: 2.0869, Val Loss: 2.0123\n","Epoch [5/100], Train Loss: 1.9227, Val Loss: 1.8502\n","Epoch [6/100], Train Loss: 1.7750, Val Loss: 1.7181\n","Epoch [7/100], Train Loss: 1.6584, Val Loss: 1.6099\n","Epoch [8/100], Train Loss: 1.5639, Val Loss: 1.5204\n","Epoch [9/100], Train Loss: 1.4837, Val Loss: 1.4473\n","Epoch [10/100], Train Loss: 1.4175, Val Loss: 1.3849\n","Epoch [11/100], Train Loss: 1.3581, Val Loss: 1.3276\n","Epoch [12/100], Train Loss: 1.3078, Val Loss: 1.2781\n","Epoch [13/100], Train Loss: 1.2597, Val Loss: 1.2305\n","Epoch [14/100], Train Loss: 1.2189, Val Loss: 1.1894\n","Epoch [15/100], Train Loss: 1.1787, Val Loss: 1.1519\n","Epoch [16/100], Train Loss: 1.1437, Val Loss: 1.1188\n","Epoch [17/100], Train Loss: 1.1076, Val Loss: 1.0877\n","Epoch [18/100], Train Loss: 1.0785, Val Loss: 1.0544\n","Epoch [19/100], Train Loss: 1.0475, Val Loss: 1.0270\n","Epoch [20/100], Train Loss: 1.0215, Val Loss: 1.0002\n","Epoch [21/100], Train Loss: 0.9952, Val Loss: 0.9762\n","Epoch [22/100], Train Loss: 0.9727, Val Loss: 0.9537\n","Epoch [23/100], Train Loss: 0.9536, Val Loss: 0.9312\n","Epoch [24/100], Train Loss: 0.9316, Val Loss: 0.9130\n","Epoch [25/100], Train Loss: 0.9115, Val Loss: 0.8958\n","Epoch [26/100], Train Loss: 0.8950, Val Loss: 0.8754\n","Epoch [27/100], Train Loss: 0.8760, Val Loss: 0.8602\n","Epoch [28/100], Train Loss: 0.8600, Val Loss: 0.8429\n","Epoch [29/100], Train Loss: 0.8445, Val Loss: 0.8266\n","Epoch [30/100], Train Loss: 0.8286, Val Loss: 0.8146\n","Epoch [31/100], Train Loss: 0.8145, Val Loss: 0.7991\n","Epoch [32/100], Train Loss: 0.8001, Val Loss: 0.7872\n","Epoch [33/100], Train Loss: 0.7878, Val Loss: 0.7736\n","Epoch [34/100], Train Loss: 0.7750, Val Loss: 0.7606\n","Epoch [35/100], Train Loss: 0.7635, Val Loss: 0.7487\n","Epoch [36/100], Train Loss: 0.7505, Val Loss: 0.7381\n","Epoch [37/100], Train Loss: 0.7411, Val Loss: 0.7271\n","Epoch [38/100], Train Loss: 0.7290, Val Loss: 0.7157\n","Epoch [39/100], Train Loss: 0.7188, Val Loss: 0.7079\n","Epoch [40/100], Train Loss: 0.7093, Val Loss: 0.6948\n","Epoch [41/100], Train Loss: 0.6998, Val Loss: 0.6888\n","Epoch [42/100], Train Loss: 0.6895, Val Loss: 0.6783\n","Epoch [43/100], Train Loss: 0.6818, Val Loss: 0.6667\n","Epoch [44/100], Train Loss: 0.6727, Val Loss: 0.6602\n","Epoch [45/100], Train Loss: 0.6647, Val Loss: 0.6509\n","Epoch [46/100], Train Loss: 0.6561, Val Loss: 0.6445\n","Epoch [47/100], Train Loss: 0.6478, Val Loss: 0.6359\n","Epoch [48/100], Train Loss: 0.6391, Val Loss: 0.6278\n","Epoch [49/100], Train Loss: 0.6326, Val Loss: 0.6223\n","Epoch [50/100], Train Loss: 0.6254, Val Loss: 0.6137\n","Epoch [51/100], Train Loss: 0.6192, Val Loss: 0.6089\n","Epoch [52/100], Train Loss: 0.6132, Val Loss: 0.6031\n","Epoch [53/100], Train Loss: 0.6075, Val Loss: 0.5954\n","Epoch [54/100], Train Loss: 0.6012, Val Loss: 0.5892\n","Epoch [55/100], Train Loss: 0.5949, Val Loss: 0.5831\n","Epoch [56/100], Train Loss: 0.5898, Val Loss: 0.5807\n","Epoch [57/100], Train Loss: 0.5836, Val Loss: 0.5710\n","Epoch [58/100], Train Loss: 0.5795, Val Loss: 0.5675\n","Epoch [59/100], Train Loss: 0.5741, Val Loss: 0.5621\n","Epoch [60/100], Train Loss: 0.5696, Val Loss: 0.5576\n","Epoch [61/100], Train Loss: 0.5649, Val Loss: 0.5524\n","Epoch [62/100], Train Loss: 0.5592, Val Loss: 0.5499\n","Epoch [63/100], Train Loss: 0.5532, Val Loss: 0.5433\n","Epoch [64/100], Train Loss: 0.5505, Val Loss: 0.5409\n","Epoch [65/100], Train Loss: 0.5461, Val Loss: 0.5353\n","Epoch [66/100], Train Loss: 0.5418, Val Loss: 0.5357\n","Epoch [67/100], Train Loss: 0.5380, Val Loss: 0.5267\n","Epoch [68/100], Train Loss: 0.5350, Val Loss: 0.5238\n","Epoch [69/100], Train Loss: 0.5311, Val Loss: 0.5205\n","Epoch [70/100], Train Loss: 0.5285, Val Loss: 0.5178\n","Epoch [71/100], Train Loss: 0.5248, Val Loss: 0.5155\n","Epoch [72/100], Train Loss: 0.5218, Val Loss: 0.5108\n","Epoch [73/100], Train Loss: 0.5182, Val Loss: 0.5095\n","Epoch [74/100], Train Loss: 0.5165, Val Loss: 0.5057\n","Epoch [75/100], Train Loss: 0.5123, Val Loss: 0.5011\n","Epoch [76/100], Train Loss: 0.5088, Val Loss: 0.5002\n","Epoch [77/100], Train Loss: 0.5064, Val Loss: 0.4966\n","Epoch [78/100], Train Loss: 0.5038, Val Loss: 0.4933\n","Epoch [79/100], Train Loss: 0.5015, Val Loss: 0.4926\n","Epoch [80/100], Train Loss: 0.4972, Val Loss: 0.4886\n","Epoch [81/100], Train Loss: 0.4954, Val Loss: 0.4875\n","Epoch [82/100], Train Loss: 0.4928, Val Loss: 0.4846\n","Epoch [83/100], Train Loss: 0.4911, Val Loss: 0.4831\n","Epoch [84/100], Train Loss: 0.4882, Val Loss: 0.4809\n","Epoch [85/100], Train Loss: 0.4858, Val Loss: 0.4771\n","Epoch [86/100], Train Loss: 0.4833, Val Loss: 0.4760\n","Epoch [87/100], Train Loss: 0.4815, Val Loss: 0.4732\n","Epoch [88/100], Train Loss: 0.4797, Val Loss: 0.4715\n","Epoch [89/100], Train Loss: 0.4784, Val Loss: 0.4680\n","Epoch [90/100], Train Loss: 0.4755, Val Loss: 0.4691\n","Epoch [91/100], Train Loss: 0.4736, Val Loss: 0.4678\n","Epoch [92/100], Train Loss: 0.4729, Val Loss: 0.4664\n","Epoch [93/100], Train Loss: 0.4707, Val Loss: 0.4630\n","Epoch [94/100], Train Loss: 0.4680, Val Loss: 0.4611\n","Epoch [95/100], Train Loss: 0.4673, Val Loss: 0.4594\n","Epoch [96/100], Train Loss: 0.4663, Val Loss: 0.4578\n","Epoch [97/100], Train Loss: 0.4645, Val Loss: 0.4573\n","Epoch [98/100], Train Loss: 0.4613, Val Loss: 0.4542\n","Epoch [99/100], Train Loss: 0.4607, Val Loss: 0.4531\n","Epoch [100/100], Train Loss: 0.4602, Val Loss: 0.4510\n","Accuracy of the model on the test set: 75.95%\n","Fold 3/5\n","LSTMModel(\n","  (lstm): LSTM(6, 64, batch_first=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n","  (fc): Linear(in_features=64, out_features=17, bias=True)\n",")\n","Epoch [1/100], Train Loss: 2.7164, Val Loss: 2.5055\n","Epoch [2/100], Train Loss: 2.3603, Val Loss: 2.2744\n","Epoch [3/100], Train Loss: 2.2285, Val Loss: 2.1662\n","Epoch [4/100], Train Loss: 2.1037, Val Loss: 2.0149\n","Epoch [5/100], Train Loss: 1.9447, Val Loss: 1.8554\n","Epoch [6/100], Train Loss: 1.7976, Val Loss: 1.7224\n","Epoch [7/100], Train Loss: 1.6790, Val Loss: 1.6140\n","Epoch [8/100], Train Loss: 1.5811, Val Loss: 1.5236\n","Epoch [9/100], Train Loss: 1.5001, Val Loss: 1.4493\n","Epoch [10/100], Train Loss: 1.4321, Val Loss: 1.3831\n","Epoch [11/100], Train Loss: 1.3702, Val Loss: 1.3261\n","Epoch [12/100], Train Loss: 1.3162, Val Loss: 1.2726\n","Epoch [13/100], Train Loss: 1.2689, Val Loss: 1.2245\n","Epoch [14/100], Train Loss: 1.2254, Val Loss: 1.1831\n","Epoch [15/100], Train Loss: 1.1832, Val Loss: 1.1429\n","Epoch [16/100], Train Loss: 1.1456, Val Loss: 1.1057\n","Epoch [17/100], Train Loss: 1.1121, Val Loss: 1.0743\n","Epoch [18/100], Train Loss: 1.0806, Val Loss: 1.0421\n","Epoch [19/100], Train Loss: 1.0501, Val Loss: 1.0141\n","Epoch [20/100], Train Loss: 1.0216, Val Loss: 0.9861\n","Epoch [21/100], Train Loss: 0.9992, Val Loss: 0.9626\n","Epoch [22/100], Train Loss: 0.9731, Val Loss: 0.9382\n","Epoch [23/100], Train Loss: 0.9512, Val Loss: 0.9161\n","Epoch [24/100], Train Loss: 0.9300, Val Loss: 0.8959\n","Epoch [25/100], Train Loss: 0.9108, Val Loss: 0.8777\n","Epoch [26/100], Train Loss: 0.8909, Val Loss: 0.8592\n","Epoch [27/100], Train Loss: 0.8740, Val Loss: 0.8423\n","Epoch [28/100], Train Loss: 0.8580, Val Loss: 0.8265\n","Epoch [29/100], Train Loss: 0.8373, Val Loss: 0.8102\n","Epoch [30/100], Train Loss: 0.8241, Val Loss: 0.7933\n","Epoch [31/100], Train Loss: 0.8086, Val Loss: 0.7805\n","Epoch [32/100], Train Loss: 0.7935, Val Loss: 0.7663\n","Epoch [33/100], Train Loss: 0.7827, Val Loss: 0.7529\n","Epoch [34/100], Train Loss: 0.7684, Val Loss: 0.7414\n","Epoch [35/100], Train Loss: 0.7532, Val Loss: 0.7298\n","Epoch [36/100], Train Loss: 0.7427, Val Loss: 0.7176\n","Epoch [37/100], Train Loss: 0.7315, Val Loss: 0.7045\n","Epoch [38/100], Train Loss: 0.7218, Val Loss: 0.6949\n","Epoch [39/100], Train Loss: 0.7101, Val Loss: 0.6852\n","Epoch [40/100], Train Loss: 0.7000, Val Loss: 0.6745\n","Epoch [41/100], Train Loss: 0.6906, Val Loss: 0.6675\n","Epoch [42/100], Train Loss: 0.6812, Val Loss: 0.6578\n","Epoch [43/100], Train Loss: 0.6708, Val Loss: 0.6479\n","Epoch [44/100], Train Loss: 0.6624, Val Loss: 0.6398\n","Epoch [45/100], Train Loss: 0.6555, Val Loss: 0.6324\n","Epoch [46/100], Train Loss: 0.6474, Val Loss: 0.6233\n","Epoch [47/100], Train Loss: 0.6403, Val Loss: 0.6178\n","Epoch [48/100], Train Loss: 0.6319, Val Loss: 0.6099\n","Epoch [49/100], Train Loss: 0.6237, Val Loss: 0.6021\n","Epoch [50/100], Train Loss: 0.6176, Val Loss: 0.5948\n","Epoch [51/100], Train Loss: 0.6101, Val Loss: 0.5902\n","Epoch [52/100], Train Loss: 0.6042, Val Loss: 0.5842\n","Epoch [53/100], Train Loss: 0.5977, Val Loss: 0.5777\n","Epoch [54/100], Train Loss: 0.5941, Val Loss: 0.5693\n","Epoch [55/100], Train Loss: 0.5867, Val Loss: 0.5649\n","Epoch [56/100], Train Loss: 0.5810, Val Loss: 0.5588\n","Epoch [57/100], Train Loss: 0.5760, Val Loss: 0.5545\n","Epoch [58/100], Train Loss: 0.5705, Val Loss: 0.5497\n","Epoch [59/100], Train Loss: 0.5656, Val Loss: 0.5455\n","Epoch [60/100], Train Loss: 0.5602, Val Loss: 0.5405\n","Epoch [61/100], Train Loss: 0.5570, Val Loss: 0.5379\n","Epoch [62/100], Train Loss: 0.5518, Val Loss: 0.5317\n","Epoch [63/100], Train Loss: 0.5470, Val Loss: 0.5265\n","Epoch [64/100], Train Loss: 0.5422, Val Loss: 0.5238\n","Epoch [65/100], Train Loss: 0.5391, Val Loss: 0.5184\n","Epoch [66/100], Train Loss: 0.5340, Val Loss: 0.5148\n","Epoch [67/100], Train Loss: 0.5330, Val Loss: 0.5124\n","Epoch [68/100], Train Loss: 0.5277, Val Loss: 0.5098\n","Epoch [69/100], Train Loss: 0.5247, Val Loss: 0.5053\n","Epoch [70/100], Train Loss: 0.5185, Val Loss: 0.5040\n","Epoch [71/100], Train Loss: 0.5171, Val Loss: 0.4984\n","Epoch [72/100], Train Loss: 0.5150, Val Loss: 0.4953\n","Epoch [73/100], Train Loss: 0.5116, Val Loss: 0.4914\n","Epoch [74/100], Train Loss: 0.5071, Val Loss: 0.4888\n","Epoch [75/100], Train Loss: 0.5044, Val Loss: 0.4874\n","Epoch [76/100], Train Loss: 0.5018, Val Loss: 0.4840\n","Epoch [77/100], Train Loss: 0.5000, Val Loss: 0.4820\n","Epoch [78/100], Train Loss: 0.4969, Val Loss: 0.4803\n","Epoch [79/100], Train Loss: 0.4940, Val Loss: 0.4764\n","Epoch [80/100], Train Loss: 0.4922, Val Loss: 0.4756\n","Epoch [81/100], Train Loss: 0.4892, Val Loss: 0.4720\n","Epoch [82/100], Train Loss: 0.4866, Val Loss: 0.4693\n","Epoch [83/100], Train Loss: 0.4853, Val Loss: 0.4692\n","Epoch [84/100], Train Loss: 0.4807, Val Loss: 0.4655\n","Epoch [85/100], Train Loss: 0.4826, Val Loss: 0.4638\n","Epoch [86/100], Train Loss: 0.4811, Val Loss: 0.4609\n","Epoch [87/100], Train Loss: 0.4764, Val Loss: 0.4614\n","Epoch [88/100], Train Loss: 0.4750, Val Loss: 0.4584\n","Epoch [89/100], Train Loss: 0.4732, Val Loss: 0.4586\n","Epoch [90/100], Train Loss: 0.4725, Val Loss: 0.4566\n","Epoch [91/100], Train Loss: 0.4707, Val Loss: 0.4556\n","Epoch [92/100], Train Loss: 0.4679, Val Loss: 0.4528\n","Epoch [93/100], Train Loss: 0.4644, Val Loss: 0.4512\n","Epoch [94/100], Train Loss: 0.4647, Val Loss: 0.4483\n","Epoch [95/100], Train Loss: 0.4643, Val Loss: 0.4484\n","Epoch [96/100], Train Loss: 0.4600, Val Loss: 0.4471\n","Epoch [97/100], Train Loss: 0.4631, Val Loss: 0.4454\n","Epoch [98/100], Train Loss: 0.4587, Val Loss: 0.4438\n","Epoch [99/100], Train Loss: 0.4585, Val Loss: 0.4429\n","Epoch [100/100], Train Loss: 0.4554, Val Loss: 0.4404\n","Accuracy of the model on the test set: 77.51%\n","Fold 4/5\n","LSTMModel(\n","  (lstm): LSTM(6, 64, batch_first=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n","  (fc): Linear(in_features=64, out_features=17, bias=True)\n",")\n","Epoch [1/100], Train Loss: 2.7210, Val Loss: 2.5135\n","Epoch [2/100], Train Loss: 2.3588, Val Loss: 2.2732\n","Epoch [3/100], Train Loss: 2.2181, Val Loss: 2.1561\n","Epoch [4/100], Train Loss: 2.0807, Val Loss: 1.9963\n","Epoch [5/100], Train Loss: 1.9189, Val Loss: 1.8325\n","Epoch [6/100], Train Loss: 1.7686, Val Loss: 1.6970\n","Epoch [7/100], Train Loss: 1.6507, Val Loss: 1.5885\n","Epoch [8/100], Train Loss: 1.5562, Val Loss: 1.4995\n","Epoch [9/100], Train Loss: 1.4767, Val Loss: 1.4244\n","Epoch [10/100], Train Loss: 1.4082, Val Loss: 1.3586\n","Epoch [11/100], Train Loss: 1.3498, Val Loss: 1.3017\n","Epoch [12/100], Train Loss: 1.2970, Val Loss: 1.2502\n","Epoch [13/100], Train Loss: 1.2493, Val Loss: 1.2064\n","Epoch [14/100], Train Loss: 1.2053, Val Loss: 1.1625\n","Epoch [15/100], Train Loss: 1.1656, Val Loss: 1.1235\n","Epoch [16/100], Train Loss: 1.1280, Val Loss: 1.0877\n","Epoch [17/100], Train Loss: 1.0936, Val Loss: 1.0551\n","Epoch [18/100], Train Loss: 1.0618, Val Loss: 1.0247\n","Epoch [19/100], Train Loss: 1.0334, Val Loss: 0.9951\n","Epoch [20/100], Train Loss: 1.0042, Val Loss: 0.9696\n","Epoch [21/100], Train Loss: 0.9800, Val Loss: 0.9442\n","Epoch [22/100], Train Loss: 0.9574, Val Loss: 0.9237\n","Epoch [23/100], Train Loss: 0.9347, Val Loss: 0.9011\n","Epoch [24/100], Train Loss: 0.9149, Val Loss: 0.8819\n","Epoch [25/100], Train Loss: 0.8936, Val Loss: 0.8635\n","Epoch [26/100], Train Loss: 0.8749, Val Loss: 0.8451\n","Epoch [27/100], Train Loss: 0.8581, Val Loss: 0.8294\n","Epoch [28/100], Train Loss: 0.8429, Val Loss: 0.8142\n","Epoch [29/100], Train Loss: 0.8282, Val Loss: 0.7969\n","Epoch [30/100], Train Loss: 0.8097, Val Loss: 0.7826\n","Epoch [31/100], Train Loss: 0.7968, Val Loss: 0.7688\n","Epoch [32/100], Train Loss: 0.7815, Val Loss: 0.7550\n","Epoch [33/100], Train Loss: 0.7702, Val Loss: 0.7444\n","Epoch [34/100], Train Loss: 0.7598, Val Loss: 0.7312\n","Epoch [35/100], Train Loss: 0.7460, Val Loss: 0.7198\n","Epoch [36/100], Train Loss: 0.7346, Val Loss: 0.7079\n","Epoch [37/100], Train Loss: 0.7246, Val Loss: 0.6983\n","Epoch [38/100], Train Loss: 0.7129, Val Loss: 0.6876\n","Epoch [39/100], Train Loss: 0.7030, Val Loss: 0.6782\n","Epoch [40/100], Train Loss: 0.6949, Val Loss: 0.6700\n","Epoch [41/100], Train Loss: 0.6845, Val Loss: 0.6599\n","Epoch [42/100], Train Loss: 0.6757, Val Loss: 0.6511\n","Epoch [43/100], Train Loss: 0.6656, Val Loss: 0.6428\n","Epoch [44/100], Train Loss: 0.6588, Val Loss: 0.6346\n","Epoch [45/100], Train Loss: 0.6503, Val Loss: 0.6273\n","Epoch [46/100], Train Loss: 0.6437, Val Loss: 0.6198\n","Epoch [47/100], Train Loss: 0.6356, Val Loss: 0.6125\n","Epoch [48/100], Train Loss: 0.6281, Val Loss: 0.6060\n","Epoch [49/100], Train Loss: 0.6216, Val Loss: 0.5998\n","Epoch [50/100], Train Loss: 0.6145, Val Loss: 0.5937\n","Epoch [51/100], Train Loss: 0.6071, Val Loss: 0.5859\n","Epoch [52/100], Train Loss: 0.6020, Val Loss: 0.5798\n","Epoch [53/100], Train Loss: 0.5971, Val Loss: 0.5742\n","Epoch [54/100], Train Loss: 0.5906, Val Loss: 0.5692\n","Epoch [55/100], Train Loss: 0.5867, Val Loss: 0.5638\n","Epoch [56/100], Train Loss: 0.5820, Val Loss: 0.5585\n","Epoch [57/100], Train Loss: 0.5747, Val Loss: 0.5533\n","Epoch [58/100], Train Loss: 0.5693, Val Loss: 0.5484\n","Epoch [59/100], Train Loss: 0.5658, Val Loss: 0.5455\n","Epoch [60/100], Train Loss: 0.5605, Val Loss: 0.5399\n","Epoch [61/100], Train Loss: 0.5559, Val Loss: 0.5346\n","Epoch [62/100], Train Loss: 0.5530, Val Loss: 0.5320\n","Epoch [63/100], Train Loss: 0.5464, Val Loss: 0.5271\n","Epoch [64/100], Train Loss: 0.5438, Val Loss: 0.5245\n","Epoch [65/100], Train Loss: 0.5393, Val Loss: 0.5210\n","Epoch [66/100], Train Loss: 0.5370, Val Loss: 0.5172\n","Epoch [67/100], Train Loss: 0.5312, Val Loss: 0.5131\n","Epoch [68/100], Train Loss: 0.5295, Val Loss: 0.5096\n","Epoch [69/100], Train Loss: 0.5251, Val Loss: 0.5067\n","Epoch [70/100], Train Loss: 0.5223, Val Loss: 0.5028\n","Epoch [71/100], Train Loss: 0.5180, Val Loss: 0.5004\n","Epoch [72/100], Train Loss: 0.5152, Val Loss: 0.5001\n","Epoch [73/100], Train Loss: 0.5127, Val Loss: 0.4948\n","Epoch [74/100], Train Loss: 0.5097, Val Loss: 0.4916\n","Epoch [75/100], Train Loss: 0.5062, Val Loss: 0.4904\n","Epoch [76/100], Train Loss: 0.5034, Val Loss: 0.4869\n","Epoch [77/100], Train Loss: 0.5014, Val Loss: 0.4856\n","Epoch [78/100], Train Loss: 0.4988, Val Loss: 0.4819\n","Epoch [79/100], Train Loss: 0.4976, Val Loss: 0.4816\n","Epoch [80/100], Train Loss: 0.4931, Val Loss: 0.4778\n","Epoch [81/100], Train Loss: 0.4905, Val Loss: 0.4748\n","Epoch [82/100], Train Loss: 0.4879, Val Loss: 0.4734\n","Epoch [83/100], Train Loss: 0.4877, Val Loss: 0.4712\n","Epoch [84/100], Train Loss: 0.4839, Val Loss: 0.4713\n","Epoch [85/100], Train Loss: 0.4828, Val Loss: 0.4686\n","Epoch [86/100], Train Loss: 0.4811, Val Loss: 0.4665\n","Epoch [87/100], Train Loss: 0.4793, Val Loss: 0.4640\n","Epoch [88/100], Train Loss: 0.4772, Val Loss: 0.4618\n","Epoch [89/100], Train Loss: 0.4746, Val Loss: 0.4619\n","Epoch [90/100], Train Loss: 0.4721, Val Loss: 0.4593\n","Epoch [91/100], Train Loss: 0.4711, Val Loss: 0.4574\n","Epoch [92/100], Train Loss: 0.4691, Val Loss: 0.4567\n","Epoch [93/100], Train Loss: 0.4675, Val Loss: 0.4546\n","Epoch [94/100], Train Loss: 0.4657, Val Loss: 0.4527\n","Epoch [95/100], Train Loss: 0.4640, Val Loss: 0.4518\n","Epoch [96/100], Train Loss: 0.4616, Val Loss: 0.4512\n","Epoch [97/100], Train Loss: 0.4604, Val Loss: 0.4485\n","Epoch [98/100], Train Loss: 0.4590, Val Loss: 0.4474\n","Epoch [99/100], Train Loss: 0.4581, Val Loss: 0.4479\n","Epoch [100/100], Train Loss: 0.4572, Val Loss: 0.4459\n","Accuracy of the model on the test set: 75.83%\n","Fold 5/5\n","LSTMModel(\n","  (lstm): LSTM(6, 64, batch_first=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n","  (fc): Linear(in_features=64, out_features=17, bias=True)\n",")\n","Epoch [1/100], Train Loss: 2.7215, Val Loss: 2.5113\n","Epoch [2/100], Train Loss: 2.3560, Val Loss: 2.2775\n","Epoch [3/100], Train Loss: 2.2219, Val Loss: 2.1717\n","Epoch [4/100], Train Loss: 2.0895, Val Loss: 2.0158\n","Epoch [5/100], Train Loss: 1.9300, Val Loss: 1.8541\n","Epoch [6/100], Train Loss: 1.7820, Val Loss: 1.7179\n","Epoch [7/100], Train Loss: 1.6629, Val Loss: 1.6113\n","Epoch [8/100], Train Loss: 1.5674, Val Loss: 1.5254\n","Epoch [9/100], Train Loss: 1.4887, Val Loss: 1.4491\n","Epoch [10/100], Train Loss: 1.4228, Val Loss: 1.3860\n","Epoch [11/100], Train Loss: 1.3619, Val Loss: 1.3276\n","Epoch [12/100], Train Loss: 1.3098, Val Loss: 1.2760\n","Epoch [13/100], Train Loss: 1.2617, Val Loss: 1.2281\n","Epoch [14/100], Train Loss: 1.2163, Val Loss: 1.1884\n","Epoch [15/100], Train Loss: 1.1777, Val Loss: 1.1459\n","Epoch [16/100], Train Loss: 1.1399, Val Loss: 1.1092\n","Epoch [17/100], Train Loss: 1.1038, Val Loss: 1.0753\n","Epoch [18/100], Train Loss: 1.0757, Val Loss: 1.0443\n","Epoch [19/100], Train Loss: 1.0446, Val Loss: 1.0144\n","Epoch [20/100], Train Loss: 1.0185, Val Loss: 0.9881\n","Epoch [21/100], Train Loss: 0.9928, Val Loss: 0.9637\n","Epoch [22/100], Train Loss: 0.9675, Val Loss: 0.9395\n","Epoch [23/100], Train Loss: 0.9451, Val Loss: 0.9176\n","Epoch [24/100], Train Loss: 0.9235, Val Loss: 0.8966\n","Epoch [25/100], Train Loss: 0.9045, Val Loss: 0.8770\n","Epoch [26/100], Train Loss: 0.8861, Val Loss: 0.8590\n","Epoch [27/100], Train Loss: 0.8679, Val Loss: 0.8411\n","Epoch [28/100], Train Loss: 0.8486, Val Loss: 0.8250\n","Epoch [29/100], Train Loss: 0.8337, Val Loss: 0.8099\n","Epoch [30/100], Train Loss: 0.8176, Val Loss: 0.7938\n","Epoch [31/100], Train Loss: 0.8031, Val Loss: 0.7775\n","Epoch [32/100], Train Loss: 0.7884, Val Loss: 0.7646\n","Epoch [33/100], Train Loss: 0.7757, Val Loss: 0.7515\n","Epoch [34/100], Train Loss: 0.7635, Val Loss: 0.7379\n","Epoch [35/100], Train Loss: 0.7486, Val Loss: 0.7254\n","Epoch [36/100], Train Loss: 0.7377, Val Loss: 0.7143\n","Epoch [37/100], Train Loss: 0.7275, Val Loss: 0.7024\n","Epoch [38/100], Train Loss: 0.7154, Val Loss: 0.6920\n","Epoch [39/100], Train Loss: 0.7059, Val Loss: 0.6816\n","Epoch [40/100], Train Loss: 0.6953, Val Loss: 0.6723\n","Epoch [41/100], Train Loss: 0.6838, Val Loss: 0.6627\n","Epoch [42/100], Train Loss: 0.6754, Val Loss: 0.6530\n","Epoch [43/100], Train Loss: 0.6672, Val Loss: 0.6434\n","Epoch [44/100], Train Loss: 0.6594, Val Loss: 0.6368\n","Epoch [45/100], Train Loss: 0.6507, Val Loss: 0.6283\n","Epoch [46/100], Train Loss: 0.6428, Val Loss: 0.6211\n","Epoch [47/100], Train Loss: 0.6355, Val Loss: 0.6121\n","Epoch [48/100], Train Loss: 0.6271, Val Loss: 0.6066\n","Epoch [49/100], Train Loss: 0.6210, Val Loss: 0.5974\n","Epoch [50/100], Train Loss: 0.6139, Val Loss: 0.5927\n","Epoch [51/100], Train Loss: 0.6069, Val Loss: 0.5871\n","Epoch [52/100], Train Loss: 0.6007, Val Loss: 0.5797\n","Epoch [53/100], Train Loss: 0.5943, Val Loss: 0.5735\n","Epoch [54/100], Train Loss: 0.5886, Val Loss: 0.5692\n","Epoch [55/100], Train Loss: 0.5830, Val Loss: 0.5615\n","Epoch [56/100], Train Loss: 0.5770, Val Loss: 0.5567\n","Epoch [57/100], Train Loss: 0.5730, Val Loss: 0.5505\n","Epoch [58/100], Train Loss: 0.5642, Val Loss: 0.5462\n","Epoch [59/100], Train Loss: 0.5641, Val Loss: 0.5402\n","Epoch [60/100], Train Loss: 0.5573, Val Loss: 0.5372\n","Epoch [61/100], Train Loss: 0.5513, Val Loss: 0.5320\n","Epoch [62/100], Train Loss: 0.5475, Val Loss: 0.5266\n","Epoch [63/100], Train Loss: 0.5441, Val Loss: 0.5224\n","Epoch [64/100], Train Loss: 0.5393, Val Loss: 0.5190\n","Epoch [65/100], Train Loss: 0.5359, Val Loss: 0.5145\n","Epoch [66/100], Train Loss: 0.5312, Val Loss: 0.5118\n","Epoch [67/100], Train Loss: 0.5261, Val Loss: 0.5079\n","Epoch [68/100], Train Loss: 0.5238, Val Loss: 0.5038\n","Epoch [69/100], Train Loss: 0.5204, Val Loss: 0.4994\n","Epoch [70/100], Train Loss: 0.5158, Val Loss: 0.4968\n","Epoch [71/100], Train Loss: 0.5132, Val Loss: 0.4935\n","Epoch [72/100], Train Loss: 0.5099, Val Loss: 0.4915\n","Epoch [73/100], Train Loss: 0.5066, Val Loss: 0.4874\n","Epoch [74/100], Train Loss: 0.5040, Val Loss: 0.4843\n","Epoch [75/100], Train Loss: 0.5003, Val Loss: 0.4814\n","Epoch [76/100], Train Loss: 0.4984, Val Loss: 0.4789\n","Epoch [77/100], Train Loss: 0.4941, Val Loss: 0.4767\n","Epoch [78/100], Train Loss: 0.4911, Val Loss: 0.4736\n","Epoch [79/100], Train Loss: 0.4907, Val Loss: 0.4718\n","Epoch [80/100], Train Loss: 0.4886, Val Loss: 0.4692\n","Epoch [81/100], Train Loss: 0.4849, Val Loss: 0.4668\n","Epoch [82/100], Train Loss: 0.4825, Val Loss: 0.4652\n","Epoch [83/100], Train Loss: 0.4800, Val Loss: 0.4623\n","Epoch [84/100], Train Loss: 0.4769, Val Loss: 0.4591\n","Epoch [85/100], Train Loss: 0.4762, Val Loss: 0.4568\n","Epoch [86/100], Train Loss: 0.4746, Val Loss: 0.4564\n","Epoch [87/100], Train Loss: 0.4719, Val Loss: 0.4546\n","Epoch [88/100], Train Loss: 0.4689, Val Loss: 0.4519\n","Epoch [89/100], Train Loss: 0.4675, Val Loss: 0.4510\n","Epoch [90/100], Train Loss: 0.4667, Val Loss: 0.4484\n","Epoch [91/100], Train Loss: 0.4644, Val Loss: 0.4481\n","Epoch [92/100], Train Loss: 0.4608, Val Loss: 0.4451\n","Epoch [93/100], Train Loss: 0.4614, Val Loss: 0.4450\n","Epoch [94/100], Train Loss: 0.4572, Val Loss: 0.4412\n","Epoch [95/100], Train Loss: 0.4564, Val Loss: 0.4434\n","Epoch [96/100], Train Loss: 0.4563, Val Loss: 0.4384\n","Epoch [97/100], Train Loss: 0.4538, Val Loss: 0.4368\n","Epoch [98/100], Train Loss: 0.4505, Val Loss: 0.4357\n","Epoch [99/100], Train Loss: 0.4499, Val Loss: 0.4339\n","Epoch [100/100], Train Loss: 0.4497, Val Loss: 0.4348\n","Accuracy of the model on the test set: 78.39%\n","Accuracy of the model on the test set: 77.14%\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-15-b8475ab89a9f>:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  X_test = torch.tensor(X_test).float().unsqueeze(1)  # Add sequence length dimension\n","<ipython-input-15-b8475ab89a9f>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_test = torch.tensor(y_test).long()\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▄▅▇▁▁▂▆▇▇▁▂▂▃▃▃▄▄▅▆▆▇▇█▂▃▃▄▅████▁▃▄▅▆▇</td></tr><tr><td>train_loss</td><td>▆▄▃▃▃▂▂▁▁▁▇▄▄▃▃▁▁▄▄▃▂▁▁▁▄▂▁▁▁▁█▆▄▄▂▂▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅▂▂▁▁▁▁▁▁▁█▆▄▂▂▁▁▇▃▃▂▂▂▂▂▁▃▂▂▂▁▁▁▇▃▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>train_loss</td><td>0.44973</td></tr><tr><td>val_loss</td><td>0.43477</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">honest-meadow-23</strong> at: <a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-exp-1/runs/0ui9c9wh' target=\"_blank\">https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-exp-1/runs/0ui9c9wh</a><br/> View project at: <a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-exp-1' target=\"_blank\">https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-exp-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20241029_151758-0ui9c9wh/logs</code>"]},"metadata":{}}],"source":["model = model_pipeline(config)"]},{"cell_type":"markdown","metadata":{"id":"Rn-KrUz59yds"},"source":["## Save the model"]},{"cell_type":"code","execution_count":17,"metadata":{"collapsed":true,"executionInfo":{"elapsed":3,"status":"ok","timestamp":1730215513418,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"AwFKsiy3LVKk"},"outputs":[],"source":["# Save the model\n","torch.save(model.state_dict(), 'lstm_model.pth')\n","\n","# # Save the model as onnx\n","# torch.onnx.export(model, X_train, 'lstm_model.onnx')"]},{"cell_type":"code","source":["def preprocess_test_data(data, data_percentage):\n","    # Windowing the data\n","    data = calculate_averages_and_dispersion(data, data_percentage)\n","    print(data.shape)\n","\n","    # Assuming the last column is the target\n","    X = data.iloc[:, :-1].values\n","    y = data.iloc[:, -1].values\n","\n","    # Encode labeling of target data using presaved pkl file\n","    # Load label encoder\n","    label_encoder_path = '/content/drive/MyDrive/PhD/Colab Notebooks/label_encoder.pkl'\n","    le = joblib.load(label_encoder_path)\n","    y = le.transform(y)\n","    print('y: ', y)\n","\n","    # Standardize the features\n","    scaler = StandardScaler()\n","    X = scaler.fit_transform(X)\n","\n","    # Convert to PyTorch tensors\n","    X = torch.tensor(X, dtype=torch.float32)\n","    y = torch.tensor(y, dtype=torch.long)\n","\n","    return X, y"],"metadata":{"id":"0ggQaw0lISGG","executionInfo":{"status":"ok","timestamp":1730221673382,"user_tz":-60,"elapsed":270,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"}}},"execution_count":171,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kXfBY8sm1Z43"},"source":["## Load New Testing Data"]},{"cell_type":"code","execution_count":172,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"ArtddUGY1Z43","executionInfo":{"status":"ok","timestamp":1730221676081,"user_tz":-60,"elapsed":1463,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"}},"outputId":"c836c30c-5905-4b59-b65b-953469a26ea0"},"outputs":[{"output_type":"stream","name":"stdout","text":["['H1_1 - Copy.csv']\n","      Sample  Frequency (GHz)    LG (mV)    HG (mV)  Thickness (mm)\n","0         H1              100  69.100232   0.244141            0.07\n","1         H1              100  53.229153   0.366211            0.07\n","2         H1              100  62.019289   1.587129            0.07\n","3         H1              100  67.268954  -0.244141            0.07\n","4         H1              100  75.326578   1.220798            0.07\n","...      ...              ...        ...        ...             ...\n","63947     H1              600   0.244170  24.417043            0.07\n","63948     H1              600  -0.732511  12.086436            0.07\n","63949     H1              600   0.122085  29.300451            0.07\n","63950     H1              600  -0.244170   1.220852            0.07\n","63951     H1              600  -0.610426  33.573434            0.07\n","\n","[63952 rows x 5 columns]\n","(63952, 5)\n","     Frequency (GHz)  LG (mV) mean  HG (mV) mean  LG (mV) std deviation  \\\n","0                100     66.101648     -0.055687              24.633872   \n","1                100     65.467662      0.083531              24.145444   \n","2                100     65.403406     -0.113516              25.154737   \n","3                100     66.551436     -0.074966              25.552439   \n","4                100     67.174713      0.023559              28.480576   \n","..               ...           ...           ...                    ...   \n","658              600      0.150426     26.175288               0.893086   \n","659              600      0.041422     24.880312               0.788236   \n","660              600      0.021801     24.905384               0.957551   \n","661              600     -0.106825     25.121213               0.909939   \n","662              600     -0.366256     19.045293               0.386067   \n","\n","     HG (mV) std deviation  Thickness (mm) Sample  \n","0                 0.634816            0.07     H1  \n","1                 0.696428            0.07     H1  \n","2                 0.695544            0.07     H1  \n","3                 0.687035            0.07     H1  \n","4                 0.729388            0.07     H1  \n","..                     ...             ...    ...  \n","658               9.735454            0.07     H1  \n","659               9.709209            0.07     H1  \n","660               9.699742            0.07     H1  \n","661               9.328112            0.07     H1  \n","662              15.081684            0.07     H1  \n","\n","[663 rows x 7 columns]\n","(663, 7)\n","y:  [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n"," 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n"," 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n"," 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n"," 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n"," 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n"," 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n"," 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n"," 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n"," 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n"," 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n"," 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n"," 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n"," 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n"," 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n"," 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n"," 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n"," 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\n"]}],"source":["# Load new data\n","input_data_test = '/content/drive/MyDrive/PhD/Colab Notebooks/test_data/'\n","print(os.listdir(input_data_test))\n","\n","data_test = load_data_from_directory(input_data_test)\n","\n","# Load and preprocess data\n","X_test, y_test = preprocess_test_data(data_test, data_percentage=8.33) # 1s window size\n"]},{"cell_type":"markdown","metadata":{"id":"faC6EfmDMsRL"},"source":["## Run inference"]},{"cell_type":"code","execution_count":173,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":254,"status":"ok","timestamp":1730221686788,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"nxNfBFvFowYp","outputId":"53932313-6593-4d90-9d50-bf63aa6caec2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","tensor([[[-1.6984e+00,  2.3475e+00, -6.4605e-01,  3.2998e+00, -8.6543e-01,\n","          -1.3878e-17]],\n","\n","        [[-1.6984e+00,  2.3206e+00, -6.4533e-01,  3.2246e+00, -8.5962e-01,\n","          -1.3878e-17]],\n","\n","        [[-1.6984e+00,  2.3179e+00, -6.4635e-01,  3.3801e+00, -8.5971e-01,\n","          -1.3878e-17]],\n","\n","        ...,\n","\n","        [[ 1.6984e+00, -4.5441e-01, -5.1670e-01, -3.4733e-01, -1.1629e-02,\n","          -1.3878e-17]],\n","\n","        [[ 1.6984e+00, -4.5986e-01, -5.1558e-01, -3.5466e-01, -4.6631e-02,\n","          -1.3878e-17]],\n","\n","        [[ 1.6984e+00, -4.7086e-01, -5.4707e-01, -4.3536e-01,  4.9528e-01,\n","          -1.3878e-17]]])\n","['B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'A1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'A1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'A1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'A1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'A1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'A1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'A1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n"," 'B1' 'B1' 'B1' 'B1' 'B1']\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-173-6d9f243c273d>:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_path))\n"]}],"source":["# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","\n","# Initialize the model\n","input_dim = X_test.shape[1]\n","hidden_dim = config['hidden_dim']  # Replace with the hidden dimension used during training\n","output_dim = config['classes']  # Replace with the number of output classes used during training\n","model = LSTMModel(input_dim, hidden_dim, output_dim).to(device)\n","\n","# # Load label encoder\n","label_encoder_path = '/content/drive/MyDrive/PhD/Colab Notebooks/label_encoder.pkl'\n","le = joblib.load(label_encoder_path)\n","\n","# Load pretrained model\n","model_path = '/content/drive/MyDrive/PhD/Colab Notebooks/lstm_model.pth'\n","model.load_state_dict(torch.load(model_path))\n","model.eval()\n","\n","with torch.no_grad():\n","    X_test = X_test.unsqueeze(1).to(device)\n","    print(X_test)\n","    outputs = model(X_test)\n","    _, predicted = torch.max(outputs.data, 1)\n","    # print(predicted)\n","\n","# Decode the predicted labels\n","# Now perform the inverse transform\n","predicted_labels = le.inverse_transform(predicted.cpu().numpy())\n","\n","print(predicted_labels)"]},{"cell_type":"markdown","source":["| Original Label | Encoded Value |\n","|----------------|---------------|\n","| A1             | 0             |\n","| B1             | 1             |\n","| C1             | 2             |\n","| D1             | 3             |\n","| E1             | 4             |\n","| E2             | 5             |\n","| E3             | 6             |\n","| F1             | 7             |\n","| G1             | 8             |\n","| H1             | 9             |\n","| I1             | 10            |\n","| J1             | 11            |\n","| K1             | 12            |\n","| L1             | 13            |\n","| M1             | 14            |\n","| N1             | 15            |\n","| REF            | 16            |"],"metadata":{"id":"ktsgpY21NcOV"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}