{"cells":[{"cell_type":"markdown","metadata":{"id":"peUcVKtbGPfD"},"source":["# LSTM Classification Model\n","\n","This notebook demonstrates how to load data, preprocess it, define an LSTM model, train the model, and evaluate its performance. The data is assumed to be in CSV format and stored in a directory.\n","\n","## Setup\n","\n","First, we need to install the necessary libraries. Run the following cell to install them."]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8076,"status":"ok","timestamp":1730196555896,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"aAZ2n50rGS-C","outputId":"543edfa0-34f7-4ced-87b3-e772c48d4c22"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n","Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"]}],"source":["%pip install torch torchvision torchaudio\n","%pip install pandas scikit-learn\n","%pip install wandb onnx -Uq"]},{"cell_type":"markdown","metadata":{"id":"l4d3KxhxGML8"},"source":["## Import Libraries and seed\n","Import the necessary libraries for data processing, model building, training, and evaluation. Adding a seed ensures reproducibility by making sure that the random number generation is consistent across different runs."]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1730196555896,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"9a9HvzrNG8iw"},"outputs":[],"source":["import os\n","import random\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","import wandb\n","\n","def set_seed(seed):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    random.seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3025,"status":"ok","timestamp":1730196558917,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"SUQtUPxSBzq4","outputId":"6e2f6799-ed79-4b6d-a48a-4fdf2853a1b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1730196558917,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"AoCyZSTt7qqS","outputId":"7827dda1-467b-4fb9-8051-846005728760"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login()"]},{"cell_type":"markdown","metadata":{"id":"S1-6Xv6LHHmZ"},"source":["## Load Data from Github Repository\n"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1730196558917,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"eI7hrAaVHL44"},"outputs":[],"source":["## Remove PIC-PAPER-01 folder:\n","!rm -rf PIC-PAPER-01\n","\n","# # Download Github Repo (Private) https://stackoverflow.com/questions/74532852/clone-github-repo-with-fine-grained-token/78280453#78280453\n","# !git clone --no-checkout https://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@github.com/danimp94/PIC-PAPER-01.git\n","\n","# # To clone data folder only:\n","# %cd PIC-PAPER-01 # Navigate to the repository directory\n","# !git sparse-checkout init --cone # Initialize sparse-checkout\n","# !git sparse-checkout set data # Set the sparse-checkout to include only the data/ folder\n","# !git checkout # Checkout the specified folder"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2617,"status":"ok","timestamp":1730196561532,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"hAY8BfVX9XCK","outputId":"eb642040-eef4-4bfd-ac32-6d73ae0e48d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["        Sample  Frequency (GHz)     LG (mV)    HG (mV)  Thickness (mm)\n","0           A1            100.0   -7.080942  -0.854611             0.2\n","1           A1            100.0   67.024785   0.244141             0.2\n","2           A1            100.0  124.893178  -1.098776             0.2\n","3           A1            100.0   91.075571   0.000000             0.2\n","4           A1            100.0   48.956174   0.122094             0.2\n","...        ...              ...         ...        ...             ...\n","2737958    REF            600.0    0.366256  16.237333             0.0\n","2737959    REF            600.0    0.000000  -7.080942             0.0\n","2737960    REF            600.0   -0.244170  15.260652             0.0\n","2737961    REF            600.0    0.366256  20.021975             0.0\n","2737962    REF            600.0    0.122085  13.185203             0.0\n","\n","[2737963 rows x 5 columns]\n","(2737963, 5)\n"]}],"source":["input_path = '/content/drive/MyDrive/PhD/Colab Notebooks/'\n","\n","data_frames = []\n","for file in os.listdir(input_path):\n","    if file.endswith('.csv'):\n","        df = pd.read_csv(os.path.join(input_path, file), delimiter=';', header=0)\n","        data_frames.append(df)\n","data = pd.concat(data_frames, ignore_index=True)\n","\n","print(data)\n","print(data.shape)"]},{"cell_type":"markdown","metadata":{"id":"wiCTw-qcKXhn"},"source":["## Preprocessing Data\n","Define a function to preprocess the data. This includes encoding categorical labels and standardizing the features."]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1730196561532,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"zJoZqi5LDPl8"},"outputs":[],"source":["def calculate_averages_and_dispersion(data, data_percentage=3.7):\n","    df = data\n","    results = []\n","    for (sample, freq), group in df.groupby(['Sample', 'Frequency (GHz)']):\n","        window_size = max(1, int(len(group) * data_percentage / 100))\n","        # print(f\"Processing sample: {sample}, frequency: {freq} with window size: {window_size}\")\n","        for start in range(0, len(group), window_size):\n","            window_data = group.iloc[start:start + window_size]\n","            mean_values = window_data[['LG (mV)', 'HG (mV)']].mean()\n","            std_deviation_values = window_data[['LG (mV)', 'HG (mV)']].std()\n","            results.append({\n","                'Frequency (GHz)': freq,\n","                'LG (mV) mean': mean_values['LG (mV)'],\n","                'HG (mV) mean': mean_values['HG (mV)'],\n","                'LG (mV) std deviation': std_deviation_values['LG (mV)'],\n","                'HG (mV) std deviation': std_deviation_values['HG (mV)'],\n","                'Thickness (mm)': window_data['Thickness (mm)'].iloc[0],\n","                'Sample': sample,\n","            })\n","    results_df = pd.DataFrame(results)\n","    # results_df.to_csv(output_file, sep=';', index=False)\n","    # print(f\"Processed {input_file} and saved to {output_file}\")\n","    print(results_df)\n","    return results_df"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1730196561532,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"KZPdMwkkKmm7"},"outputs":[],"source":["def preprocess_data(data):\n","    # Windowing the data\n","    data = calculate_averages_and_dispersion(data)\n","    print(data.shape)\n","\n","    # Assuming the last column is the target\n","    X = data.iloc[:, :-1].values\n","    y = data.iloc[:, -1].values\n","\n","    # Encode the target variable if it's categorical\n","    if y.dtype == 'object':\n","        le = LabelEncoder()\n","        y = le.fit_transform(y)\n","\n","    # Standardize the features\n","    scaler = StandardScaler()\n","    X = scaler.fit_transform(X)\n","\n","    # Convert to PyTorch tensors\n","    X = torch.tensor(X, dtype=torch.float32)\n","    y = torch.tensor(y, dtype=torch.long)\n","\n","    return X, y"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45308,"status":"ok","timestamp":1730196606834,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"UTXvxCzu6MBA","outputId":"7ea57e02-e279-473d-c54d-82ba86c1f28a"},"outputs":[{"name":"stdout","output_type":"stream","text":["       Frequency (GHz)  LG (mV) mean  HG (mV) mean  LG (mV) std deviation  \\\n","0                100.0     54.879155     -0.022198              29.958659   \n","1                100.0     54.511665      0.048093              28.096155   \n","2                100.0     55.099894     -0.118380              26.833871   \n","3                100.0     48.387674      0.103588              28.843498   \n","4                100.0     49.932853     -0.071525              23.093397   \n","...                ...           ...           ...                    ...   \n","24271            600.0     -0.006143     10.237498               0.890999   \n","24272            600.0     -0.006910     10.949278               0.858148   \n","24273            600.0      0.029178     10.547702               0.842082   \n","24274            600.0      0.065266     10.051683               0.891632   \n","24275            600.0     -0.228910      9.766817               0.856929   \n","\n","       HG (mV) std deviation  Thickness (mm) Sample  \n","0                   0.644211             0.2     A1  \n","1                   0.704742             0.2     A1  \n","2                   0.755493             0.2     A1  \n","3                   0.854655             0.2     A1  \n","4                   0.650231             0.2     A1  \n","...                      ...             ...    ...  \n","24271               8.205224             0.0    REF  \n","24272               8.679982             0.0    REF  \n","24273               8.802972             0.0    REF  \n","24274               8.365464             0.0    REF  \n","24275               9.725526             0.0    REF  \n","\n","[24276 rows x 7 columns]\n","(24276, 7)\n"]}],"source":["# Load and preprocess data\n","X, y = preprocess_data(data)"]},{"cell_type":"markdown","metadata":{"id":"BZjha9Cx6MBB"},"source":["## Config"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1730196606834,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"oLRhwd8cKKJH","outputId":"64e406f8-666c-47b8-b200-1bc595b20067"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'epochs': 2000, 'seed': 40, 'classes': 17, 'batch_size': 128, 'learning_rate': 0.001, 'dataset': 'experiment_1', 'architecture': 'LSTM'}\n"]}],"source":["config = dict(\n","    epochs=2000,\n","    seed = 40,\n","    classes = data['Sample'].nunique(), # Each different sample is a different class\n","    batch_size=128,\n","    learning_rate=0.001,\n","    dataset=\"experiment_1\",\n","    architecture=\"LSTM\")\n","\n","print(config)"]},{"cell_type":"markdown","metadata":{"id":"RI96M_V6KpyH"},"source":["## Define Model\n","Define the LSTM model architecture"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1730196606834,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"h6j-_U_RKxlR"},"outputs":[],"source":["class LSTMModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(LSTMModel, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.lstm = nn.LSTM(input_dim, hidden_dim)\n","        self.dropout = nn.Dropout(0.2)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        # Reshape input to (seq_len, batch_size, input_size)\n","        x = x.unsqueeze(1)  # Adding a dimension for batch size (batch_size=1 in this case)\n","\n","        # Initialize hidden and cell states with the correct dimensions\n","        h_0 = torch.zeros(1, x.size(1), self.hidden_dim).to(x.device)\n","        c_0 = torch.zeros(1, x.size(1), self.hidden_dim).to(x.device)\n","\n","        # Now the input is 3-dimensional, so hx and cx should be 3-dimensional as well\n","        out, _ = self.lstm(x, (h_0, c_0))\n","\n","        out = self.dropout(out[:, -1, :])\n","        out = self.fc(out)\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"afvxx1KzKzYp"},"source":["## Train Model\n","Define a function to train the model"]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1730196606834,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"-3a9IuRVK3Wq"},"outputs":[],"source":["def train_model(model, train_loader, criterion, optimizer, device, num_epochs=10):\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        for X_batch, y_batch in train_loader:\n","              X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","\n","              outputs = model(X_batch)\n","              loss = criterion(outputs, y_batch)\n","\n","              optimizer.zero_grad()\n","              loss.backward()\n","              optimizer.step()\n","\n","              running_loss += loss.item()\n","\n","                # Log metrics to W&B\n","        wandb.log({\"epoch\": epoch, \"train_loss\": running_loss / len(train_loader)})\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}\")\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"G1bDpP8JK5jp"},"source":["## Evaluate Model\n"]},{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1730196606834,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"TIDNXJGRK9af"},"outputs":[],"source":["def evaluate_model(model, test_loader, device):\n","    model.eval()\n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        for X_batch, y_batch in test_loader:\n","            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","            outputs = model(X_batch)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += y_batch.size(0)\n","            correct += (predicted == y_batch).sum().item()\n","\n","        accuracy = correct / total\n","        print(f'Test Accuracy: {accuracy:.4f}')"]},{"cell_type":"code","execution_count":50,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1730196606834,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"KBOELYWD6MBB"},"outputs":[],"source":["def make(config):\n","    # Split the data\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=config.seed)\n","    # X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=config.seed)\n","\n","    train_dataset = TensorDataset(X_train, y_train)\n","    # val_dataset = TensorDataset(X_val, y_val)\n","    test_dataset = TensorDataset(X_test, y_test)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n","    # val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n","\n","    # Define the model\n","    model = LSTMModel(X_train.shape[1], 64, config['classes']).to(device)\n","\n","    # Loss and optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n","\n","    return model, train_loader, test_loader, criterion, optimizer"]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1730196606834,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"SlnfXqu-6MBB"},"outputs":[],"source":["def model_pipeline(hyperparameters):\n","\n","    with wandb.init(project=\"PIC-PAPER-01-exp-1\", config=hyperparameters):\n","        config = wandb.config\n","        set_seed(config.seed)\n","        # print(config['seed'])\n","\n","        # Create data loaders and model\n","        model, train_loader, test_loader, criterion, optimizer = make(config)\n","        print(model)\n","\n","        # Train the model\n","        train_model(model, train_loader, criterion, optimizer, device, config.epochs)\n","\n","        # Evaluate the model\n","        evaluate_model(model, test_loader, device)\n","\n","    return model\n"]},{"cell_type":"markdown","metadata":{"id":"WEtZPi2fLQGL"},"source":["## Run Training"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d3a8aac031be45a19e66f495aed88416","af16d21180564b818b3dd48fb935a3dd","cb140d7ce2fb4340955b7f629a793096","90b548504b704a07b637a94a5734a5a5","0bcb4f140af44970a195805336fa2f45","b2b8d7f483b84089b346dbd7d53a6e58","08677188194645c49bc77537acbdcea3","003686f77a4b4fd7ac837ad65d707b02"]},"executionInfo":{"elapsed":1006603,"status":"ok","timestamp":1730197613429,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"tScgXvxA6MBC","outputId":"fe05ea6c-8bd5-49e2-b566-7380431e3491"},"outputs":[{"data":{"text/html":["Tracking run with wandb version 0.18.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20241029_101009-6dhw7563</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-exp-1/runs/6dhw7563' target=\"_blank\">trim-sea-12</a></strong> to <a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-exp-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-exp-1' target=\"_blank\">https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-exp-1</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-exp-1/runs/6dhw7563' target=\"_blank\">https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-exp-1/runs/6dhw7563</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["LSTMModel(\n","  (lstm): LSTM(6, 64)\n","  (dropout): Dropout(p=0.2, inplace=False)\n","  (fc): Linear(in_features=64, out_features=17, bias=True)\n",")\n","Epoch [1/2000], Train Loss: 2.6490\n","Epoch [2/2000], Train Loss: 2.2570\n","Epoch [3/2000], Train Loss: 2.1174\n","Epoch [4/2000], Train Loss: 1.9824\n","Epoch [5/2000], Train Loss: 1.8653\n","Epoch [6/2000], Train Loss: 1.7684\n","Epoch [7/2000], Train Loss: 1.6865\n","Epoch [8/2000], Train Loss: 1.6141\n","Epoch [9/2000], Train Loss: 1.5529\n","Epoch [10/2000], Train Loss: 1.4991\n","Epoch [11/2000], Train Loss: 1.4469\n","Epoch [12/2000], Train Loss: 1.4038\n","Epoch [13/2000], Train Loss: 1.3605\n","Epoch [14/2000], Train Loss: 1.3263\n","Epoch [15/2000], Train Loss: 1.2869\n","Epoch [16/2000], Train Loss: 1.2543\n","Epoch [17/2000], Train Loss: 1.2235\n","Epoch [18/2000], Train Loss: 1.1933\n","Epoch [19/2000], Train Loss: 1.1670\n","Epoch [20/2000], Train Loss: 1.1369\n","Epoch [21/2000], Train Loss: 1.1118\n","Epoch [22/2000], Train Loss: 1.0915\n","Epoch [23/2000], Train Loss: 1.0689\n","Epoch [24/2000], Train Loss: 1.0490\n","Epoch [25/2000], Train Loss: 1.0272\n","Epoch [26/2000], Train Loss: 1.0090\n","Epoch [27/2000], Train Loss: 0.9897\n","Epoch [28/2000], Train Loss: 0.9748\n","Epoch [29/2000], Train Loss: 0.9553\n","Epoch [30/2000], Train Loss: 0.9420\n","Epoch [31/2000], Train Loss: 0.9265\n","Epoch [32/2000], Train Loss: 0.9104\n","Epoch [33/2000], Train Loss: 0.8971\n","Epoch [34/2000], Train Loss: 0.8829\n","Epoch [35/2000], Train Loss: 0.8704\n","Epoch [36/2000], Train Loss: 0.8602\n","Epoch [37/2000], Train Loss: 0.8438\n","Epoch [38/2000], Train Loss: 0.8343\n","Epoch [39/2000], Train Loss: 0.8247\n","Epoch [40/2000], Train Loss: 0.8112\n","Epoch [41/2000], Train Loss: 0.7989\n","Epoch [42/2000], Train Loss: 0.7884\n","Epoch [43/2000], Train Loss: 0.7782\n","Epoch [44/2000], Train Loss: 0.7697\n","Epoch [45/2000], Train Loss: 0.7606\n","Epoch [46/2000], Train Loss: 0.7511\n","Epoch [47/2000], Train Loss: 0.7415\n","Epoch [48/2000], Train Loss: 0.7335\n","Epoch [49/2000], Train Loss: 0.7264\n","Epoch [50/2000], Train Loss: 0.7175\n","Epoch [51/2000], Train Loss: 0.7103\n","Epoch [52/2000], Train Loss: 0.7037\n","Epoch [53/2000], Train Loss: 0.6960\n","Epoch [54/2000], Train Loss: 0.6869\n","Epoch [55/2000], Train Loss: 0.6812\n","Epoch [56/2000], Train Loss: 0.6754\n","Epoch [57/2000], Train Loss: 0.6706\n","Epoch [58/2000], Train Loss: 0.6625\n","Epoch [59/2000], Train Loss: 0.6558\n","Epoch [60/2000], Train Loss: 0.6508\n","Epoch [61/2000], Train Loss: 0.6448\n","Epoch [62/2000], Train Loss: 0.6390\n","Epoch [63/2000], Train Loss: 0.6339\n","Epoch [64/2000], Train Loss: 0.6311\n","Epoch [65/2000], Train Loss: 0.6242\n","Epoch [66/2000], Train Loss: 0.6200\n","Epoch [67/2000], Train Loss: 0.6141\n","Epoch [68/2000], Train Loss: 0.6115\n","Epoch [69/2000], Train Loss: 0.6069\n","Epoch [70/2000], Train Loss: 0.6039\n","Epoch [71/2000], Train Loss: 0.5972\n","Epoch [72/2000], Train Loss: 0.5933\n","Epoch [73/2000], Train Loss: 0.5876\n","Epoch [74/2000], Train Loss: 0.5847\n","Epoch [75/2000], Train Loss: 0.5818\n","Epoch [76/2000], Train Loss: 0.5758\n","Epoch [77/2000], Train Loss: 0.5743\n","Epoch [78/2000], Train Loss: 0.5726\n","Epoch [79/2000], Train Loss: 0.5673\n","Epoch [80/2000], Train Loss: 0.5637\n","Epoch [81/2000], Train Loss: 0.5613\n","Epoch [82/2000], Train Loss: 0.5593\n","Epoch [83/2000], Train Loss: 0.5549\n","Epoch [84/2000], Train Loss: 0.5534\n","Epoch [85/2000], Train Loss: 0.5524\n","Epoch [86/2000], Train Loss: 0.5475\n","Epoch [87/2000], Train Loss: 0.5444\n","Epoch [88/2000], Train Loss: 0.5415\n","Epoch [89/2000], Train Loss: 0.5415\n","Epoch [90/2000], Train Loss: 0.5376\n","Epoch [91/2000], Train Loss: 0.5366\n","Epoch [92/2000], Train Loss: 0.5347\n","Epoch [93/2000], Train Loss: 0.5328\n","Epoch [94/2000], Train Loss: 0.5305\n","Epoch [95/2000], Train Loss: 0.5274\n","Epoch [96/2000], Train Loss: 0.5262\n","Epoch [97/2000], Train Loss: 0.5231\n","Epoch [98/2000], Train Loss: 0.5216\n","Epoch [99/2000], Train Loss: 0.5189\n","Epoch [100/2000], Train Loss: 0.5204\n","Epoch [101/2000], Train Loss: 0.5149\n","Epoch [102/2000], Train Loss: 0.5159\n","Epoch [103/2000], Train Loss: 0.5131\n","Epoch [104/2000], Train Loss: 0.5124\n","Epoch [105/2000], Train Loss: 0.5093\n","Epoch [106/2000], Train Loss: 0.5092\n","Epoch [107/2000], Train Loss: 0.5074\n","Epoch [108/2000], Train Loss: 0.5050\n","Epoch [109/2000], Train Loss: 0.5055\n","Epoch [110/2000], Train Loss: 0.5030\n","Epoch [111/2000], Train Loss: 0.5006\n","Epoch [112/2000], Train Loss: 0.5016\n","Epoch [113/2000], Train Loss: 0.4978\n","Epoch [114/2000], Train Loss: 0.4994\n","Epoch [115/2000], Train Loss: 0.4978\n","Epoch [116/2000], Train Loss: 0.4969\n","Epoch [117/2000], Train Loss: 0.4963\n","Epoch [118/2000], Train Loss: 0.4920\n","Epoch [119/2000], Train Loss: 0.4936\n","Epoch [120/2000], Train Loss: 0.4920\n","Epoch [121/2000], Train Loss: 0.4902\n","Epoch [122/2000], Train Loss: 0.4897\n","Epoch [123/2000], Train Loss: 0.4903\n","Epoch [124/2000], Train Loss: 0.4894\n","Epoch [125/2000], Train Loss: 0.4869\n","Epoch [126/2000], Train Loss: 0.4870\n","Epoch [127/2000], Train Loss: 0.4853\n","Epoch [128/2000], Train Loss: 0.4828\n","Epoch [129/2000], Train Loss: 0.4827\n","Epoch [130/2000], Train Loss: 0.4833\n","Epoch [131/2000], Train Loss: 0.4836\n","Epoch [132/2000], Train Loss: 0.4823\n","Epoch [133/2000], Train Loss: 0.4820\n","Epoch [134/2000], Train Loss: 0.4827\n","Epoch [135/2000], Train Loss: 0.4810\n","Epoch [136/2000], Train Loss: 0.4790\n","Epoch [137/2000], Train Loss: 0.4795\n","Epoch [138/2000], Train Loss: 0.4785\n","Epoch [139/2000], Train Loss: 0.4804\n","Epoch [140/2000], Train Loss: 0.4763\n","Epoch [141/2000], Train Loss: 0.4769\n","Epoch [142/2000], Train Loss: 0.4740\n","Epoch [143/2000], Train Loss: 0.4748\n","Epoch [144/2000], Train Loss: 0.4735\n","Epoch [145/2000], Train Loss: 0.4752\n","Epoch [146/2000], Train Loss: 0.4720\n","Epoch [147/2000], Train Loss: 0.4708\n","Epoch [148/2000], Train Loss: 0.4708\n","Epoch [149/2000], Train Loss: 0.4719\n","Epoch [150/2000], Train Loss: 0.4698\n","Epoch [151/2000], Train Loss: 0.4705\n","Epoch [152/2000], Train Loss: 0.4693\n","Epoch [153/2000], Train Loss: 0.4688\n","Epoch [154/2000], Train Loss: 0.4685\n","Epoch [155/2000], Train Loss: 0.4689\n","Epoch [156/2000], Train Loss: 0.4682\n","Epoch [157/2000], Train Loss: 0.4673\n","Epoch [158/2000], Train Loss: 0.4674\n","Epoch [159/2000], Train Loss: 0.4680\n","Epoch [160/2000], Train Loss: 0.4685\n","Epoch [161/2000], Train Loss: 0.4641\n","Epoch [162/2000], Train Loss: 0.4667\n","Epoch [163/2000], Train Loss: 0.4664\n","Epoch [164/2000], Train Loss: 0.4624\n","Epoch [165/2000], Train Loss: 0.4621\n","Epoch [166/2000], Train Loss: 0.4628\n","Epoch [167/2000], Train Loss: 0.4625\n","Epoch [168/2000], Train Loss: 0.4630\n","Epoch [169/2000], Train Loss: 0.4609\n","Epoch [170/2000], Train Loss: 0.4632\n","Epoch [171/2000], Train Loss: 0.4630\n","Epoch [172/2000], Train Loss: 0.4615\n","Epoch [173/2000], Train Loss: 0.4601\n","Epoch [174/2000], Train Loss: 0.4589\n","Epoch [175/2000], Train Loss: 0.4602\n","Epoch [176/2000], Train Loss: 0.4589\n","Epoch [177/2000], Train Loss: 0.4611\n","Epoch [178/2000], Train Loss: 0.4581\n","Epoch [179/2000], Train Loss: 0.4569\n","Epoch [180/2000], Train Loss: 0.4560\n","Epoch [181/2000], Train Loss: 0.4574\n","Epoch [182/2000], Train Loss: 0.4567\n","Epoch [183/2000], Train Loss: 0.4553\n","Epoch [184/2000], Train Loss: 0.4572\n","Epoch [185/2000], Train Loss: 0.4549\n","Epoch [186/2000], Train Loss: 0.4572\n","Epoch [187/2000], Train Loss: 0.4563\n","Epoch [188/2000], Train Loss: 0.4553\n","Epoch [189/2000], Train Loss: 0.4539\n","Epoch [190/2000], Train Loss: 0.4540\n","Epoch [191/2000], Train Loss: 0.4526\n","Epoch [192/2000], Train Loss: 0.4523\n","Epoch [193/2000], Train Loss: 0.4502\n","Epoch [194/2000], Train Loss: 0.4503\n","Epoch [195/2000], Train Loss: 0.4499\n","Epoch [196/2000], Train Loss: 0.4501\n","Epoch [197/2000], Train Loss: 0.4502\n","Epoch [198/2000], Train Loss: 0.4507\n","Epoch [199/2000], Train Loss: 0.4503\n","Epoch [200/2000], Train Loss: 0.4516\n","Epoch [201/2000], Train Loss: 0.4513\n","Epoch [202/2000], Train Loss: 0.4510\n","Epoch [203/2000], Train Loss: 0.4489\n","Epoch [204/2000], Train Loss: 0.4482\n","Epoch [205/2000], Train Loss: 0.4472\n","Epoch [206/2000], Train Loss: 0.4474\n","Epoch [207/2000], Train Loss: 0.4459\n","Epoch [208/2000], Train Loss: 0.4451\n","Epoch [209/2000], Train Loss: 0.4480\n","Epoch [210/2000], Train Loss: 0.4462\n","Epoch [211/2000], Train Loss: 0.4479\n","Epoch [212/2000], Train Loss: 0.4453\n","Epoch [213/2000], Train Loss: 0.4438\n","Epoch [214/2000], Train Loss: 0.4431\n","Epoch [215/2000], Train Loss: 0.4460\n","Epoch [216/2000], Train Loss: 0.4444\n","Epoch [217/2000], Train Loss: 0.4456\n","Epoch [218/2000], Train Loss: 0.4427\n","Epoch [219/2000], Train Loss: 0.4436\n","Epoch [220/2000], Train Loss: 0.4437\n","Epoch [221/2000], Train Loss: 0.4428\n","Epoch [222/2000], Train Loss: 0.4446\n","Epoch [223/2000], Train Loss: 0.4426\n","Epoch [224/2000], Train Loss: 0.4412\n","Epoch [225/2000], Train Loss: 0.4430\n","Epoch [226/2000], Train Loss: 0.4424\n","Epoch [227/2000], Train Loss: 0.4385\n","Epoch [228/2000], Train Loss: 0.4408\n","Epoch [229/2000], Train Loss: 0.4399\n","Epoch [230/2000], Train Loss: 0.4414\n","Epoch [231/2000], Train Loss: 0.4406\n","Epoch [232/2000], Train Loss: 0.4396\n","Epoch [233/2000], Train Loss: 0.4413\n","Epoch [234/2000], Train Loss: 0.4417\n","Epoch [235/2000], Train Loss: 0.4395\n","Epoch [236/2000], Train Loss: 0.4411\n","Epoch [237/2000], Train Loss: 0.4378\n","Epoch [238/2000], Train Loss: 0.4405\n","Epoch [239/2000], Train Loss: 0.4392\n","Epoch [240/2000], Train Loss: 0.4367\n","Epoch [241/2000], Train Loss: 0.4364\n","Epoch [242/2000], Train Loss: 0.4366\n","Epoch [243/2000], Train Loss: 0.4378\n","Epoch [244/2000], Train Loss: 0.4400\n","Epoch [245/2000], Train Loss: 0.4376\n","Epoch [246/2000], Train Loss: 0.4348\n","Epoch [247/2000], Train Loss: 0.4361\n","Epoch [248/2000], Train Loss: 0.4374\n","Epoch [249/2000], Train Loss: 0.4360\n","Epoch [250/2000], Train Loss: 0.4348\n","Epoch [251/2000], Train Loss: 0.4355\n","Epoch [252/2000], Train Loss: 0.4358\n","Epoch [253/2000], Train Loss: 0.4345\n","Epoch [254/2000], Train Loss: 0.4339\n","Epoch [255/2000], Train Loss: 0.4356\n","Epoch [256/2000], Train Loss: 0.4370\n","Epoch [257/2000], Train Loss: 0.4365\n","Epoch [258/2000], Train Loss: 0.4345\n","Epoch [259/2000], Train Loss: 0.4345\n","Epoch [260/2000], Train Loss: 0.4350\n","Epoch [261/2000], Train Loss: 0.4336\n","Epoch [262/2000], Train Loss: 0.4333\n","Epoch [263/2000], Train Loss: 0.4341\n","Epoch [264/2000], Train Loss: 0.4311\n","Epoch [265/2000], Train Loss: 0.4309\n","Epoch [266/2000], Train Loss: 0.4321\n","Epoch [267/2000], Train Loss: 0.4320\n","Epoch [268/2000], Train Loss: 0.4302\n","Epoch [269/2000], Train Loss: 0.4332\n","Epoch [270/2000], Train Loss: 0.4308\n","Epoch [271/2000], Train Loss: 0.4302\n","Epoch [272/2000], Train Loss: 0.4310\n","Epoch [273/2000], Train Loss: 0.4320\n","Epoch [274/2000], Train Loss: 0.4312\n","Epoch [275/2000], Train Loss: 0.4294\n","Epoch [276/2000], Train Loss: 0.4301\n","Epoch [277/2000], Train Loss: 0.4284\n","Epoch [278/2000], Train Loss: 0.4303\n","Epoch [279/2000], Train Loss: 0.4280\n","Epoch [280/2000], Train Loss: 0.4294\n","Epoch [281/2000], Train Loss: 0.4301\n","Epoch [282/2000], Train Loss: 0.4315\n","Epoch [283/2000], Train Loss: 0.4268\n","Epoch [284/2000], Train Loss: 0.4290\n","Epoch [285/2000], Train Loss: 0.4256\n","Epoch [286/2000], Train Loss: 0.4255\n","Epoch [287/2000], Train Loss: 0.4283\n","Epoch [288/2000], Train Loss: 0.4258\n","Epoch [289/2000], Train Loss: 0.4298\n","Epoch [290/2000], Train Loss: 0.4272\n","Epoch [291/2000], Train Loss: 0.4268\n","Epoch [292/2000], Train Loss: 0.4280\n","Epoch [293/2000], Train Loss: 0.4274\n","Epoch [294/2000], Train Loss: 0.4277\n","Epoch [295/2000], Train Loss: 0.4269\n","Epoch [296/2000], Train Loss: 0.4264\n","Epoch [297/2000], Train Loss: 0.4263\n","Epoch [298/2000], Train Loss: 0.4261\n","Epoch [299/2000], Train Loss: 0.4258\n","Epoch [300/2000], Train Loss: 0.4242\n","Epoch [301/2000], Train Loss: 0.4256\n","Epoch [302/2000], Train Loss: 0.4251\n","Epoch [303/2000], Train Loss: 0.4252\n","Epoch [304/2000], Train Loss: 0.4228\n","Epoch [305/2000], Train Loss: 0.4233\n","Epoch [306/2000], Train Loss: 0.4243\n","Epoch [307/2000], Train Loss: 0.4238\n","Epoch [308/2000], Train Loss: 0.4227\n","Epoch [309/2000], Train Loss: 0.4246\n","Epoch [310/2000], Train Loss: 0.4237\n","Epoch [311/2000], Train Loss: 0.4224\n","Epoch [312/2000], Train Loss: 0.4219\n","Epoch [313/2000], Train Loss: 0.4239\n","Epoch [314/2000], Train Loss: 0.4258\n","Epoch [315/2000], Train Loss: 0.4237\n","Epoch [316/2000], Train Loss: 0.4250\n","Epoch [317/2000], Train Loss: 0.4217\n","Epoch [318/2000], Train Loss: 0.4231\n","Epoch [319/2000], Train Loss: 0.4221\n","Epoch [320/2000], Train Loss: 0.4212\n","Epoch [321/2000], Train Loss: 0.4193\n","Epoch [322/2000], Train Loss: 0.4194\n","Epoch [323/2000], Train Loss: 0.4207\n","Epoch [324/2000], Train Loss: 0.4211\n","Epoch [325/2000], Train Loss: 0.4195\n","Epoch [326/2000], Train Loss: 0.4207\n","Epoch [327/2000], Train Loss: 0.4199\n","Epoch [328/2000], Train Loss: 0.4199\n","Epoch [329/2000], Train Loss: 0.4209\n","Epoch [330/2000], Train Loss: 0.4203\n","Epoch [331/2000], Train Loss: 0.4182\n","Epoch [332/2000], Train Loss: 0.4173\n","Epoch [333/2000], Train Loss: 0.4208\n","Epoch [334/2000], Train Loss: 0.4184\n","Epoch [335/2000], Train Loss: 0.4202\n","Epoch [336/2000], Train Loss: 0.4167\n","Epoch [337/2000], Train Loss: 0.4194\n","Epoch [338/2000], Train Loss: 0.4184\n","Epoch [339/2000], Train Loss: 0.4198\n","Epoch [340/2000], Train Loss: 0.4189\n","Epoch [341/2000], Train Loss: 0.4177\n","Epoch [342/2000], Train Loss: 0.4177\n","Epoch [343/2000], Train Loss: 0.4183\n","Epoch [344/2000], Train Loss: 0.4164\n","Epoch [345/2000], Train Loss: 0.4164\n","Epoch [346/2000], Train Loss: 0.4163\n","Epoch [347/2000], Train Loss: 0.4181\n","Epoch [348/2000], Train Loss: 0.4164\n","Epoch [349/2000], Train Loss: 0.4163\n","Epoch [350/2000], Train Loss: 0.4150\n","Epoch [351/2000], Train Loss: 0.4184\n","Epoch [352/2000], Train Loss: 0.4174\n","Epoch [353/2000], Train Loss: 0.4151\n","Epoch [354/2000], Train Loss: 0.4159\n","Epoch [355/2000], Train Loss: 0.4165\n","Epoch [356/2000], Train Loss: 0.4145\n","Epoch [357/2000], Train Loss: 0.4147\n","Epoch [358/2000], Train Loss: 0.4160\n","Epoch [359/2000], Train Loss: 0.4166\n","Epoch [360/2000], Train Loss: 0.4158\n","Epoch [361/2000], Train Loss: 0.4152\n","Epoch [362/2000], Train Loss: 0.4149\n","Epoch [363/2000], Train Loss: 0.4148\n","Epoch [364/2000], Train Loss: 0.4165\n","Epoch [365/2000], Train Loss: 0.4134\n","Epoch [366/2000], Train Loss: 0.4167\n","Epoch [367/2000], Train Loss: 0.4147\n","Epoch [368/2000], Train Loss: 0.4146\n","Epoch [369/2000], Train Loss: 0.4158\n","Epoch [370/2000], Train Loss: 0.4142\n","Epoch [371/2000], Train Loss: 0.4144\n","Epoch [372/2000], Train Loss: 0.4127\n","Epoch [373/2000], Train Loss: 0.4166\n","Epoch [374/2000], Train Loss: 0.4155\n","Epoch [375/2000], Train Loss: 0.4138\n","Epoch [376/2000], Train Loss: 0.4135\n","Epoch [377/2000], Train Loss: 0.4120\n","Epoch [378/2000], Train Loss: 0.4124\n","Epoch [379/2000], Train Loss: 0.4130\n","Epoch [380/2000], Train Loss: 0.4127\n","Epoch [381/2000], Train Loss: 0.4119\n","Epoch [382/2000], Train Loss: 0.4137\n","Epoch [383/2000], Train Loss: 0.4140\n","Epoch [384/2000], Train Loss: 0.4120\n","Epoch [385/2000], Train Loss: 0.4148\n","Epoch [386/2000], Train Loss: 0.4151\n","Epoch [387/2000], Train Loss: 0.4141\n","Epoch [388/2000], Train Loss: 0.4104\n","Epoch [389/2000], Train Loss: 0.4129\n","Epoch [390/2000], Train Loss: 0.4113\n","Epoch [391/2000], Train Loss: 0.4128\n","Epoch [392/2000], Train Loss: 0.4105\n","Epoch [393/2000], Train Loss: 0.4129\n","Epoch [394/2000], Train Loss: 0.4129\n","Epoch [395/2000], Train Loss: 0.4121\n","Epoch [396/2000], Train Loss: 0.4109\n","Epoch [397/2000], Train Loss: 0.4120\n","Epoch [398/2000], Train Loss: 0.4102\n","Epoch [399/2000], Train Loss: 0.4115\n","Epoch [400/2000], Train Loss: 0.4143\n","Epoch [401/2000], Train Loss: 0.4126\n","Epoch [402/2000], Train Loss: 0.4109\n","Epoch [403/2000], Train Loss: 0.4102\n","Epoch [404/2000], Train Loss: 0.4098\n","Epoch [405/2000], Train Loss: 0.4093\n","Epoch [406/2000], Train Loss: 0.4080\n","Epoch [407/2000], Train Loss: 0.4093\n","Epoch [408/2000], Train Loss: 0.4105\n","Epoch [409/2000], Train Loss: 0.4126\n","Epoch [410/2000], Train Loss: 0.4117\n","Epoch [411/2000], Train Loss: 0.4097\n","Epoch [412/2000], Train Loss: 0.4098\n","Epoch [413/2000], Train Loss: 0.4112\n","Epoch [414/2000], Train Loss: 0.4123\n","Epoch [415/2000], Train Loss: 0.4094\n","Epoch [416/2000], Train Loss: 0.4086\n","Epoch [417/2000], Train Loss: 0.4090\n","Epoch [418/2000], Train Loss: 0.4092\n","Epoch [419/2000], Train Loss: 0.4088\n","Epoch [420/2000], Train Loss: 0.4090\n","Epoch [421/2000], Train Loss: 0.4108\n","Epoch [422/2000], Train Loss: 0.4118\n","Epoch [423/2000], Train Loss: 0.4121\n","Epoch [424/2000], Train Loss: 0.4111\n","Epoch [425/2000], Train Loss: 0.4070\n","Epoch [426/2000], Train Loss: 0.4080\n","Epoch [427/2000], Train Loss: 0.4087\n","Epoch [428/2000], Train Loss: 0.4090\n","Epoch [429/2000], Train Loss: 0.4062\n","Epoch [430/2000], Train Loss: 0.4080\n","Epoch [431/2000], Train Loss: 0.4100\n","Epoch [432/2000], Train Loss: 0.4094\n","Epoch [433/2000], Train Loss: 0.4067\n","Epoch [434/2000], Train Loss: 0.4077\n","Epoch [435/2000], Train Loss: 0.4068\n","Epoch [436/2000], Train Loss: 0.4068\n","Epoch [437/2000], Train Loss: 0.4068\n","Epoch [438/2000], Train Loss: 0.4063\n","Epoch [439/2000], Train Loss: 0.4076\n","Epoch [440/2000], Train Loss: 0.4066\n","Epoch [441/2000], Train Loss: 0.4062\n","Epoch [442/2000], Train Loss: 0.4090\n","Epoch [443/2000], Train Loss: 0.4081\n","Epoch [444/2000], Train Loss: 0.4116\n","Epoch [445/2000], Train Loss: 0.4051\n","Epoch [446/2000], Train Loss: 0.4058\n","Epoch [447/2000], Train Loss: 0.4050\n","Epoch [448/2000], Train Loss: 0.4059\n","Epoch [449/2000], Train Loss: 0.4063\n","Epoch [450/2000], Train Loss: 0.4050\n","Epoch [451/2000], Train Loss: 0.4057\n","Epoch [452/2000], Train Loss: 0.4051\n","Epoch [453/2000], Train Loss: 0.4078\n","Epoch [454/2000], Train Loss: 0.4074\n","Epoch [455/2000], Train Loss: 0.4065\n","Epoch [456/2000], Train Loss: 0.4059\n","Epoch [457/2000], Train Loss: 0.4070\n","Epoch [458/2000], Train Loss: 0.4069\n","Epoch [459/2000], Train Loss: 0.4073\n","Epoch [460/2000], Train Loss: 0.4054\n","Epoch [461/2000], Train Loss: 0.4055\n","Epoch [462/2000], Train Loss: 0.4075\n","Epoch [463/2000], Train Loss: 0.4062\n","Epoch [464/2000], Train Loss: 0.4070\n","Epoch [465/2000], Train Loss: 0.4060\n","Epoch [466/2000], Train Loss: 0.4072\n","Epoch [467/2000], Train Loss: 0.4081\n","Epoch [468/2000], Train Loss: 0.4023\n","Epoch [469/2000], Train Loss: 0.4052\n","Epoch [470/2000], Train Loss: 0.4046\n","Epoch [471/2000], Train Loss: 0.4056\n","Epoch [472/2000], Train Loss: 0.4042\n","Epoch [473/2000], Train Loss: 0.4051\n","Epoch [474/2000], Train Loss: 0.4052\n","Epoch [475/2000], Train Loss: 0.4038\n","Epoch [476/2000], Train Loss: 0.4066\n","Epoch [477/2000], Train Loss: 0.4011\n","Epoch [478/2000], Train Loss: 0.4042\n","Epoch [479/2000], Train Loss: 0.4030\n","Epoch [480/2000], Train Loss: 0.4033\n","Epoch [481/2000], Train Loss: 0.4052\n","Epoch [482/2000], Train Loss: 0.4018\n","Epoch [483/2000], Train Loss: 0.4056\n","Epoch [484/2000], Train Loss: 0.4026\n","Epoch [485/2000], Train Loss: 0.4044\n","Epoch [486/2000], Train Loss: 0.4032\n","Epoch [487/2000], Train Loss: 0.4032\n","Epoch [488/2000], Train Loss: 0.4026\n","Epoch [489/2000], Train Loss: 0.4042\n","Epoch [490/2000], Train Loss: 0.4042\n","Epoch [491/2000], Train Loss: 0.4028\n","Epoch [492/2000], Train Loss: 0.4029\n","Epoch [493/2000], Train Loss: 0.4036\n","Epoch [494/2000], Train Loss: 0.4022\n","Epoch [495/2000], Train Loss: 0.4001\n","Epoch [496/2000], Train Loss: 0.4026\n","Epoch [497/2000], Train Loss: 0.4036\n","Epoch [498/2000], Train Loss: 0.4016\n","Epoch [499/2000], Train Loss: 0.4026\n","Epoch [500/2000], Train Loss: 0.4006\n","Epoch [501/2000], Train Loss: 0.4021\n","Epoch [502/2000], Train Loss: 0.4020\n","Epoch [503/2000], Train Loss: 0.4015\n","Epoch [504/2000], Train Loss: 0.4028\n","Epoch [505/2000], Train Loss: 0.4039\n","Epoch [506/2000], Train Loss: 0.4017\n","Epoch [507/2000], Train Loss: 0.4018\n","Epoch [508/2000], Train Loss: 0.4044\n","Epoch [509/2000], Train Loss: 0.4004\n","Epoch [510/2000], Train Loss: 0.4004\n","Epoch [511/2000], Train Loss: 0.4029\n","Epoch [512/2000], Train Loss: 0.4022\n","Epoch [513/2000], Train Loss: 0.3984\n","Epoch [514/2000], Train Loss: 0.4016\n","Epoch [515/2000], Train Loss: 0.4031\n","Epoch [516/2000], Train Loss: 0.4024\n","Epoch [517/2000], Train Loss: 0.4026\n","Epoch [518/2000], Train Loss: 0.4022\n","Epoch [519/2000], Train Loss: 0.4001\n","Epoch [520/2000], Train Loss: 0.4028\n","Epoch [521/2000], Train Loss: 0.4015\n","Epoch [522/2000], Train Loss: 0.4026\n","Epoch [523/2000], Train Loss: 0.4011\n","Epoch [524/2000], Train Loss: 0.4012\n","Epoch [525/2000], Train Loss: 0.3976\n","Epoch [526/2000], Train Loss: 0.4004\n","Epoch [527/2000], Train Loss: 0.4016\n","Epoch [528/2000], Train Loss: 0.4005\n","Epoch [529/2000], Train Loss: 0.4005\n","Epoch [530/2000], Train Loss: 0.4040\n","Epoch [531/2000], Train Loss: 0.4002\n","Epoch [532/2000], Train Loss: 0.4020\n","Epoch [533/2000], Train Loss: 0.4007\n","Epoch [534/2000], Train Loss: 0.4006\n","Epoch [535/2000], Train Loss: 0.4001\n","Epoch [536/2000], Train Loss: 0.3986\n","Epoch [537/2000], Train Loss: 0.4031\n","Epoch [538/2000], Train Loss: 0.3984\n","Epoch [539/2000], Train Loss: 0.4002\n","Epoch [540/2000], Train Loss: 0.3993\n","Epoch [541/2000], Train Loss: 0.4026\n","Epoch [542/2000], Train Loss: 0.3988\n","Epoch [543/2000], Train Loss: 0.3999\n","Epoch [544/2000], Train Loss: 0.3985\n","Epoch [545/2000], Train Loss: 0.4017\n","Epoch [546/2000], Train Loss: 0.4004\n","Epoch [547/2000], Train Loss: 0.3990\n","Epoch [548/2000], Train Loss: 0.3986\n","Epoch [549/2000], Train Loss: 0.4009\n","Epoch [550/2000], Train Loss: 0.3994\n","Epoch [551/2000], Train Loss: 0.3987\n","Epoch [552/2000], Train Loss: 0.3994\n","Epoch [553/2000], Train Loss: 0.3967\n","Epoch [554/2000], Train Loss: 0.3981\n","Epoch [555/2000], Train Loss: 0.3979\n","Epoch [556/2000], Train Loss: 0.3978\n","Epoch [557/2000], Train Loss: 0.4022\n","Epoch [558/2000], Train Loss: 0.3986\n","Epoch [559/2000], Train Loss: 0.3972\n","Epoch [560/2000], Train Loss: 0.3967\n","Epoch [561/2000], Train Loss: 0.3988\n","Epoch [562/2000], Train Loss: 0.3957\n","Epoch [563/2000], Train Loss: 0.4006\n","Epoch [564/2000], Train Loss: 0.3988\n","Epoch [565/2000], Train Loss: 0.3994\n","Epoch [566/2000], Train Loss: 0.3969\n","Epoch [567/2000], Train Loss: 0.3968\n","Epoch [568/2000], Train Loss: 0.3964\n","Epoch [569/2000], Train Loss: 0.3964\n","Epoch [570/2000], Train Loss: 0.3977\n","Epoch [571/2000], Train Loss: 0.3978\n","Epoch [572/2000], Train Loss: 0.3975\n","Epoch [573/2000], Train Loss: 0.3972\n","Epoch [574/2000], Train Loss: 0.3991\n","Epoch [575/2000], Train Loss: 0.3985\n","Epoch [576/2000], Train Loss: 0.3998\n","Epoch [577/2000], Train Loss: 0.3989\n","Epoch [578/2000], Train Loss: 0.3998\n","Epoch [579/2000], Train Loss: 0.3964\n","Epoch [580/2000], Train Loss: 0.3965\n","Epoch [581/2000], Train Loss: 0.3993\n","Epoch [582/2000], Train Loss: 0.3975\n","Epoch [583/2000], Train Loss: 0.4010\n","Epoch [584/2000], Train Loss: 0.3950\n","Epoch [585/2000], Train Loss: 0.3948\n","Epoch [586/2000], Train Loss: 0.3966\n","Epoch [587/2000], Train Loss: 0.3963\n","Epoch [588/2000], Train Loss: 0.3968\n","Epoch [589/2000], Train Loss: 0.3947\n","Epoch [590/2000], Train Loss: 0.3963\n","Epoch [591/2000], Train Loss: 0.3977\n","Epoch [592/2000], Train Loss: 0.3958\n","Epoch [593/2000], Train Loss: 0.3923\n","Epoch [594/2000], Train Loss: 0.3965\n","Epoch [595/2000], Train Loss: 0.3991\n","Epoch [596/2000], Train Loss: 0.3956\n","Epoch [597/2000], Train Loss: 0.3952\n","Epoch [598/2000], Train Loss: 0.3963\n","Epoch [599/2000], Train Loss: 0.3952\n","Epoch [600/2000], Train Loss: 0.3966\n","Epoch [601/2000], Train Loss: 0.3959\n","Epoch [602/2000], Train Loss: 0.3953\n","Epoch [603/2000], Train Loss: 0.3959\n","Epoch [604/2000], Train Loss: 0.3944\n","Epoch [605/2000], Train Loss: 0.3989\n","Epoch [606/2000], Train Loss: 0.3964\n","Epoch [607/2000], Train Loss: 0.3951\n","Epoch [608/2000], Train Loss: 0.3951\n","Epoch [609/2000], Train Loss: 0.3915\n","Epoch [610/2000], Train Loss: 0.3958\n","Epoch [611/2000], Train Loss: 0.3963\n","Epoch [612/2000], Train Loss: 0.3930\n","Epoch [613/2000], Train Loss: 0.3956\n","Epoch [614/2000], Train Loss: 0.3947\n","Epoch [615/2000], Train Loss: 0.3963\n","Epoch [616/2000], Train Loss: 0.3954\n","Epoch [617/2000], Train Loss: 0.3946\n","Epoch [618/2000], Train Loss: 0.3952\n","Epoch [619/2000], Train Loss: 0.3951\n","Epoch [620/2000], Train Loss: 0.3953\n","Epoch [621/2000], Train Loss: 0.3952\n","Epoch [622/2000], Train Loss: 0.3959\n","Epoch [623/2000], Train Loss: 0.3944\n","Epoch [624/2000], Train Loss: 0.3922\n","Epoch [625/2000], Train Loss: 0.3931\n","Epoch [626/2000], Train Loss: 0.3937\n","Epoch [627/2000], Train Loss: 0.3939\n","Epoch [628/2000], Train Loss: 0.3934\n","Epoch [629/2000], Train Loss: 0.3927\n","Epoch [630/2000], Train Loss: 0.3926\n","Epoch [631/2000], Train Loss: 0.3935\n","Epoch [632/2000], Train Loss: 0.3942\n","Epoch [633/2000], Train Loss: 0.3930\n","Epoch [634/2000], Train Loss: 0.3953\n","Epoch [635/2000], Train Loss: 0.3942\n","Epoch [636/2000], Train Loss: 0.3949\n","Epoch [637/2000], Train Loss: 0.3947\n","Epoch [638/2000], Train Loss: 0.3939\n","Epoch [639/2000], Train Loss: 0.3931\n","Epoch [640/2000], Train Loss: 0.3919\n","Epoch [641/2000], Train Loss: 0.3932\n","Epoch [642/2000], Train Loss: 0.3921\n","Epoch [643/2000], Train Loss: 0.3937\n","Epoch [644/2000], Train Loss: 0.3947\n","Epoch [645/2000], Train Loss: 0.3954\n","Epoch [646/2000], Train Loss: 0.3919\n","Epoch [647/2000], Train Loss: 0.3925\n","Epoch [648/2000], Train Loss: 0.3910\n","Epoch [649/2000], Train Loss: 0.3910\n","Epoch [650/2000], Train Loss: 0.3940\n","Epoch [651/2000], Train Loss: 0.3916\n","Epoch [652/2000], Train Loss: 0.3922\n","Epoch [653/2000], Train Loss: 0.3926\n","Epoch [654/2000], Train Loss: 0.3931\n","Epoch [655/2000], Train Loss: 0.3912\n","Epoch [656/2000], Train Loss: 0.3905\n","Epoch [657/2000], Train Loss: 0.3940\n","Epoch [658/2000], Train Loss: 0.3944\n","Epoch [659/2000], Train Loss: 0.3914\n","Epoch [660/2000], Train Loss: 0.3916\n","Epoch [661/2000], Train Loss: 0.3940\n","Epoch [662/2000], Train Loss: 0.3915\n","Epoch [663/2000], Train Loss: 0.3921\n","Epoch [664/2000], Train Loss: 0.3911\n","Epoch [665/2000], Train Loss: 0.3944\n","Epoch [666/2000], Train Loss: 0.3916\n","Epoch [667/2000], Train Loss: 0.3917\n","Epoch [668/2000], Train Loss: 0.3910\n","Epoch [669/2000], Train Loss: 0.3914\n","Epoch [670/2000], Train Loss: 0.3932\n","Epoch [671/2000], Train Loss: 0.3893\n","Epoch [672/2000], Train Loss: 0.3937\n","Epoch [673/2000], Train Loss: 0.3915\n","Epoch [674/2000], Train Loss: 0.3909\n","Epoch [675/2000], Train Loss: 0.3929\n","Epoch [676/2000], Train Loss: 0.3911\n","Epoch [677/2000], Train Loss: 0.3935\n","Epoch [678/2000], Train Loss: 0.3904\n","Epoch [679/2000], Train Loss: 0.3895\n","Epoch [680/2000], Train Loss: 0.3905\n","Epoch [681/2000], Train Loss: 0.3913\n","Epoch [682/2000], Train Loss: 0.3915\n","Epoch [683/2000], Train Loss: 0.3905\n","Epoch [684/2000], Train Loss: 0.3932\n","Epoch [685/2000], Train Loss: 0.3905\n","Epoch [686/2000], Train Loss: 0.3933\n","Epoch [687/2000], Train Loss: 0.3903\n","Epoch [688/2000], Train Loss: 0.3930\n","Epoch [689/2000], Train Loss: 0.3905\n","Epoch [690/2000], Train Loss: 0.3891\n","Epoch [691/2000], Train Loss: 0.3901\n","Epoch [692/2000], Train Loss: 0.3906\n","Epoch [693/2000], Train Loss: 0.3890\n","Epoch [694/2000], Train Loss: 0.3884\n","Epoch [695/2000], Train Loss: 0.3917\n","Epoch [696/2000], Train Loss: 0.3886\n","Epoch [697/2000], Train Loss: 0.3899\n","Epoch [698/2000], Train Loss: 0.3875\n","Epoch [699/2000], Train Loss: 0.3916\n","Epoch [700/2000], Train Loss: 0.3916\n","Epoch [701/2000], Train Loss: 0.3891\n","Epoch [702/2000], Train Loss: 0.3909\n","Epoch [703/2000], Train Loss: 0.3872\n","Epoch [704/2000], Train Loss: 0.3885\n","Epoch [705/2000], Train Loss: 0.3902\n","Epoch [706/2000], Train Loss: 0.3908\n","Epoch [707/2000], Train Loss: 0.3896\n","Epoch [708/2000], Train Loss: 0.3913\n","Epoch [709/2000], Train Loss: 0.3903\n","Epoch [710/2000], Train Loss: 0.3884\n","Epoch [711/2000], Train Loss: 0.3890\n","Epoch [712/2000], Train Loss: 0.3908\n","Epoch [713/2000], Train Loss: 0.3898\n","Epoch [714/2000], Train Loss: 0.3883\n","Epoch [715/2000], Train Loss: 0.3901\n","Epoch [716/2000], Train Loss: 0.3899\n","Epoch [717/2000], Train Loss: 0.3897\n","Epoch [718/2000], Train Loss: 0.3910\n","Epoch [719/2000], Train Loss: 0.3922\n","Epoch [720/2000], Train Loss: 0.3873\n","Epoch [721/2000], Train Loss: 0.3884\n","Epoch [722/2000], Train Loss: 0.3886\n","Epoch [723/2000], Train Loss: 0.3874\n","Epoch [724/2000], Train Loss: 0.3867\n","Epoch [725/2000], Train Loss: 0.3910\n","Epoch [726/2000], Train Loss: 0.3894\n","Epoch [727/2000], Train Loss: 0.3889\n","Epoch [728/2000], Train Loss: 0.3881\n","Epoch [729/2000], Train Loss: 0.3864\n","Epoch [730/2000], Train Loss: 0.3918\n","Epoch [731/2000], Train Loss: 0.3863\n","Epoch [732/2000], Train Loss: 0.3867\n","Epoch [733/2000], Train Loss: 0.3889\n","Epoch [734/2000], Train Loss: 0.3858\n","Epoch [735/2000], Train Loss: 0.3867\n","Epoch [736/2000], Train Loss: 0.3880\n","Epoch [737/2000], Train Loss: 0.3858\n","Epoch [738/2000], Train Loss: 0.3956\n","Epoch [739/2000], Train Loss: 0.3908\n","Epoch [740/2000], Train Loss: 0.3863\n","Epoch [741/2000], Train Loss: 0.3893\n","Epoch [742/2000], Train Loss: 0.3882\n","Epoch [743/2000], Train Loss: 0.3880\n","Epoch [744/2000], Train Loss: 0.3868\n","Epoch [745/2000], Train Loss: 0.3884\n","Epoch [746/2000], Train Loss: 0.3869\n","Epoch [747/2000], Train Loss: 0.3880\n","Epoch [748/2000], Train Loss: 0.3849\n","Epoch [749/2000], Train Loss: 0.3869\n","Epoch [750/2000], Train Loss: 0.3875\n","Epoch [751/2000], Train Loss: 0.3882\n","Epoch [752/2000], Train Loss: 0.3873\n","Epoch [753/2000], Train Loss: 0.3880\n","Epoch [754/2000], Train Loss: 0.3873\n","Epoch [755/2000], Train Loss: 0.3863\n","Epoch [756/2000], Train Loss: 0.3837\n","Epoch [757/2000], Train Loss: 0.3884\n","Epoch [758/2000], Train Loss: 0.3866\n","Epoch [759/2000], Train Loss: 0.3861\n","Epoch [760/2000], Train Loss: 0.3870\n","Epoch [761/2000], Train Loss: 0.3871\n","Epoch [762/2000], Train Loss: 0.3841\n","Epoch [763/2000], Train Loss: 0.3846\n","Epoch [764/2000], Train Loss: 0.3863\n","Epoch [765/2000], Train Loss: 0.3864\n","Epoch [766/2000], Train Loss: 0.3908\n","Epoch [767/2000], Train Loss: 0.3873\n","Epoch [768/2000], Train Loss: 0.3879\n","Epoch [769/2000], Train Loss: 0.3852\n","Epoch [770/2000], Train Loss: 0.3875\n","Epoch [771/2000], Train Loss: 0.3837\n","Epoch [772/2000], Train Loss: 0.3888\n","Epoch [773/2000], Train Loss: 0.3889\n","Epoch [774/2000], Train Loss: 0.3849\n","Epoch [775/2000], Train Loss: 0.3861\n","Epoch [776/2000], Train Loss: 0.3866\n","Epoch [777/2000], Train Loss: 0.3873\n","Epoch [778/2000], Train Loss: 0.3847\n","Epoch [779/2000], Train Loss: 0.3867\n","Epoch [780/2000], Train Loss: 0.3871\n","Epoch [781/2000], Train Loss: 0.3868\n","Epoch [782/2000], Train Loss: 0.3874\n","Epoch [783/2000], Train Loss: 0.3852\n","Epoch [784/2000], Train Loss: 0.3861\n","Epoch [785/2000], Train Loss: 0.3845\n","Epoch [786/2000], Train Loss: 0.3808\n","Epoch [787/2000], Train Loss: 0.3862\n","Epoch [788/2000], Train Loss: 0.3869\n","Epoch [789/2000], Train Loss: 0.3859\n","Epoch [790/2000], Train Loss: 0.3828\n","Epoch [791/2000], Train Loss: 0.3878\n","Epoch [792/2000], Train Loss: 0.3869\n","Epoch [793/2000], Train Loss: 0.3861\n","Epoch [794/2000], Train Loss: 0.3830\n","Epoch [795/2000], Train Loss: 0.3842\n","Epoch [796/2000], Train Loss: 0.3852\n","Epoch [797/2000], Train Loss: 0.3912\n","Epoch [798/2000], Train Loss: 0.3871\n","Epoch [799/2000], Train Loss: 0.3851\n","Epoch [800/2000], Train Loss: 0.3845\n","Epoch [801/2000], Train Loss: 0.3851\n","Epoch [802/2000], Train Loss: 0.3850\n","Epoch [803/2000], Train Loss: 0.3834\n","Epoch [804/2000], Train Loss: 0.3837\n","Epoch [805/2000], Train Loss: 0.3858\n","Epoch [806/2000], Train Loss: 0.3855\n","Epoch [807/2000], Train Loss: 0.3903\n","Epoch [808/2000], Train Loss: 0.3855\n","Epoch [809/2000], Train Loss: 0.3855\n","Epoch [810/2000], Train Loss: 0.3870\n","Epoch [811/2000], Train Loss: 0.3851\n","Epoch [812/2000], Train Loss: 0.3849\n","Epoch [813/2000], Train Loss: 0.3828\n","Epoch [814/2000], Train Loss: 0.3863\n","Epoch [815/2000], Train Loss: 0.3834\n","Epoch [816/2000], Train Loss: 0.3832\n","Epoch [817/2000], Train Loss: 0.3837\n","Epoch [818/2000], Train Loss: 0.3840\n","Epoch [819/2000], Train Loss: 0.3861\n","Epoch [820/2000], Train Loss: 0.3867\n","Epoch [821/2000], Train Loss: 0.3827\n","Epoch [822/2000], Train Loss: 0.3837\n","Epoch [823/2000], Train Loss: 0.3857\n","Epoch [824/2000], Train Loss: 0.3849\n","Epoch [825/2000], Train Loss: 0.3840\n","Epoch [826/2000], Train Loss: 0.3852\n","Epoch [827/2000], Train Loss: 0.3834\n","Epoch [828/2000], Train Loss: 0.3848\n","Epoch [829/2000], Train Loss: 0.3854\n","Epoch [830/2000], Train Loss: 0.3841\n","Epoch [831/2000], Train Loss: 0.3853\n","Epoch [832/2000], Train Loss: 0.3821\n","Epoch [833/2000], Train Loss: 0.3842\n","Epoch [834/2000], Train Loss: 0.3852\n","Epoch [835/2000], Train Loss: 0.3851\n","Epoch [836/2000], Train Loss: 0.3834\n","Epoch [837/2000], Train Loss: 0.3844\n","Epoch [838/2000], Train Loss: 0.3840\n","Epoch [839/2000], Train Loss: 0.3853\n","Epoch [840/2000], Train Loss: 0.3832\n","Epoch [841/2000], Train Loss: 0.3825\n","Epoch [842/2000], Train Loss: 0.3842\n","Epoch [843/2000], Train Loss: 0.3842\n","Epoch [844/2000], Train Loss: 0.3801\n","Epoch [845/2000], Train Loss: 0.3868\n","Epoch [846/2000], Train Loss: 0.3837\n","Epoch [847/2000], Train Loss: 0.3830\n","Epoch [848/2000], Train Loss: 0.3850\n","Epoch [849/2000], Train Loss: 0.3824\n","Epoch [850/2000], Train Loss: 0.3796\n","Epoch [851/2000], Train Loss: 0.3809\n","Epoch [852/2000], Train Loss: 0.3830\n","Epoch [853/2000], Train Loss: 0.3826\n","Epoch [854/2000], Train Loss: 0.3833\n","Epoch [855/2000], Train Loss: 0.3849\n","Epoch [856/2000], Train Loss: 0.3843\n","Epoch [857/2000], Train Loss: 0.3838\n","Epoch [858/2000], Train Loss: 0.3830\n","Epoch [859/2000], Train Loss: 0.3824\n","Epoch [860/2000], Train Loss: 0.3827\n","Epoch [861/2000], Train Loss: 0.3834\n","Epoch [862/2000], Train Loss: 0.3832\n","Epoch [863/2000], Train Loss: 0.3834\n","Epoch [864/2000], Train Loss: 0.3853\n","Epoch [865/2000], Train Loss: 0.3866\n","Epoch [866/2000], Train Loss: 0.3821\n","Epoch [867/2000], Train Loss: 0.3823\n","Epoch [868/2000], Train Loss: 0.3841\n","Epoch [869/2000], Train Loss: 0.3828\n","Epoch [870/2000], Train Loss: 0.3804\n","Epoch [871/2000], Train Loss: 0.3843\n","Epoch [872/2000], Train Loss: 0.3833\n","Epoch [873/2000], Train Loss: 0.3849\n","Epoch [874/2000], Train Loss: 0.3847\n","Epoch [875/2000], Train Loss: 0.3823\n","Epoch [876/2000], Train Loss: 0.3833\n","Epoch [877/2000], Train Loss: 0.3834\n","Epoch [878/2000], Train Loss: 0.3820\n","Epoch [879/2000], Train Loss: 0.3891\n","Epoch [880/2000], Train Loss: 0.3803\n","Epoch [881/2000], Train Loss: 0.3836\n","Epoch [882/2000], Train Loss: 0.3836\n","Epoch [883/2000], Train Loss: 0.3818\n","Epoch [884/2000], Train Loss: 0.3799\n","Epoch [885/2000], Train Loss: 0.3818\n","Epoch [886/2000], Train Loss: 0.3809\n","Epoch [887/2000], Train Loss: 0.3841\n","Epoch [888/2000], Train Loss: 0.3816\n","Epoch [889/2000], Train Loss: 0.3803\n","Epoch [890/2000], Train Loss: 0.3829\n","Epoch [891/2000], Train Loss: 0.3810\n","Epoch [892/2000], Train Loss: 0.3803\n","Epoch [893/2000], Train Loss: 0.3836\n","Epoch [894/2000], Train Loss: 0.3824\n","Epoch [895/2000], Train Loss: 0.3825\n","Epoch [896/2000], Train Loss: 0.3808\n","Epoch [897/2000], Train Loss: 0.3829\n","Epoch [898/2000], Train Loss: 0.3810\n","Epoch [899/2000], Train Loss: 0.3821\n","Epoch [900/2000], Train Loss: 0.3823\n","Epoch [901/2000], Train Loss: 0.3822\n","Epoch [902/2000], Train Loss: 0.3810\n","Epoch [903/2000], Train Loss: 0.3821\n","Epoch [904/2000], Train Loss: 0.3799\n","Epoch [905/2000], Train Loss: 0.3806\n","Epoch [906/2000], Train Loss: 0.3818\n","Epoch [907/2000], Train Loss: 0.3812\n","Epoch [908/2000], Train Loss: 0.3804\n","Epoch [909/2000], Train Loss: 0.3850\n","Epoch [910/2000], Train Loss: 0.3858\n","Epoch [911/2000], Train Loss: 0.3824\n","Epoch [912/2000], Train Loss: 0.3818\n","Epoch [913/2000], Train Loss: 0.3812\n","Epoch [914/2000], Train Loss: 0.3809\n","Epoch [915/2000], Train Loss: 0.3794\n","Epoch [916/2000], Train Loss: 0.3792\n","Epoch [917/2000], Train Loss: 0.3818\n","Epoch [918/2000], Train Loss: 0.3780\n","Epoch [919/2000], Train Loss: 0.3804\n","Epoch [920/2000], Train Loss: 0.3800\n","Epoch [921/2000], Train Loss: 0.3779\n","Epoch [922/2000], Train Loss: 0.3805\n","Epoch [923/2000], Train Loss: 0.3785\n","Epoch [924/2000], Train Loss: 0.3817\n","Epoch [925/2000], Train Loss: 0.3795\n","Epoch [926/2000], Train Loss: 0.3800\n","Epoch [927/2000], Train Loss: 0.3831\n","Epoch [928/2000], Train Loss: 0.3795\n","Epoch [929/2000], Train Loss: 0.3799\n","Epoch [930/2000], Train Loss: 0.3817\n","Epoch [931/2000], Train Loss: 0.3817\n","Epoch [932/2000], Train Loss: 0.3803\n","Epoch [933/2000], Train Loss: 0.3807\n","Epoch [934/2000], Train Loss: 0.3838\n","Epoch [935/2000], Train Loss: 0.3789\n","Epoch [936/2000], Train Loss: 0.3817\n","Epoch [937/2000], Train Loss: 0.3798\n","Epoch [938/2000], Train Loss: 0.3789\n","Epoch [939/2000], Train Loss: 0.3797\n","Epoch [940/2000], Train Loss: 0.3806\n","Epoch [941/2000], Train Loss: 0.3804\n","Epoch [942/2000], Train Loss: 0.3806\n","Epoch [943/2000], Train Loss: 0.3839\n","Epoch [944/2000], Train Loss: 0.3809\n","Epoch [945/2000], Train Loss: 0.3808\n","Epoch [946/2000], Train Loss: 0.3808\n","Epoch [947/2000], Train Loss: 0.3794\n","Epoch [948/2000], Train Loss: 0.3770\n","Epoch [949/2000], Train Loss: 0.3790\n","Epoch [950/2000], Train Loss: 0.3801\n","Epoch [951/2000], Train Loss: 0.3799\n","Epoch [952/2000], Train Loss: 0.3795\n","Epoch [953/2000], Train Loss: 0.3799\n","Epoch [954/2000], Train Loss: 0.3773\n","Epoch [955/2000], Train Loss: 0.3809\n","Epoch [956/2000], Train Loss: 0.3791\n","Epoch [957/2000], Train Loss: 0.3802\n","Epoch [958/2000], Train Loss: 0.3803\n","Epoch [959/2000], Train Loss: 0.3813\n","Epoch [960/2000], Train Loss: 0.3821\n","Epoch [961/2000], Train Loss: 0.3786\n","Epoch [962/2000], Train Loss: 0.3785\n","Epoch [963/2000], Train Loss: 0.3795\n","Epoch [964/2000], Train Loss: 0.3797\n","Epoch [965/2000], Train Loss: 0.3792\n","Epoch [966/2000], Train Loss: 0.3809\n","Epoch [967/2000], Train Loss: 0.3809\n","Epoch [968/2000], Train Loss: 0.3859\n","Epoch [969/2000], Train Loss: 0.3813\n","Epoch [970/2000], Train Loss: 0.3770\n","Epoch [971/2000], Train Loss: 0.3765\n","Epoch [972/2000], Train Loss: 0.3789\n","Epoch [973/2000], Train Loss: 0.3780\n","Epoch [974/2000], Train Loss: 0.3784\n","Epoch [975/2000], Train Loss: 0.3806\n","Epoch [976/2000], Train Loss: 0.3787\n","Epoch [977/2000], Train Loss: 0.3792\n","Epoch [978/2000], Train Loss: 0.3786\n","Epoch [979/2000], Train Loss: 0.3791\n","Epoch [980/2000], Train Loss: 0.3770\n","Epoch [981/2000], Train Loss: 0.3810\n","Epoch [982/2000], Train Loss: 0.3795\n","Epoch [983/2000], Train Loss: 0.3802\n","Epoch [984/2000], Train Loss: 0.3804\n","Epoch [985/2000], Train Loss: 0.3787\n","Epoch [986/2000], Train Loss: 0.3776\n","Epoch [987/2000], Train Loss: 0.3784\n","Epoch [988/2000], Train Loss: 0.3792\n","Epoch [989/2000], Train Loss: 0.3800\n","Epoch [990/2000], Train Loss: 0.3794\n","Epoch [991/2000], Train Loss: 0.3799\n","Epoch [992/2000], Train Loss: 0.3770\n","Epoch [993/2000], Train Loss: 0.3774\n","Epoch [994/2000], Train Loss: 0.3800\n","Epoch [995/2000], Train Loss: 0.3790\n","Epoch [996/2000], Train Loss: 0.3778\n","Epoch [997/2000], Train Loss: 0.3786\n","Epoch [998/2000], Train Loss: 0.3794\n","Epoch [999/2000], Train Loss: 0.3778\n","Epoch [1000/2000], Train Loss: 0.3773\n","Epoch [1001/2000], Train Loss: 0.3769\n","Epoch [1002/2000], Train Loss: 0.3756\n","Epoch [1003/2000], Train Loss: 0.3808\n","Epoch [1004/2000], Train Loss: 0.3765\n","Epoch [1005/2000], Train Loss: 0.3781\n","Epoch [1006/2000], Train Loss: 0.3770\n","Epoch [1007/2000], Train Loss: 0.3764\n","Epoch [1008/2000], Train Loss: 0.3754\n","Epoch [1009/2000], Train Loss: 0.3775\n","Epoch [1010/2000], Train Loss: 0.3767\n","Epoch [1011/2000], Train Loss: 0.3794\n","Epoch [1012/2000], Train Loss: 0.3762\n","Epoch [1013/2000], Train Loss: 0.3791\n","Epoch [1014/2000], Train Loss: 0.3749\n","Epoch [1015/2000], Train Loss: 0.3752\n","Epoch [1016/2000], Train Loss: 0.3767\n","Epoch [1017/2000], Train Loss: 0.3773\n","Epoch [1018/2000], Train Loss: 0.3777\n","Epoch [1019/2000], Train Loss: 0.3787\n","Epoch [1020/2000], Train Loss: 0.3775\n","Epoch [1021/2000], Train Loss: 0.3784\n","Epoch [1022/2000], Train Loss: 0.3767\n","Epoch [1023/2000], Train Loss: 0.3763\n","Epoch [1024/2000], Train Loss: 0.3767\n","Epoch [1025/2000], Train Loss: 0.3760\n","Epoch [1026/2000], Train Loss: 0.3743\n","Epoch [1027/2000], Train Loss: 0.3790\n","Epoch [1028/2000], Train Loss: 0.3748\n","Epoch [1029/2000], Train Loss: 0.3781\n","Epoch [1030/2000], Train Loss: 0.3798\n","Epoch [1031/2000], Train Loss: 0.3776\n","Epoch [1032/2000], Train Loss: 0.3760\n","Epoch [1033/2000], Train Loss: 0.3762\n","Epoch [1034/2000], Train Loss: 0.3747\n","Epoch [1035/2000], Train Loss: 0.3729\n","Epoch [1036/2000], Train Loss: 0.3741\n","Epoch [1037/2000], Train Loss: 0.3761\n","Epoch [1038/2000], Train Loss: 0.3754\n","Epoch [1039/2000], Train Loss: 0.3737\n","Epoch [1040/2000], Train Loss: 0.3766\n","Epoch [1041/2000], Train Loss: 0.3750\n","Epoch [1042/2000], Train Loss: 0.3766\n","Epoch [1043/2000], Train Loss: 0.3733\n","Epoch [1044/2000], Train Loss: 0.3748\n","Epoch [1045/2000], Train Loss: 0.3733\n","Epoch [1046/2000], Train Loss: 0.3780\n","Epoch [1047/2000], Train Loss: 0.3750\n","Epoch [1048/2000], Train Loss: 0.3797\n","Epoch [1049/2000], Train Loss: 0.3743\n","Epoch [1050/2000], Train Loss: 0.3761\n","Epoch [1051/2000], Train Loss: 0.3740\n","Epoch [1052/2000], Train Loss: 0.3751\n","Epoch [1053/2000], Train Loss: 0.3756\n","Epoch [1054/2000], Train Loss: 0.3772\n","Epoch [1055/2000], Train Loss: 0.3737\n","Epoch [1056/2000], Train Loss: 0.3740\n","Epoch [1057/2000], Train Loss: 0.3757\n","Epoch [1058/2000], Train Loss: 0.3762\n","Epoch [1059/2000], Train Loss: 0.3780\n","Epoch [1060/2000], Train Loss: 0.3759\n","Epoch [1061/2000], Train Loss: 0.3735\n","Epoch [1062/2000], Train Loss: 0.3751\n","Epoch [1063/2000], Train Loss: 0.3738\n","Epoch [1064/2000], Train Loss: 0.3750\n","Epoch [1065/2000], Train Loss: 0.3748\n","Epoch [1066/2000], Train Loss: 0.3734\n","Epoch [1067/2000], Train Loss: 0.3742\n","Epoch [1068/2000], Train Loss: 0.3746\n","Epoch [1069/2000], Train Loss: 0.3736\n","Epoch [1070/2000], Train Loss: 0.3717\n","Epoch [1071/2000], Train Loss: 0.3727\n","Epoch [1072/2000], Train Loss: 0.3783\n","Epoch [1073/2000], Train Loss: 0.3709\n","Epoch [1074/2000], Train Loss: 0.3717\n","Epoch [1075/2000], Train Loss: 0.3740\n","Epoch [1076/2000], Train Loss: 0.3726\n","Epoch [1077/2000], Train Loss: 0.3723\n","Epoch [1078/2000], Train Loss: 0.3734\n","Epoch [1079/2000], Train Loss: 0.3749\n","Epoch [1080/2000], Train Loss: 0.3712\n","Epoch [1081/2000], Train Loss: 0.3737\n","Epoch [1082/2000], Train Loss: 0.3742\n","Epoch [1083/2000], Train Loss: 0.3713\n","Epoch [1084/2000], Train Loss: 0.3717\n","Epoch [1085/2000], Train Loss: 0.3717\n","Epoch [1086/2000], Train Loss: 0.3744\n","Epoch [1087/2000], Train Loss: 0.3740\n","Epoch [1088/2000], Train Loss: 0.3718\n","Epoch [1089/2000], Train Loss: 0.3744\n","Epoch [1090/2000], Train Loss: 0.3752\n","Epoch [1091/2000], Train Loss: 0.3712\n","Epoch [1092/2000], Train Loss: 0.3723\n","Epoch [1093/2000], Train Loss: 0.3692\n","Epoch [1094/2000], Train Loss: 0.3722\n","Epoch [1095/2000], Train Loss: 0.3735\n","Epoch [1096/2000], Train Loss: 0.3695\n","Epoch [1097/2000], Train Loss: 0.3732\n","Epoch [1098/2000], Train Loss: 0.3733\n","Epoch [1099/2000], Train Loss: 0.3707\n","Epoch [1100/2000], Train Loss: 0.3729\n","Epoch [1101/2000], Train Loss: 0.3689\n","Epoch [1102/2000], Train Loss: 0.3697\n","Epoch [1103/2000], Train Loss: 0.3717\n","Epoch [1104/2000], Train Loss: 0.3740\n","Epoch [1105/2000], Train Loss: 0.3706\n","Epoch [1106/2000], Train Loss: 0.3689\n","Epoch [1107/2000], Train Loss: 0.3691\n","Epoch [1108/2000], Train Loss: 0.3698\n","Epoch [1109/2000], Train Loss: 0.3702\n","Epoch [1110/2000], Train Loss: 0.3713\n","Epoch [1111/2000], Train Loss: 0.3706\n","Epoch [1112/2000], Train Loss: 0.3697\n","Epoch [1113/2000], Train Loss: 0.3725\n","Epoch [1114/2000], Train Loss: 0.3721\n","Epoch [1115/2000], Train Loss: 0.3697\n","Epoch [1116/2000], Train Loss: 0.3740\n","Epoch [1117/2000], Train Loss: 0.3735\n","Epoch [1118/2000], Train Loss: 0.3681\n","Epoch [1119/2000], Train Loss: 0.3697\n","Epoch [1120/2000], Train Loss: 0.3738\n","Epoch [1121/2000], Train Loss: 0.3697\n","Epoch [1122/2000], Train Loss: 0.3718\n","Epoch [1123/2000], Train Loss: 0.3700\n","Epoch [1124/2000], Train Loss: 0.3729\n","Epoch [1125/2000], Train Loss: 0.3729\n","Epoch [1126/2000], Train Loss: 0.3686\n","Epoch [1127/2000], Train Loss: 0.3703\n","Epoch [1128/2000], Train Loss: 0.3732\n","Epoch [1129/2000], Train Loss: 0.3692\n","Epoch [1130/2000], Train Loss: 0.3694\n","Epoch [1131/2000], Train Loss: 0.3681\n","Epoch [1132/2000], Train Loss: 0.3714\n","Epoch [1133/2000], Train Loss: 0.3696\n","Epoch [1134/2000], Train Loss: 0.3683\n","Epoch [1135/2000], Train Loss: 0.3699\n","Epoch [1136/2000], Train Loss: 0.3702\n","Epoch [1137/2000], Train Loss: 0.3695\n","Epoch [1138/2000], Train Loss: 0.3689\n","Epoch [1139/2000], Train Loss: 0.3711\n","Epoch [1140/2000], Train Loss: 0.3754\n","Epoch [1141/2000], Train Loss: 0.3680\n","Epoch [1142/2000], Train Loss: 0.3711\n","Epoch [1143/2000], Train Loss: 0.3697\n","Epoch [1144/2000], Train Loss: 0.3685\n","Epoch [1145/2000], Train Loss: 0.3685\n","Epoch [1146/2000], Train Loss: 0.3678\n","Epoch [1147/2000], Train Loss: 0.3712\n","Epoch [1148/2000], Train Loss: 0.3679\n","Epoch [1149/2000], Train Loss: 0.3682\n","Epoch [1150/2000], Train Loss: 0.3678\n","Epoch [1151/2000], Train Loss: 0.3695\n","Epoch [1152/2000], Train Loss: 0.3715\n","Epoch [1153/2000], Train Loss: 0.3699\n","Epoch [1154/2000], Train Loss: 0.3688\n","Epoch [1155/2000], Train Loss: 0.3727\n","Epoch [1156/2000], Train Loss: 0.3705\n","Epoch [1157/2000], Train Loss: 0.3682\n","Epoch [1158/2000], Train Loss: 0.3675\n","Epoch [1159/2000], Train Loss: 0.3682\n","Epoch [1160/2000], Train Loss: 0.3697\n","Epoch [1161/2000], Train Loss: 0.3689\n","Epoch [1162/2000], Train Loss: 0.3685\n","Epoch [1163/2000], Train Loss: 0.3693\n","Epoch [1164/2000], Train Loss: 0.3705\n","Epoch [1165/2000], Train Loss: 0.3703\n","Epoch [1166/2000], Train Loss: 0.3690\n","Epoch [1167/2000], Train Loss: 0.3704\n","Epoch [1168/2000], Train Loss: 0.3648\n","Epoch [1169/2000], Train Loss: 0.3699\n","Epoch [1170/2000], Train Loss: 0.3686\n","Epoch [1171/2000], Train Loss: 0.3682\n","Epoch [1172/2000], Train Loss: 0.3671\n","Epoch [1173/2000], Train Loss: 0.3688\n","Epoch [1174/2000], Train Loss: 0.3702\n","Epoch [1175/2000], Train Loss: 0.3685\n","Epoch [1176/2000], Train Loss: 0.3665\n","Epoch [1177/2000], Train Loss: 0.3673\n","Epoch [1178/2000], Train Loss: 0.3685\n","Epoch [1179/2000], Train Loss: 0.3671\n","Epoch [1180/2000], Train Loss: 0.3679\n","Epoch [1181/2000], Train Loss: 0.3688\n","Epoch [1182/2000], Train Loss: 0.3672\n","Epoch [1183/2000], Train Loss: 0.3686\n","Epoch [1184/2000], Train Loss: 0.3656\n","Epoch [1185/2000], Train Loss: 0.3680\n","Epoch [1186/2000], Train Loss: 0.3692\n","Epoch [1187/2000], Train Loss: 0.3702\n","Epoch [1188/2000], Train Loss: 0.3679\n","Epoch [1189/2000], Train Loss: 0.3677\n","Epoch [1190/2000], Train Loss: 0.3679\n","Epoch [1191/2000], Train Loss: 0.3676\n","Epoch [1192/2000], Train Loss: 0.3653\n","Epoch [1193/2000], Train Loss: 0.3665\n","Epoch [1194/2000], Train Loss: 0.3687\n","Epoch [1195/2000], Train Loss: 0.3658\n","Epoch [1196/2000], Train Loss: 0.3675\n","Epoch [1197/2000], Train Loss: 0.3677\n","Epoch [1198/2000], Train Loss: 0.3663\n","Epoch [1199/2000], Train Loss: 0.3677\n","Epoch [1200/2000], Train Loss: 0.3665\n","Epoch [1201/2000], Train Loss: 0.3682\n","Epoch [1202/2000], Train Loss: 0.3667\n","Epoch [1203/2000], Train Loss: 0.3675\n","Epoch [1204/2000], Train Loss: 0.3675\n","Epoch [1205/2000], Train Loss: 0.3664\n","Epoch [1206/2000], Train Loss: 0.3687\n","Epoch [1207/2000], Train Loss: 0.3660\n","Epoch [1208/2000], Train Loss: 0.3662\n","Epoch [1209/2000], Train Loss: 0.3696\n","Epoch [1210/2000], Train Loss: 0.3692\n","Epoch [1211/2000], Train Loss: 0.3663\n","Epoch [1212/2000], Train Loss: 0.3634\n","Epoch [1213/2000], Train Loss: 0.3678\n","Epoch [1214/2000], Train Loss: 0.3643\n","Epoch [1215/2000], Train Loss: 0.3648\n","Epoch [1216/2000], Train Loss: 0.3683\n","Epoch [1217/2000], Train Loss: 0.3652\n","Epoch [1218/2000], Train Loss: 0.3671\n","Epoch [1219/2000], Train Loss: 0.3640\n","Epoch [1220/2000], Train Loss: 0.3673\n","Epoch [1221/2000], Train Loss: 0.3657\n","Epoch [1222/2000], Train Loss: 0.3714\n","Epoch [1223/2000], Train Loss: 0.3686\n","Epoch [1224/2000], Train Loss: 0.3671\n","Epoch [1225/2000], Train Loss: 0.3625\n","Epoch [1226/2000], Train Loss: 0.3653\n","Epoch [1227/2000], Train Loss: 0.3641\n","Epoch [1228/2000], Train Loss: 0.3655\n","Epoch [1229/2000], Train Loss: 0.3676\n","Epoch [1230/2000], Train Loss: 0.3664\n","Epoch [1231/2000], Train Loss: 0.3684\n","Epoch [1232/2000], Train Loss: 0.3660\n","Epoch [1233/2000], Train Loss: 0.3663\n","Epoch [1234/2000], Train Loss: 0.3661\n","Epoch [1235/2000], Train Loss: 0.3670\n","Epoch [1236/2000], Train Loss: 0.3668\n","Epoch [1237/2000], Train Loss: 0.3642\n","Epoch [1238/2000], Train Loss: 0.3663\n","Epoch [1239/2000], Train Loss: 0.3656\n","Epoch [1240/2000], Train Loss: 0.3643\n","Epoch [1241/2000], Train Loss: 0.3658\n","Epoch [1242/2000], Train Loss: 0.3721\n","Epoch [1243/2000], Train Loss: 0.3674\n","Epoch [1244/2000], Train Loss: 0.3645\n","Epoch [1245/2000], Train Loss: 0.3642\n","Epoch [1246/2000], Train Loss: 0.3615\n","Epoch [1247/2000], Train Loss: 0.3674\n","Epoch [1248/2000], Train Loss: 0.3657\n","Epoch [1249/2000], Train Loss: 0.3658\n","Epoch [1250/2000], Train Loss: 0.3655\n","Epoch [1251/2000], Train Loss: 0.3651\n","Epoch [1252/2000], Train Loss: 0.3663\n","Epoch [1253/2000], Train Loss: 0.3672\n","Epoch [1254/2000], Train Loss: 0.3665\n","Epoch [1255/2000], Train Loss: 0.3664\n","Epoch [1256/2000], Train Loss: 0.3648\n","Epoch [1257/2000], Train Loss: 0.3648\n","Epoch [1258/2000], Train Loss: 0.3669\n","Epoch [1259/2000], Train Loss: 0.3658\n","Epoch [1260/2000], Train Loss: 0.3687\n","Epoch [1261/2000], Train Loss: 0.3657\n","Epoch [1262/2000], Train Loss: 0.3730\n","Epoch [1263/2000], Train Loss: 0.3653\n","Epoch [1264/2000], Train Loss: 0.3646\n","Epoch [1265/2000], Train Loss: 0.3637\n","Epoch [1266/2000], Train Loss: 0.3641\n","Epoch [1267/2000], Train Loss: 0.3648\n","Epoch [1268/2000], Train Loss: 0.3616\n","Epoch [1269/2000], Train Loss: 0.3678\n","Epoch [1270/2000], Train Loss: 0.3663\n","Epoch [1271/2000], Train Loss: 0.3639\n","Epoch [1272/2000], Train Loss: 0.3652\n","Epoch [1273/2000], Train Loss: 0.3661\n","Epoch [1274/2000], Train Loss: 0.3645\n","Epoch [1275/2000], Train Loss: 0.3646\n","Epoch [1276/2000], Train Loss: 0.3654\n","Epoch [1277/2000], Train Loss: 0.3641\n","Epoch [1278/2000], Train Loss: 0.3665\n","Epoch [1279/2000], Train Loss: 0.3603\n","Epoch [1280/2000], Train Loss: 0.3675\n","Epoch [1281/2000], Train Loss: 0.3654\n","Epoch [1282/2000], Train Loss: 0.3654\n","Epoch [1283/2000], Train Loss: 0.3684\n","Epoch [1284/2000], Train Loss: 0.3632\n","Epoch [1285/2000], Train Loss: 0.3654\n","Epoch [1286/2000], Train Loss: 0.3645\n","Epoch [1287/2000], Train Loss: 0.3649\n","Epoch [1288/2000], Train Loss: 0.3658\n","Epoch [1289/2000], Train Loss: 0.3648\n","Epoch [1290/2000], Train Loss: 0.3630\n","Epoch [1291/2000], Train Loss: 0.3710\n","Epoch [1292/2000], Train Loss: 0.3655\n","Epoch [1293/2000], Train Loss: 0.3631\n","Epoch [1294/2000], Train Loss: 0.3632\n","Epoch [1295/2000], Train Loss: 0.3633\n","Epoch [1296/2000], Train Loss: 0.3634\n","Epoch [1297/2000], Train Loss: 0.3640\n","Epoch [1298/2000], Train Loss: 0.3659\n","Epoch [1299/2000], Train Loss: 0.3637\n","Epoch [1300/2000], Train Loss: 0.3640\n","Epoch [1301/2000], Train Loss: 0.3640\n","Epoch [1302/2000], Train Loss: 0.3652\n","Epoch [1303/2000], Train Loss: 0.3613\n","Epoch [1304/2000], Train Loss: 0.3608\n","Epoch [1305/2000], Train Loss: 0.3662\n","Epoch [1306/2000], Train Loss: 0.3641\n","Epoch [1307/2000], Train Loss: 0.3648\n","Epoch [1308/2000], Train Loss: 0.3658\n","Epoch [1309/2000], Train Loss: 0.3630\n","Epoch [1310/2000], Train Loss: 0.3624\n","Epoch [1311/2000], Train Loss: 0.3650\n","Epoch [1312/2000], Train Loss: 0.3651\n","Epoch [1313/2000], Train Loss: 0.3631\n","Epoch [1314/2000], Train Loss: 0.3662\n","Epoch [1315/2000], Train Loss: 0.3632\n","Epoch [1316/2000], Train Loss: 0.3659\n","Epoch [1317/2000], Train Loss: 0.3648\n","Epoch [1318/2000], Train Loss: 0.3623\n","Epoch [1319/2000], Train Loss: 0.3631\n","Epoch [1320/2000], Train Loss: 0.3606\n","Epoch [1321/2000], Train Loss: 0.3645\n","Epoch [1322/2000], Train Loss: 0.3634\n","Epoch [1323/2000], Train Loss: 0.3638\n","Epoch [1324/2000], Train Loss: 0.3649\n","Epoch [1325/2000], Train Loss: 0.3661\n","Epoch [1326/2000], Train Loss: 0.3614\n","Epoch [1327/2000], Train Loss: 0.3612\n","Epoch [1328/2000], Train Loss: 0.3632\n","Epoch [1329/2000], Train Loss: 0.3641\n","Epoch [1330/2000], Train Loss: 0.3637\n","Epoch [1331/2000], Train Loss: 0.3614\n","Epoch [1332/2000], Train Loss: 0.3615\n","Epoch [1333/2000], Train Loss: 0.3630\n","Epoch [1334/2000], Train Loss: 0.3656\n","Epoch [1335/2000], Train Loss: 0.3646\n","Epoch [1336/2000], Train Loss: 0.3626\n","Epoch [1337/2000], Train Loss: 0.3653\n","Epoch [1338/2000], Train Loss: 0.3637\n","Epoch [1339/2000], Train Loss: 0.3627\n","Epoch [1340/2000], Train Loss: 0.3624\n","Epoch [1341/2000], Train Loss: 0.3641\n","Epoch [1342/2000], Train Loss: 0.3639\n","Epoch [1343/2000], Train Loss: 0.3632\n","Epoch [1344/2000], Train Loss: 0.3625\n","Epoch [1345/2000], Train Loss: 0.3636\n","Epoch [1346/2000], Train Loss: 0.3626\n","Epoch [1347/2000], Train Loss: 0.3616\n","Epoch [1348/2000], Train Loss: 0.3629\n","Epoch [1349/2000], Train Loss: 0.3644\n","Epoch [1350/2000], Train Loss: 0.3628\n","Epoch [1351/2000], Train Loss: 0.3633\n","Epoch [1352/2000], Train Loss: 0.3654\n","Epoch [1353/2000], Train Loss: 0.3597\n","Epoch [1354/2000], Train Loss: 0.3609\n","Epoch [1355/2000], Train Loss: 0.3633\n","Epoch [1356/2000], Train Loss: 0.3612\n","Epoch [1357/2000], Train Loss: 0.3637\n","Epoch [1358/2000], Train Loss: 0.3614\n","Epoch [1359/2000], Train Loss: 0.3618\n","Epoch [1360/2000], Train Loss: 0.3637\n","Epoch [1361/2000], Train Loss: 0.3622\n","Epoch [1362/2000], Train Loss: 0.3621\n","Epoch [1363/2000], Train Loss: 0.3598\n","Epoch [1364/2000], Train Loss: 0.3616\n","Epoch [1365/2000], Train Loss: 0.3627\n","Epoch [1366/2000], Train Loss: 0.3610\n","Epoch [1367/2000], Train Loss: 0.3598\n","Epoch [1368/2000], Train Loss: 0.3631\n","Epoch [1369/2000], Train Loss: 0.3641\n","Epoch [1370/2000], Train Loss: 0.3603\n","Epoch [1371/2000], Train Loss: 0.3624\n","Epoch [1372/2000], Train Loss: 0.3638\n","Epoch [1373/2000], Train Loss: 0.3602\n","Epoch [1374/2000], Train Loss: 0.3638\n","Epoch [1375/2000], Train Loss: 0.3613\n","Epoch [1376/2000], Train Loss: 0.3623\n","Epoch [1377/2000], Train Loss: 0.3617\n","Epoch [1378/2000], Train Loss: 0.3610\n","Epoch [1379/2000], Train Loss: 0.3612\n","Epoch [1380/2000], Train Loss: 0.3611\n","Epoch [1381/2000], Train Loss: 0.3621\n","Epoch [1382/2000], Train Loss: 0.3642\n","Epoch [1383/2000], Train Loss: 0.3631\n","Epoch [1384/2000], Train Loss: 0.3637\n","Epoch [1385/2000], Train Loss: 0.3617\n","Epoch [1386/2000], Train Loss: 0.3626\n","Epoch [1387/2000], Train Loss: 0.3630\n","Epoch [1388/2000], Train Loss: 0.3596\n","Epoch [1389/2000], Train Loss: 0.3595\n","Epoch [1390/2000], Train Loss: 0.3601\n","Epoch [1391/2000], Train Loss: 0.3611\n","Epoch [1392/2000], Train Loss: 0.3598\n","Epoch [1393/2000], Train Loss: 0.3622\n","Epoch [1394/2000], Train Loss: 0.3603\n","Epoch [1395/2000], Train Loss: 0.3619\n","Epoch [1396/2000], Train Loss: 0.3626\n","Epoch [1397/2000], Train Loss: 0.3634\n","Epoch [1398/2000], Train Loss: 0.3609\n","Epoch [1399/2000], Train Loss: 0.3589\n","Epoch [1400/2000], Train Loss: 0.3618\n","Epoch [1401/2000], Train Loss: 0.3614\n","Epoch [1402/2000], Train Loss: 0.3645\n","Epoch [1403/2000], Train Loss: 0.3612\n","Epoch [1404/2000], Train Loss: 0.3611\n","Epoch [1405/2000], Train Loss: 0.3580\n","Epoch [1406/2000], Train Loss: 0.3628\n","Epoch [1407/2000], Train Loss: 0.3615\n","Epoch [1408/2000], Train Loss: 0.3624\n","Epoch [1409/2000], Train Loss: 0.3616\n","Epoch [1410/2000], Train Loss: 0.3590\n","Epoch [1411/2000], Train Loss: 0.3610\n","Epoch [1412/2000], Train Loss: 0.3608\n","Epoch [1413/2000], Train Loss: 0.3612\n","Epoch [1414/2000], Train Loss: 0.3600\n","Epoch [1415/2000], Train Loss: 0.3614\n","Epoch [1416/2000], Train Loss: 0.3601\n","Epoch [1417/2000], Train Loss: 0.3600\n","Epoch [1418/2000], Train Loss: 0.3616\n","Epoch [1419/2000], Train Loss: 0.3615\n","Epoch [1420/2000], Train Loss: 0.3596\n","Epoch [1421/2000], Train Loss: 0.3597\n","Epoch [1422/2000], Train Loss: 0.3603\n","Epoch [1423/2000], Train Loss: 0.3618\n","Epoch [1424/2000], Train Loss: 0.3612\n","Epoch [1425/2000], Train Loss: 0.3599\n","Epoch [1426/2000], Train Loss: 0.3632\n","Epoch [1427/2000], Train Loss: 0.3608\n","Epoch [1428/2000], Train Loss: 0.3579\n","Epoch [1429/2000], Train Loss: 0.3593\n","Epoch [1430/2000], Train Loss: 0.3598\n","Epoch [1431/2000], Train Loss: 0.3602\n","Epoch [1432/2000], Train Loss: 0.3617\n","Epoch [1433/2000], Train Loss: 0.3600\n","Epoch [1434/2000], Train Loss: 0.3610\n","Epoch [1435/2000], Train Loss: 0.3598\n","Epoch [1436/2000], Train Loss: 0.3612\n","Epoch [1437/2000], Train Loss: 0.3597\n","Epoch [1438/2000], Train Loss: 0.3597\n","Epoch [1439/2000], Train Loss: 0.3595\n","Epoch [1440/2000], Train Loss: 0.3608\n","Epoch [1441/2000], Train Loss: 0.3608\n","Epoch [1442/2000], Train Loss: 0.3614\n","Epoch [1443/2000], Train Loss: 0.3614\n","Epoch [1444/2000], Train Loss: 0.3590\n","Epoch [1445/2000], Train Loss: 0.3631\n","Epoch [1446/2000], Train Loss: 0.3611\n","Epoch [1447/2000], Train Loss: 0.3596\n","Epoch [1448/2000], Train Loss: 0.3590\n","Epoch [1449/2000], Train Loss: 0.3579\n","Epoch [1450/2000], Train Loss: 0.3598\n","Epoch [1451/2000], Train Loss: 0.3614\n","Epoch [1452/2000], Train Loss: 0.3617\n","Epoch [1453/2000], Train Loss: 0.3593\n","Epoch [1454/2000], Train Loss: 0.3591\n","Epoch [1455/2000], Train Loss: 0.3600\n","Epoch [1456/2000], Train Loss: 0.3617\n","Epoch [1457/2000], Train Loss: 0.3619\n","Epoch [1458/2000], Train Loss: 0.3608\n","Epoch [1459/2000], Train Loss: 0.3622\n","Epoch [1460/2000], Train Loss: 0.3606\n","Epoch [1461/2000], Train Loss: 0.3569\n","Epoch [1462/2000], Train Loss: 0.3608\n","Epoch [1463/2000], Train Loss: 0.3597\n","Epoch [1464/2000], Train Loss: 0.3590\n","Epoch [1465/2000], Train Loss: 0.3614\n","Epoch [1466/2000], Train Loss: 0.3597\n","Epoch [1467/2000], Train Loss: 0.3586\n","Epoch [1468/2000], Train Loss: 0.3612\n","Epoch [1469/2000], Train Loss: 0.3599\n","Epoch [1470/2000], Train Loss: 0.3589\n","Epoch [1471/2000], Train Loss: 0.3593\n","Epoch [1472/2000], Train Loss: 0.3585\n","Epoch [1473/2000], Train Loss: 0.3597\n","Epoch [1474/2000], Train Loss: 0.3616\n","Epoch [1475/2000], Train Loss: 0.3605\n","Epoch [1476/2000], Train Loss: 0.3598\n","Epoch [1477/2000], Train Loss: 0.3577\n","Epoch [1478/2000], Train Loss: 0.3579\n","Epoch [1479/2000], Train Loss: 0.3592\n","Epoch [1480/2000], Train Loss: 0.3609\n","Epoch [1481/2000], Train Loss: 0.3591\n","Epoch [1482/2000], Train Loss: 0.3586\n","Epoch [1483/2000], Train Loss: 0.3601\n","Epoch [1484/2000], Train Loss: 0.3581\n","Epoch [1485/2000], Train Loss: 0.3591\n","Epoch [1486/2000], Train Loss: 0.3619\n","Epoch [1487/2000], Train Loss: 0.3616\n","Epoch [1488/2000], Train Loss: 0.3619\n","Epoch [1489/2000], Train Loss: 0.3573\n","Epoch [1490/2000], Train Loss: 0.3567\n","Epoch [1491/2000], Train Loss: 0.3585\n","Epoch [1492/2000], Train Loss: 0.3622\n","Epoch [1493/2000], Train Loss: 0.3582\n","Epoch [1494/2000], Train Loss: 0.3573\n","Epoch [1495/2000], Train Loss: 0.3611\n","Epoch [1496/2000], Train Loss: 0.3644\n","Epoch [1497/2000], Train Loss: 0.3611\n","Epoch [1498/2000], Train Loss: 0.3593\n","Epoch [1499/2000], Train Loss: 0.3569\n","Epoch [1500/2000], Train Loss: 0.3593\n","Epoch [1501/2000], Train Loss: 0.3584\n","Epoch [1502/2000], Train Loss: 0.3591\n","Epoch [1503/2000], Train Loss: 0.3588\n","Epoch [1504/2000], Train Loss: 0.3564\n","Epoch [1505/2000], Train Loss: 0.3584\n","Epoch [1506/2000], Train Loss: 0.3606\n","Epoch [1507/2000], Train Loss: 0.3575\n","Epoch [1508/2000], Train Loss: 0.3573\n","Epoch [1509/2000], Train Loss: 0.3605\n","Epoch [1510/2000], Train Loss: 0.3598\n","Epoch [1511/2000], Train Loss: 0.3586\n","Epoch [1512/2000], Train Loss: 0.3589\n","Epoch [1513/2000], Train Loss: 0.3588\n","Epoch [1514/2000], Train Loss: 0.3566\n","Epoch [1515/2000], Train Loss: 0.3595\n","Epoch [1516/2000], Train Loss: 0.3610\n","Epoch [1517/2000], Train Loss: 0.3605\n","Epoch [1518/2000], Train Loss: 0.3574\n","Epoch [1519/2000], Train Loss: 0.3573\n","Epoch [1520/2000], Train Loss: 0.3565\n","Epoch [1521/2000], Train Loss: 0.3589\n","Epoch [1522/2000], Train Loss: 0.3561\n","Epoch [1523/2000], Train Loss: 0.3562\n","Epoch [1524/2000], Train Loss: 0.3575\n","Epoch [1525/2000], Train Loss: 0.3576\n","Epoch [1526/2000], Train Loss: 0.3584\n","Epoch [1527/2000], Train Loss: 0.3592\n","Epoch [1528/2000], Train Loss: 0.3574\n","Epoch [1529/2000], Train Loss: 0.3595\n","Epoch [1530/2000], Train Loss: 0.3591\n","Epoch [1531/2000], Train Loss: 0.3601\n","Epoch [1532/2000], Train Loss: 0.3573\n","Epoch [1533/2000], Train Loss: 0.3573\n","Epoch [1534/2000], Train Loss: 0.3563\n","Epoch [1535/2000], Train Loss: 0.3582\n","Epoch [1536/2000], Train Loss: 0.3547\n","Epoch [1537/2000], Train Loss: 0.3589\n","Epoch [1538/2000], Train Loss: 0.3570\n","Epoch [1539/2000], Train Loss: 0.3571\n","Epoch [1540/2000], Train Loss: 0.3566\n","Epoch [1541/2000], Train Loss: 0.3600\n","Epoch [1542/2000], Train Loss: 0.3589\n","Epoch [1543/2000], Train Loss: 0.3564\n","Epoch [1544/2000], Train Loss: 0.3561\n","Epoch [1545/2000], Train Loss: 0.3599\n","Epoch [1546/2000], Train Loss: 0.3571\n","Epoch [1547/2000], Train Loss: 0.3581\n","Epoch [1548/2000], Train Loss: 0.3580\n","Epoch [1549/2000], Train Loss: 0.3584\n","Epoch [1550/2000], Train Loss: 0.3575\n","Epoch [1551/2000], Train Loss: 0.3593\n","Epoch [1552/2000], Train Loss: 0.3575\n","Epoch [1553/2000], Train Loss: 0.3579\n","Epoch [1554/2000], Train Loss: 0.3588\n","Epoch [1555/2000], Train Loss: 0.3570\n","Epoch [1556/2000], Train Loss: 0.3571\n","Epoch [1557/2000], Train Loss: 0.3582\n","Epoch [1558/2000], Train Loss: 0.3575\n","Epoch [1559/2000], Train Loss: 0.3570\n","Epoch [1560/2000], Train Loss: 0.3598\n","Epoch [1561/2000], Train Loss: 0.3582\n","Epoch [1562/2000], Train Loss: 0.3577\n","Epoch [1563/2000], Train Loss: 0.3580\n","Epoch [1564/2000], Train Loss: 0.3574\n","Epoch [1565/2000], Train Loss: 0.3598\n","Epoch [1566/2000], Train Loss: 0.3590\n","Epoch [1567/2000], Train Loss: 0.3585\n","Epoch [1568/2000], Train Loss: 0.3576\n","Epoch [1569/2000], Train Loss: 0.3570\n","Epoch [1570/2000], Train Loss: 0.3568\n","Epoch [1571/2000], Train Loss: 0.3583\n","Epoch [1572/2000], Train Loss: 0.3566\n","Epoch [1573/2000], Train Loss: 0.3596\n","Epoch [1574/2000], Train Loss: 0.3555\n","Epoch [1575/2000], Train Loss: 0.3587\n","Epoch [1576/2000], Train Loss: 0.3558\n","Epoch [1577/2000], Train Loss: 0.3554\n","Epoch [1578/2000], Train Loss: 0.3598\n","Epoch [1579/2000], Train Loss: 0.3582\n","Epoch [1580/2000], Train Loss: 0.3580\n","Epoch [1581/2000], Train Loss: 0.3600\n","Epoch [1582/2000], Train Loss: 0.3578\n","Epoch [1583/2000], Train Loss: 0.3541\n","Epoch [1584/2000], Train Loss: 0.3566\n","Epoch [1585/2000], Train Loss: 0.3584\n","Epoch [1586/2000], Train Loss: 0.3580\n","Epoch [1587/2000], Train Loss: 0.3570\n","Epoch [1588/2000], Train Loss: 0.3561\n","Epoch [1589/2000], Train Loss: 0.3538\n","Epoch [1590/2000], Train Loss: 0.3576\n","Epoch [1591/2000], Train Loss: 0.3577\n","Epoch [1592/2000], Train Loss: 0.3559\n","Epoch [1593/2000], Train Loss: 0.3572\n","Epoch [1594/2000], Train Loss: 0.3605\n","Epoch [1595/2000], Train Loss: 0.3550\n","Epoch [1596/2000], Train Loss: 0.3553\n","Epoch [1597/2000], Train Loss: 0.3571\n","Epoch [1598/2000], Train Loss: 0.3574\n","Epoch [1599/2000], Train Loss: 0.3571\n","Epoch [1600/2000], Train Loss: 0.3579\n","Epoch [1601/2000], Train Loss: 0.3592\n","Epoch [1602/2000], Train Loss: 0.3583\n","Epoch [1603/2000], Train Loss: 0.3548\n","Epoch [1604/2000], Train Loss: 0.3540\n","Epoch [1605/2000], Train Loss: 0.3559\n","Epoch [1606/2000], Train Loss: 0.3572\n","Epoch [1607/2000], Train Loss: 0.3569\n","Epoch [1608/2000], Train Loss: 0.3549\n","Epoch [1609/2000], Train Loss: 0.3562\n","Epoch [1610/2000], Train Loss: 0.3573\n","Epoch [1611/2000], Train Loss: 0.3577\n","Epoch [1612/2000], Train Loss: 0.3559\n","Epoch [1613/2000], Train Loss: 0.3575\n","Epoch [1614/2000], Train Loss: 0.3588\n","Epoch [1615/2000], Train Loss: 0.3540\n","Epoch [1616/2000], Train Loss: 0.3555\n","Epoch [1617/2000], Train Loss: 0.3559\n","Epoch [1618/2000], Train Loss: 0.3550\n","Epoch [1619/2000], Train Loss: 0.3536\n","Epoch [1620/2000], Train Loss: 0.3557\n","Epoch [1621/2000], Train Loss: 0.3586\n","Epoch [1622/2000], Train Loss: 0.3559\n","Epoch [1623/2000], Train Loss: 0.3563\n","Epoch [1624/2000], Train Loss: 0.3562\n","Epoch [1625/2000], Train Loss: 0.3561\n","Epoch [1626/2000], Train Loss: 0.3561\n","Epoch [1627/2000], Train Loss: 0.3546\n","Epoch [1628/2000], Train Loss: 0.3538\n","Epoch [1629/2000], Train Loss: 0.3555\n","Epoch [1630/2000], Train Loss: 0.3545\n","Epoch [1631/2000], Train Loss: 0.3575\n","Epoch [1632/2000], Train Loss: 0.3555\n","Epoch [1633/2000], Train Loss: 0.3540\n","Epoch [1634/2000], Train Loss: 0.3552\n","Epoch [1635/2000], Train Loss: 0.3567\n","Epoch [1636/2000], Train Loss: 0.3592\n","Epoch [1637/2000], Train Loss: 0.3570\n","Epoch [1638/2000], Train Loss: 0.3577\n","Epoch [1639/2000], Train Loss: 0.3555\n","Epoch [1640/2000], Train Loss: 0.3551\n","Epoch [1641/2000], Train Loss: 0.3573\n","Epoch [1642/2000], Train Loss: 0.3538\n","Epoch [1643/2000], Train Loss: 0.3571\n","Epoch [1644/2000], Train Loss: 0.3558\n","Epoch [1645/2000], Train Loss: 0.3568\n","Epoch [1646/2000], Train Loss: 0.3518\n","Epoch [1647/2000], Train Loss: 0.3555\n","Epoch [1648/2000], Train Loss: 0.3595\n","Epoch [1649/2000], Train Loss: 0.3555\n","Epoch [1650/2000], Train Loss: 0.3553\n","Epoch [1651/2000], Train Loss: 0.3541\n","Epoch [1652/2000], Train Loss: 0.3579\n","Epoch [1653/2000], Train Loss: 0.3537\n","Epoch [1654/2000], Train Loss: 0.3580\n","Epoch [1655/2000], Train Loss: 0.3560\n","Epoch [1656/2000], Train Loss: 0.3556\n","Epoch [1657/2000], Train Loss: 0.3549\n","Epoch [1658/2000], Train Loss: 0.3559\n","Epoch [1659/2000], Train Loss: 0.3548\n","Epoch [1660/2000], Train Loss: 0.3560\n","Epoch [1661/2000], Train Loss: 0.3554\n","Epoch [1662/2000], Train Loss: 0.3536\n","Epoch [1663/2000], Train Loss: 0.3569\n","Epoch [1664/2000], Train Loss: 0.3557\n","Epoch [1665/2000], Train Loss: 0.3560\n","Epoch [1666/2000], Train Loss: 0.3593\n","Epoch [1667/2000], Train Loss: 0.3545\n","Epoch [1668/2000], Train Loss: 0.3559\n","Epoch [1669/2000], Train Loss: 0.3574\n","Epoch [1670/2000], Train Loss: 0.3552\n","Epoch [1671/2000], Train Loss: 0.3566\n","Epoch [1672/2000], Train Loss: 0.3545\n","Epoch [1673/2000], Train Loss: 0.3536\n","Epoch [1674/2000], Train Loss: 0.3545\n","Epoch [1675/2000], Train Loss: 0.3554\n","Epoch [1676/2000], Train Loss: 0.3549\n","Epoch [1677/2000], Train Loss: 0.3552\n","Epoch [1678/2000], Train Loss: 0.3543\n","Epoch [1679/2000], Train Loss: 0.3526\n","Epoch [1680/2000], Train Loss: 0.3566\n","Epoch [1681/2000], Train Loss: 0.3558\n","Epoch [1682/2000], Train Loss: 0.3564\n","Epoch [1683/2000], Train Loss: 0.3542\n","Epoch [1684/2000], Train Loss: 0.3561\n","Epoch [1685/2000], Train Loss: 0.3531\n","Epoch [1686/2000], Train Loss: 0.3556\n","Epoch [1687/2000], Train Loss: 0.3547\n","Epoch [1688/2000], Train Loss: 0.3544\n","Epoch [1689/2000], Train Loss: 0.3544\n","Epoch [1690/2000], Train Loss: 0.3565\n","Epoch [1691/2000], Train Loss: 0.3568\n","Epoch [1692/2000], Train Loss: 0.3524\n","Epoch [1693/2000], Train Loss: 0.3562\n","Epoch [1694/2000], Train Loss: 0.3533\n","Epoch [1695/2000], Train Loss: 0.3554\n","Epoch [1696/2000], Train Loss: 0.3548\n","Epoch [1697/2000], Train Loss: 0.3533\n","Epoch [1698/2000], Train Loss: 0.3543\n","Epoch [1699/2000], Train Loss: 0.3541\n","Epoch [1700/2000], Train Loss: 0.3536\n","Epoch [1701/2000], Train Loss: 0.3583\n","Epoch [1702/2000], Train Loss: 0.3540\n","Epoch [1703/2000], Train Loss: 0.3561\n","Epoch [1704/2000], Train Loss: 0.3535\n","Epoch [1705/2000], Train Loss: 0.3551\n","Epoch [1706/2000], Train Loss: 0.3545\n","Epoch [1707/2000], Train Loss: 0.3534\n","Epoch [1708/2000], Train Loss: 0.3530\n","Epoch [1709/2000], Train Loss: 0.3554\n","Epoch [1710/2000], Train Loss: 0.3538\n","Epoch [1711/2000], Train Loss: 0.3553\n","Epoch [1712/2000], Train Loss: 0.3560\n","Epoch [1713/2000], Train Loss: 0.3541\n","Epoch [1714/2000], Train Loss: 0.3574\n","Epoch [1715/2000], Train Loss: 0.3575\n","Epoch [1716/2000], Train Loss: 0.3529\n","Epoch [1717/2000], Train Loss: 0.3548\n","Epoch [1718/2000], Train Loss: 0.3544\n","Epoch [1719/2000], Train Loss: 0.3554\n","Epoch [1720/2000], Train Loss: 0.3534\n","Epoch [1721/2000], Train Loss: 0.3542\n","Epoch [1722/2000], Train Loss: 0.3534\n","Epoch [1723/2000], Train Loss: 0.3511\n","Epoch [1724/2000], Train Loss: 0.3528\n","Epoch [1725/2000], Train Loss: 0.3526\n","Epoch [1726/2000], Train Loss: 0.3550\n","Epoch [1727/2000], Train Loss: 0.3560\n","Epoch [1728/2000], Train Loss: 0.3542\n","Epoch [1729/2000], Train Loss: 0.3541\n","Epoch [1730/2000], Train Loss: 0.3567\n","Epoch [1731/2000], Train Loss: 0.3563\n","Epoch [1732/2000], Train Loss: 0.3520\n","Epoch [1733/2000], Train Loss: 0.3574\n","Epoch [1734/2000], Train Loss: 0.3544\n","Epoch [1735/2000], Train Loss: 0.3558\n","Epoch [1736/2000], Train Loss: 0.3546\n","Epoch [1737/2000], Train Loss: 0.3530\n","Epoch [1738/2000], Train Loss: 0.3528\n","Epoch [1739/2000], Train Loss: 0.3543\n","Epoch [1740/2000], Train Loss: 0.3534\n","Epoch [1741/2000], Train Loss: 0.3553\n","Epoch [1742/2000], Train Loss: 0.3526\n","Epoch [1743/2000], Train Loss: 0.3534\n","Epoch [1744/2000], Train Loss: 0.3541\n","Epoch [1745/2000], Train Loss: 0.3550\n","Epoch [1746/2000], Train Loss: 0.3533\n","Epoch [1747/2000], Train Loss: 0.3548\n","Epoch [1748/2000], Train Loss: 0.3535\n","Epoch [1749/2000], Train Loss: 0.3552\n","Epoch [1750/2000], Train Loss: 0.3532\n","Epoch [1751/2000], Train Loss: 0.3518\n","Epoch [1752/2000], Train Loss: 0.3556\n","Epoch [1753/2000], Train Loss: 0.3582\n","Epoch [1754/2000], Train Loss: 0.3524\n","Epoch [1755/2000], Train Loss: 0.3562\n","Epoch [1756/2000], Train Loss: 0.3526\n","Epoch [1757/2000], Train Loss: 0.3528\n","Epoch [1758/2000], Train Loss: 0.3529\n","Epoch [1759/2000], Train Loss: 0.3526\n","Epoch [1760/2000], Train Loss: 0.3545\n","Epoch [1761/2000], Train Loss: 0.3537\n","Epoch [1762/2000], Train Loss: 0.3551\n","Epoch [1763/2000], Train Loss: 0.3555\n","Epoch [1764/2000], Train Loss: 0.3539\n","Epoch [1765/2000], Train Loss: 0.3511\n","Epoch [1766/2000], Train Loss: 0.3540\n","Epoch [1767/2000], Train Loss: 0.3517\n","Epoch [1768/2000], Train Loss: 0.3520\n","Epoch [1769/2000], Train Loss: 0.3505\n","Epoch [1770/2000], Train Loss: 0.3524\n","Epoch [1771/2000], Train Loss: 0.3562\n","Epoch [1772/2000], Train Loss: 0.3524\n","Epoch [1773/2000], Train Loss: 0.3557\n","Epoch [1774/2000], Train Loss: 0.3547\n","Epoch [1775/2000], Train Loss: 0.3510\n","Epoch [1776/2000], Train Loss: 0.3512\n","Epoch [1777/2000], Train Loss: 0.3531\n","Epoch [1778/2000], Train Loss: 0.3539\n","Epoch [1779/2000], Train Loss: 0.3555\n","Epoch [1780/2000], Train Loss: 0.3534\n","Epoch [1781/2000], Train Loss: 0.3525\n","Epoch [1782/2000], Train Loss: 0.3521\n","Epoch [1783/2000], Train Loss: 0.3546\n","Epoch [1784/2000], Train Loss: 0.3528\n","Epoch [1785/2000], Train Loss: 0.3521\n","Epoch [1786/2000], Train Loss: 0.3523\n","Epoch [1787/2000], Train Loss: 0.3565\n","Epoch [1788/2000], Train Loss: 0.3521\n","Epoch [1789/2000], Train Loss: 0.3568\n","Epoch [1790/2000], Train Loss: 0.3540\n","Epoch [1791/2000], Train Loss: 0.3540\n","Epoch [1792/2000], Train Loss: 0.3513\n","Epoch [1793/2000], Train Loss: 0.3535\n","Epoch [1794/2000], Train Loss: 0.3528\n","Epoch [1795/2000], Train Loss: 0.3526\n","Epoch [1796/2000], Train Loss: 0.3513\n","Epoch [1797/2000], Train Loss: 0.3523\n","Epoch [1798/2000], Train Loss: 0.3522\n","Epoch [1799/2000], Train Loss: 0.3513\n","Epoch [1800/2000], Train Loss: 0.3509\n","Epoch [1801/2000], Train Loss: 0.3548\n","Epoch [1802/2000], Train Loss: 0.3536\n","Epoch [1803/2000], Train Loss: 0.3505\n","Epoch [1804/2000], Train Loss: 0.3512\n","Epoch [1805/2000], Train Loss: 0.3515\n","Epoch [1806/2000], Train Loss: 0.3511\n","Epoch [1807/2000], Train Loss: 0.3548\n","Epoch [1808/2000], Train Loss: 0.3546\n","Epoch [1809/2000], Train Loss: 0.3528\n","Epoch [1810/2000], Train Loss: 0.3508\n","Epoch [1811/2000], Train Loss: 0.3535\n","Epoch [1812/2000], Train Loss: 0.3522\n","Epoch [1813/2000], Train Loss: 0.3531\n","Epoch [1814/2000], Train Loss: 0.3533\n","Epoch [1815/2000], Train Loss: 0.3534\n","Epoch [1816/2000], Train Loss: 0.3539\n","Epoch [1817/2000], Train Loss: 0.3533\n","Epoch [1818/2000], Train Loss: 0.3516\n","Epoch [1819/2000], Train Loss: 0.3518\n","Epoch [1820/2000], Train Loss: 0.3524\n","Epoch [1821/2000], Train Loss: 0.3511\n","Epoch [1822/2000], Train Loss: 0.3516\n","Epoch [1823/2000], Train Loss: 0.3523\n","Epoch [1824/2000], Train Loss: 0.3498\n","Epoch [1825/2000], Train Loss: 0.3543\n","Epoch [1826/2000], Train Loss: 0.3510\n","Epoch [1827/2000], Train Loss: 0.3565\n","Epoch [1828/2000], Train Loss: 0.3559\n","Epoch [1829/2000], Train Loss: 0.3521\n","Epoch [1830/2000], Train Loss: 0.3500\n","Epoch [1831/2000], Train Loss: 0.3504\n","Epoch [1832/2000], Train Loss: 0.3527\n","Epoch [1833/2000], Train Loss: 0.3522\n","Epoch [1834/2000], Train Loss: 0.3524\n","Epoch [1835/2000], Train Loss: 0.3507\n","Epoch [1836/2000], Train Loss: 0.3528\n","Epoch [1837/2000], Train Loss: 0.3518\n","Epoch [1838/2000], Train Loss: 0.3519\n","Epoch [1839/2000], Train Loss: 0.3514\n","Epoch [1840/2000], Train Loss: 0.3537\n","Epoch [1841/2000], Train Loss: 0.3517\n","Epoch [1842/2000], Train Loss: 0.3514\n","Epoch [1843/2000], Train Loss: 0.3514\n","Epoch [1844/2000], Train Loss: 0.3512\n","Epoch [1845/2000], Train Loss: 0.3539\n","Epoch [1846/2000], Train Loss: 0.3517\n","Epoch [1847/2000], Train Loss: 0.3508\n","Epoch [1848/2000], Train Loss: 0.3523\n","Epoch [1849/2000], Train Loss: 0.3509\n","Epoch [1850/2000], Train Loss: 0.3523\n","Epoch [1851/2000], Train Loss: 0.3525\n","Epoch [1852/2000], Train Loss: 0.3509\n","Epoch [1853/2000], Train Loss: 0.3499\n","Epoch [1854/2000], Train Loss: 0.3509\n","Epoch [1855/2000], Train Loss: 0.3501\n","Epoch [1856/2000], Train Loss: 0.3522\n","Epoch [1857/2000], Train Loss: 0.3510\n","Epoch [1858/2000], Train Loss: 0.3520\n","Epoch [1859/2000], Train Loss: 0.3503\n","Epoch [1860/2000], Train Loss: 0.3498\n","Epoch [1861/2000], Train Loss: 0.3527\n","Epoch [1862/2000], Train Loss: 0.3546\n","Epoch [1863/2000], Train Loss: 0.3517\n","Epoch [1864/2000], Train Loss: 0.3487\n","Epoch [1865/2000], Train Loss: 0.3524\n","Epoch [1866/2000], Train Loss: 0.3491\n","Epoch [1867/2000], Train Loss: 0.3505\n","Epoch [1868/2000], Train Loss: 0.3527\n","Epoch [1869/2000], Train Loss: 0.3502\n","Epoch [1870/2000], Train Loss: 0.3510\n","Epoch [1871/2000], Train Loss: 0.3500\n","Epoch [1872/2000], Train Loss: 0.3520\n","Epoch [1873/2000], Train Loss: 0.3488\n","Epoch [1874/2000], Train Loss: 0.3484\n","Epoch [1875/2000], Train Loss: 0.3516\n","Epoch [1876/2000], Train Loss: 0.3501\n","Epoch [1877/2000], Train Loss: 0.3493\n","Epoch [1878/2000], Train Loss: 0.3486\n","Epoch [1879/2000], Train Loss: 0.3503\n","Epoch [1880/2000], Train Loss: 0.3558\n","Epoch [1881/2000], Train Loss: 0.3499\n","Epoch [1882/2000], Train Loss: 0.3486\n","Epoch [1883/2000], Train Loss: 0.3500\n","Epoch [1884/2000], Train Loss: 0.3495\n","Epoch [1885/2000], Train Loss: 0.3496\n","Epoch [1886/2000], Train Loss: 0.3501\n","Epoch [1887/2000], Train Loss: 0.3515\n","Epoch [1888/2000], Train Loss: 0.3465\n","Epoch [1889/2000], Train Loss: 0.3461\n","Epoch [1890/2000], Train Loss: 0.3477\n","Epoch [1891/2000], Train Loss: 0.3498\n","Epoch [1892/2000], Train Loss: 0.3466\n","Epoch [1893/2000], Train Loss: 0.3502\n","Epoch [1894/2000], Train Loss: 0.3511\n","Epoch [1895/2000], Train Loss: 0.3529\n","Epoch [1896/2000], Train Loss: 0.3488\n","Epoch [1897/2000], Train Loss: 0.3480\n","Epoch [1898/2000], Train Loss: 0.3483\n","Epoch [1899/2000], Train Loss: 0.3482\n","Epoch [1900/2000], Train Loss: 0.3489\n","Epoch [1901/2000], Train Loss: 0.3511\n","Epoch [1902/2000], Train Loss: 0.3520\n","Epoch [1903/2000], Train Loss: 0.3478\n","Epoch [1904/2000], Train Loss: 0.3493\n","Epoch [1905/2000], Train Loss: 0.3513\n","Epoch [1906/2000], Train Loss: 0.3520\n","Epoch [1907/2000], Train Loss: 0.3488\n","Epoch [1908/2000], Train Loss: 0.3499\n","Epoch [1909/2000], Train Loss: 0.3520\n","Epoch [1910/2000], Train Loss: 0.3472\n","Epoch [1911/2000], Train Loss: 0.3485\n","Epoch [1912/2000], Train Loss: 0.3477\n","Epoch [1913/2000], Train Loss: 0.3481\n","Epoch [1914/2000], Train Loss: 0.3532\n","Epoch [1915/2000], Train Loss: 0.3498\n","Epoch [1916/2000], Train Loss: 0.3476\n","Epoch [1917/2000], Train Loss: 0.3526\n","Epoch [1918/2000], Train Loss: 0.3476\n","Epoch [1919/2000], Train Loss: 0.3491\n","Epoch [1920/2000], Train Loss: 0.3478\n","Epoch [1921/2000], Train Loss: 0.3505\n","Epoch [1922/2000], Train Loss: 0.3481\n","Epoch [1923/2000], Train Loss: 0.3497\n","Epoch [1924/2000], Train Loss: 0.3476\n","Epoch [1925/2000], Train Loss: 0.3513\n","Epoch [1926/2000], Train Loss: 0.3510\n","Epoch [1927/2000], Train Loss: 0.3506\n","Epoch [1928/2000], Train Loss: 0.3500\n","Epoch [1929/2000], Train Loss: 0.3473\n","Epoch [1930/2000], Train Loss: 0.3481\n","Epoch [1931/2000], Train Loss: 0.3466\n","Epoch [1932/2000], Train Loss: 0.3462\n","Epoch [1933/2000], Train Loss: 0.3486\n","Epoch [1934/2000], Train Loss: 0.3474\n","Epoch [1935/2000], Train Loss: 0.3490\n","Epoch [1936/2000], Train Loss: 0.3468\n","Epoch [1937/2000], Train Loss: 0.3478\n","Epoch [1938/2000], Train Loss: 0.3485\n","Epoch [1939/2000], Train Loss: 0.3491\n","Epoch [1940/2000], Train Loss: 0.3493\n","Epoch [1941/2000], Train Loss: 0.3503\n","Epoch [1942/2000], Train Loss: 0.3471\n","Epoch [1943/2000], Train Loss: 0.3461\n","Epoch [1944/2000], Train Loss: 0.3517\n","Epoch [1945/2000], Train Loss: 0.3491\n","Epoch [1946/2000], Train Loss: 0.3508\n","Epoch [1947/2000], Train Loss: 0.3489\n","Epoch [1948/2000], Train Loss: 0.3460\n","Epoch [1949/2000], Train Loss: 0.3479\n","Epoch [1950/2000], Train Loss: 0.3434\n","Epoch [1951/2000], Train Loss: 0.3437\n","Epoch [1952/2000], Train Loss: 0.3480\n","Epoch [1953/2000], Train Loss: 0.3500\n","Epoch [1954/2000], Train Loss: 0.3475\n","Epoch [1955/2000], Train Loss: 0.3476\n","Epoch [1956/2000], Train Loss: 0.3469\n","Epoch [1957/2000], Train Loss: 0.3484\n","Epoch [1958/2000], Train Loss: 0.3464\n","Epoch [1959/2000], Train Loss: 0.3506\n","Epoch [1960/2000], Train Loss: 0.3464\n","Epoch [1961/2000], Train Loss: 0.3478\n","Epoch [1962/2000], Train Loss: 0.3464\n","Epoch [1963/2000], Train Loss: 0.3445\n","Epoch [1964/2000], Train Loss: 0.3432\n","Epoch [1965/2000], Train Loss: 0.3495\n","Epoch [1966/2000], Train Loss: 0.3468\n","Epoch [1967/2000], Train Loss: 0.3486\n","Epoch [1968/2000], Train Loss: 0.3476\n","Epoch [1969/2000], Train Loss: 0.3483\n","Epoch [1970/2000], Train Loss: 0.3457\n","Epoch [1971/2000], Train Loss: 0.3446\n","Epoch [1972/2000], Train Loss: 0.3466\n","Epoch [1973/2000], Train Loss: 0.3454\n","Epoch [1974/2000], Train Loss: 0.3459\n","Epoch [1975/2000], Train Loss: 0.3450\n","Epoch [1976/2000], Train Loss: 0.3477\n","Epoch [1977/2000], Train Loss: 0.3471\n","Epoch [1978/2000], Train Loss: 0.3465\n","Epoch [1979/2000], Train Loss: 0.3461\n","Epoch [1980/2000], Train Loss: 0.3457\n","Epoch [1981/2000], Train Loss: 0.3469\n","Epoch [1982/2000], Train Loss: 0.3487\n","Epoch [1983/2000], Train Loss: 0.3457\n","Epoch [1984/2000], Train Loss: 0.3458\n","Epoch [1985/2000], Train Loss: 0.3444\n","Epoch [1986/2000], Train Loss: 0.3449\n","Epoch [1987/2000], Train Loss: 0.3464\n","Epoch [1988/2000], Train Loss: 0.3474\n","Epoch [1989/2000], Train Loss: 0.3478\n","Epoch [1990/2000], Train Loss: 0.3466\n","Epoch [1991/2000], Train Loss: 0.3471\n","Epoch [1992/2000], Train Loss: 0.3480\n","Epoch [1993/2000], Train Loss: 0.3440\n","Epoch [1994/2000], Train Loss: 0.3461\n","Epoch [1995/2000], Train Loss: 0.3447\n","Epoch [1996/2000], Train Loss: 0.3451\n","Epoch [1997/2000], Train Loss: 0.3458\n","Epoch [1998/2000], Train Loss: 0.3464\n","Epoch [1999/2000], Train Loss: 0.3440\n","Epoch [2000/2000], Train Loss: 0.3464\n","Test Accuracy: 0.8031\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d3a8aac031be45a19e66f495aed88416","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.083 MB of 0.083 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>train_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1999</td></tr><tr><td>train_loss</td><td>0.34643</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">trim-sea-12</strong> at: <a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-exp-1/runs/6dhw7563' target=\"_blank\">https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-exp-1/runs/6dhw7563</a><br/> View project at: <a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-exp-1' target=\"_blank\">https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-exp-1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241029_101009-6dhw7563/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["model = model_pipeline(config)"]},{"cell_type":"markdown","metadata":{"id":"Rn-KrUz59yds"},"source":["## Save the model"]},{"cell_type":"code","execution_count":53,"metadata":{"collapsed":true,"executionInfo":{"elapsed":7,"status":"ok","timestamp":1730197613430,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"AwFKsiy3LVKk"},"outputs":[],"source":["# Save the model\n","torch.save(model.state_dict(), 'lstm_model.pth')\n","\n","# # Save the model as onnx\n","# torch.onnx.export(model, X_train, 'lstm_model.onnx')"]},{"cell_type":"markdown","metadata":{"id":"faC6EfmDMsRL"},"source":["## Run inference"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1730197613430,"user":{"displayName":"DANIEL MORENO PARIS","userId":"17921422726169854440"},"user_tz":-60},"id":"nxNfBFvFowYp","outputId":"f22177fb-ecf1-46e3-f10a-7b8ab98ca7af"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([ 5,  5,  0,  ..., 16, 16, 16], device='cuda:0')\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-54-f0ed6b1134de>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(input_path))\n"]}],"source":["# Run model Inference\n","\n","# Load test data\n","\n","\n","\n","# Load pretrained model\n","input_path = '/content/lstm_model.pth'\n","model.load_state_dict(torch.load(input_path))\n","model.eval()\n","\n","with torch.no_grad():\n","    X = X.to(device)\n","    outputs = model(X)\n","    _, predicted = torch.max(outputs.data, 1)\n","print(predicted)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"003686f77a4b4fd7ac837ad65d707b02":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"08677188194645c49bc77537acbdcea3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bcb4f140af44970a195805336fa2f45":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90b548504b704a07b637a94a5734a5a5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af16d21180564b818b3dd48fb935a3dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0bcb4f140af44970a195805336fa2f45","placeholder":"","style":"IPY_MODEL_b2b8d7f483b84089b346dbd7d53a6e58","value":"0.083 MB of 0.083 MB uploaded\r"}},"b2b8d7f483b84089b346dbd7d53a6e58":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb140d7ce2fb4340955b7f629a793096":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_08677188194645c49bc77537acbdcea3","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_003686f77a4b4fd7ac837ad65d707b02","value":1}},"d3a8aac031be45a19e66f495aed88416":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_af16d21180564b818b3dd48fb935a3dd","IPY_MODEL_cb140d7ce2fb4340955b7f629a793096"],"layout":"IPY_MODEL_90b548504b704a07b637a94a5734a5a5"}}}}},"nbformat":4,"nbformat_minor":0}
