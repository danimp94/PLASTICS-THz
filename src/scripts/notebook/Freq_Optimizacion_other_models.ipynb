{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peUcVKtbGPfD"
   },
   "source": [
    "# Important Frequencies Selection\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we need to install the necessary libraries. Run the following cell to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55915,
     "status": "ok",
     "timestamp": 1731066537611,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "aAZ2n50rGS-C",
    "outputId": "de4fca74-da23-4517-a0ea-55d44a50028c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (0.20.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (2.2.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: joblib in c:\\users\\danim\\documents\\github\\pic-paper-01\\.venv\\lib\\site-packages (1.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio\n",
    "%pip install pandas scikit-learn\n",
    "%pip install wandb onnx -Uq\n",
    "%pip install joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4d3KxhxGML8"
   },
   "source": [
    "## Import Libraries and seed\n",
    "Import the necessary libraries for data processing, model building, training, and evaluation. Adding a seed ensures reproducibility by making sure that the random number generation is consistent across different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15527,
     "status": "ok",
     "timestamp": 1731066553135,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "9a9HvzrNG8iw",
    "outputId": "19350658-7b5d-48e7-e90c-e7ddb3a578b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from collections import Counter\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "import wandb\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    return seed\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1731066584724,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "hAY8BfVX9XCK"
   },
   "outputs": [],
   "source": [
    "def load_data_from_directory(input_path):\n",
    "    data_frames = []\n",
    "    for file in os.listdir(input_path):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(input_path, file), delimiter=';', header=0)\n",
    "            data_frames.append(df)\n",
    "    data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    print(data)\n",
    "    print(data.shape)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiCTw-qcKXhn"
   },
   "source": [
    "## Preprocessing Data\n",
    "Define a function to preprocess the data. This includes encoding categorical labels and standardizing the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1731066584724,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "zJoZqi5LDPl8"
   },
   "outputs": [],
   "source": [
    "def calculate_averages_and_dispersion(data, data_percentage):\n",
    "    df = data\n",
    "    results = []\n",
    "    for (sample, freq), group in df.groupby(['Sample', 'Frequency (GHz)']):\n",
    "        window_size = max(1, int(len(group) * data_percentage / 100))\n",
    "        # print(f\"Processing sample: {sample}, frequency: {freq} with window size: {window_size}\")\n",
    "        for start in range(0, len(group), window_size):\n",
    "            window_data = group.iloc[start:start + window_size]\n",
    "            mean_values = window_data[['LG (mV)', 'HG (mV)']].mean()\n",
    "            std_deviation_values = window_data[['LG (mV)', 'HG (mV)']].std()\n",
    "            results.append({\n",
    "                'Frequency (GHz)': freq,\n",
    "                'LG (mV) mean': mean_values['LG (mV)'],\n",
    "                'HG (mV) mean': mean_values['HG (mV)'],\n",
    "                'LG (mV) std deviation': std_deviation_values['LG (mV)'],\n",
    "                'HG (mV) std deviation': std_deviation_values['HG (mV)'],\n",
    "                # 'Thickness (mm)': window_data['Thickness (mm)'].iloc[0], ## COMMENT\n",
    "                'Sample': sample,\n",
    "            })\n",
    "    results_df = pd.DataFrame(results)\n",
    "    # results_df.to_csv(output_file, sep=';', index=False)\n",
    "    # print(f\"Processed {input_file} and saved to {output_file}\")\n",
    "    # print(results_df)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1731066584724,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "KZPdMwkkKmm7"
   },
   "outputs": [],
   "source": [
    "def split_label(df):\n",
    "\n",
    "    # Assuming the last column is the target\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:, -1].values\n",
    "\n",
    "    # Encode the target variable if it's categorical\n",
    "    if y.dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "    # le is the fitted LabelEncoder - Saving Encoder\n",
    "    joblib.dump(le, 'label_encoder.pkl')\n",
    "\n",
    "    # Normalization TBD\n",
    "    # # Standardize the features\n",
    "    # scaler = StandardScaler()\n",
    "    # X = scaler.fit_transform(X)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXiN30xA1kKX"
   },
   "source": [
    "## Pivoting Frequency values to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1731067118965,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "Kijzt73J1nyo"
   },
   "outputs": [],
   "source": [
    "def freq_as_variable(df, data_percentage):\n",
    "    '''Modify df to have Frequency values (100,110,120 and so on) as and input variables in the columns'''\n",
    "\n",
    "    # Remove Thickness column\n",
    "    if 'Thickness (mm)' in df.columns:\n",
    "        df = df.drop(columns=['Thickness (mm)'])\n",
    "\n",
    "    # 1s window_size 100/27s = 3.7% of the data is used for each window\n",
    "    df_window = calculate_averages_and_dispersion(df, data_percentage = 3.7) \n",
    "\n",
    "    print(df_window)\n",
    "\n",
    "    # Add a unique identifier column to avoid duplicate entries in the index\n",
    "    df_window['unique_id'] = df_window.groupby(['Sample', 'Frequency (GHz)']).cumcount()\n",
    "\n",
    "    # Pivot the DataFrame to wide format\n",
    "    df_pivot = df_window.pivot(index=['Sample', 'unique_id'], columns='Frequency (GHz)')\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    df_pivot.columns = [' '.join(map(str, col)) for col in df_pivot.columns]\n",
    "\n",
    "    # Drop columns with all NaN values\n",
    "    df_pivot = df_pivot.dropna(axis=1, how='all')\n",
    "\n",
    "    # Reset index to make 'Sample' and 'unique_id' columns again\n",
    "    df_pivot = df_pivot.reset_index()\n",
    "\n",
    "    # Optional - Sort the columns if needed\n",
    "    df_pivot = df_pivot.reindex(sorted(df_pivot.columns), axis=1)\n",
    "\n",
    "    # Remove 'unique_id' column\n",
    "    df_pivot = df_pivot.drop(columns=['unique_id'])\n",
    "\n",
    "    return df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 652
    },
    "executionInfo": {
     "elapsed": 52182,
     "status": "error",
     "timestamp": 1731067172783,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "UTXvxCzu6MBA",
    "outputId": "41925df4-d9a4-424b-fc94-bf98e95a101b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Sample  Frequency (GHz)     LG (mV)    HG (mV)  Thickness (mm)\n",
      "0           A1            100.0   -7.080942  -0.854611             0.2\n",
      "1           A1            100.0   67.024785   0.244141             0.2\n",
      "2           A1            100.0  124.893178  -1.098776             0.2\n",
      "3           A1            100.0   91.075571   0.000000             0.2\n",
      "4           A1            100.0   48.956174   0.122094             0.2\n",
      "...        ...              ...         ...        ...             ...\n",
      "2420620    REF            600.0    0.366256  16.237333             0.0\n",
      "2420621    REF            600.0    0.000000  -7.080942             0.0\n",
      "2420622    REF            600.0   -0.244170  15.260652             0.0\n",
      "2420623    REF            600.0    0.366256  20.021975             0.0\n",
      "2420624    REF            600.0    0.122085  13.185203             0.0\n",
      "\n",
      "[2420625 rows x 5 columns]\n",
      "(2420625, 5)\n",
      "       Frequency (GHz)  LG (mV) mean  HG (mV) mean  LG (mV) std deviation  \\\n",
      "0                100.0     54.879155     -0.022198              29.958659   \n",
      "1                100.0     54.511665      0.048093              28.096155   \n",
      "2                100.0     55.099894     -0.118380              26.833871   \n",
      "3                100.0     48.387674      0.103588              28.843498   \n",
      "4                100.0     49.932853     -0.071525              23.093397   \n",
      "...                ...           ...           ...                    ...   \n",
      "21415            600.0     -0.006143     10.237498               0.890999   \n",
      "21416            600.0     -0.006910     10.949278               0.858148   \n",
      "21417            600.0      0.029178     10.547702               0.842082   \n",
      "21418            600.0      0.065266     10.051683               0.891632   \n",
      "21419            600.0     -0.228910      9.766817               0.856929   \n",
      "\n",
      "       HG (mV) std deviation Sample  \n",
      "0                   0.644211     A1  \n",
      "1                   0.704742     A1  \n",
      "2                   0.755493     A1  \n",
      "3                   0.854655     A1  \n",
      "4                   0.650231     A1  \n",
      "...                      ...    ...  \n",
      "21415               8.205224    REF  \n",
      "21416               8.679982    REF  \n",
      "21417               8.802972    REF  \n",
      "21418               8.365464    REF  \n",
      "21419               9.725526    REF  \n",
      "\n",
      "[21420 rows x 6 columns]\n",
      "X : [[-2.21984593e-02 -1.30394159e-02 -1.40432818e-02 ...  8.14279665e-01\n",
      "   9.56395968e-01  2.98414595e+01]\n",
      " [ 4.80931111e-02 -3.08205748e-02 -8.42702534e-02 ...  8.89085054e-01\n",
      "   8.33620436e-01  8.36258918e-01]\n",
      " [-1.18380115e-01  2.72626065e-02  5.40283931e-03 ...  8.25771198e-01\n",
      "   8.85385266e-01  8.75094315e-01]\n",
      " ...\n",
      " [-9.24219533e-02 -1.13703180e-01 -7.67055426e-02 ...  8.03971580e-01\n",
      "   8.33043253e-01  8.42082041e-01]\n",
      " [-4.22201604e-02  5.62536842e-02 -1.18825580e-02 ...  8.97857647e-01\n",
      "   9.17720546e-01  8.91631588e-01]\n",
      " [ 1.75498375e-01 -4.66149411e-01 -3.79840556e-01 ...  1.00074348e+00\n",
      "   8.75430012e-01  8.56929097e-01]]\n",
      "y : [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  2  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  3  3\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  4  4  4  4  4  4  4  4\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "  6  6  6  6  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  9  9  9  9  9  9  9  9  9  9  9  9\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n",
      " 12 12 12 12 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13\n",
      " 13 13 13 13 13 13 13 13 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14]\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the directory\n",
    "input_path = 'C:/Users/Danim/Documents/GitHub/PIC-PAPER-01/data/experiment_1_plastics/processed_27s/training_file/test/'\n",
    "df = load_data_from_directory(input_path)\n",
    "\n",
    "# Introduce Frequency values as input variables\n",
    "df = freq_as_variable(df, data_percentage=3.7)\n",
    "# print(f'data : {df}')\n",
    "\n",
    "# Save the processed data to a CSV file\n",
    "output_path = 'C:/Users/Danim/Documents/GitHub/PIC-PAPER-01/data/experiment_1_plastics/processed_27s/training_file/test.csv'\n",
    "df.to_csv(output_path, sep = ';', index=False)\n",
    "\n",
    "# Split the data into X and y\n",
    "X, y = split_label(df)\n",
    "# print(f'data : {df}')\n",
    "print(f'X : {X}')\n",
    "print(f'y : {y}')\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# X_groups = torch.tensor(X_groups, dtype=torch.float32)\n",
    "# y_groups = torch.tensor(y_groups, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login in Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "executionInfo": {
     "elapsed": 12301,
     "status": "ok",
     "timestamp": 1731066584285,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "AoCyZSTt7qqS",
    "outputId": "c7c7febf-6ef9-4081-c578-72109a882497"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_API_KEY=94b4debef3cc9601df4d91995649548f8ab3a097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "### SKIP FOR NOW ###\n",
    "###\n",
    "\n",
    "%env WANDB_API_KEY=94b4debef3cc9601df4d91995649548f8ab3a097\n",
    "wandb.login()\n",
    "\n",
    "# wandb.init(project='PIC-PAPER-01', entity='UC3M', name='RandomForest')\n",
    "# wandb.watch(rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afvxx1KzKzYp"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3a9IuRVK3Wq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Actual  Predicted\n",
      "0       5          5\n",
      "1      11         11\n",
      "2       6          6\n",
      "3      13         13\n",
      "4      14         14\n",
      "5       2          2\n",
      "6       4          4\n",
      "7       4          4\n",
      "8       1          1\n",
      "9       2          2\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       1.00      1.00      1.00        10\n",
      "           3       1.00      1.00      1.00         3\n",
      "           4       1.00      1.00      1.00         4\n",
      "           5       1.00      1.00      1.00         2\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      1.00      1.00         3\n",
      "           8       1.00      1.00      1.00         4\n",
      "           9       1.00      1.00      1.00         4\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       1.00      1.00      1.00         4\n",
      "          12       1.00      1.00      1.00         4\n",
      "          13       1.00      1.00      1.00         4\n",
      "          14       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           1.00        63\n",
      "   macro avg       1.00      1.00      1.00        63\n",
      "weighted avg       1.00      1.00      1.00        63\n",
      "\n",
      "[[ 5  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  4  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  4  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  5  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  4  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  3  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  4  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  4  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  4]]\n"
     ]
    }
   ],
   "source": [
    "# Model Parameters\n",
    "input_size = X.shape[1]\n",
    "output_size = len(np.unique(y))\n",
    "seed = set_seed(42)\n",
    "# print(f'input_size: {input_size}, output_size: {output_size}')\n",
    "# print(X_train, y_train)\n",
    "\n",
    "# Define Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, criterion='gini', random_state=seed)\n",
    "\n",
    "# # Define Naive-Bayes model\n",
    "# nb_model = GaussianNB()\n",
    "\n",
    "# # Define Logistic Regression model\n",
    "# lr_model = LogisticRegression(max_iter=1000, random_state = seed)\n",
    "\n",
    "\n",
    "# Train model using wandb for metrics\n",
    "# TBD \n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Create a DataFrame to compare\n",
    "df_comparison = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "print(df_comparison.head(10))\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)\n",
    "\n",
    "# Feature importance\n",
    "\n",
    "\n",
    "# Log metrics wandb\n",
    "# wandb.log({\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1})\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess Other Test Data (Experiment 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Confusion Matrix\n",
    "# conf_matrix = confusion_matrix(le.inverse_transform(y_test.cpu().numpy()), predicted_labels, labels=le.classes_)\n",
    "\n",
    "# # Confusion matrix with matplotlib\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.colorbar()\n",
    "# tick_marks = np.arange(len(le.classes_))\n",
    "# plt.xticks(tick_marks, le.classes_, rotation=45)\n",
    "# plt.yticks(tick_marks, le.classes_)\n",
    "\n",
    "# # Normalize the confusion matrix\n",
    "# conf_matrix_normalized = np.nan_to_num(conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis])\n",
    "\n",
    "# # Print the normalized values inside the matrix as percentages\n",
    "# thresh = conf_matrix.max() / 2.\n",
    "# for i, j in np.ndindex(conf_matrix.shape):\n",
    "#     plt.text(j, i, f\"{conf_matrix[i, j]}\\n ({conf_matrix_normalized[i, j] * 100:.1f}%)\",\n",
    "#              horizontalalignment=\"center\",\n",
    "#              verticalalignment=\"center\",\n",
    "#              fontsize=9,\n",
    "#              color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "# plt.ylabel('True label')\n",
    "# plt.xlabel('Predicted label')\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEtZPi2fLQGL"
   },
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktsgpY21NcOV"
   },
   "source": [
    "| Original Label | Encoded Value |\n",
    "|----------------|---------------|\n",
    "| A1             | 0             |\n",
    "| B1             | 1             |\n",
    "| C1             | 2             |\n",
    "| D1             | 3             |\n",
    "| E1             | 4             |\n",
    "| F1             | 5             |\n",
    "| G1             | 6             |\n",
    "| H1             | 7             |\n",
    "| I1             | 8             |\n",
    "| J1             | 9             |\n",
    "| K1             | 10            |\n",
    "| L1             | 11            |\n",
    "| M1             | 12            |\n",
    "| N1             | 13            |\n",
    "| REF            | 14            |"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
