{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peUcVKtbGPfD"
      },
      "source": [
        "# LSTM Classification Model\n",
        "\n",
        "This notebook demonstrates how to load data, preprocess it, define an LSTM model, train the model, and evaluate its performance. The data is assumed to be in CSV format and stored in a directory.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, we need to install the necessary libraries. Run the following cell to install them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAZ2n50rGS-C",
        "outputId": "ac4576e3-7161-4e46-f28b-6d4c8cd04f04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install torch torchvision torchaudio\n",
        "%pip install pandas scikit-learn\n",
        "%pip install wandb onnx -Uq\n",
        "%pip install joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4d3KxhxGML8"
      },
      "source": [
        "## Import Libraries and seed\n",
        "Import the necessary libraries for data processing, model building, training, and evaluation. Adding a seed ensures reproducibility by making sure that the random number generation is consistent across different runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a9HvzrNG8iw",
        "outputId": "21bc86e6-802a-4c13-87d0-15ded3f83e59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import wandb\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUQtUPxSBzq4",
        "outputId": "8903074c-5db2-4352-fde1-7bab7cad099b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "AoCyZSTt7qqS",
        "outputId": "8118b196-559f-4305-ebc1-e0c13767f63b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "wandb.login()\n",
        "#94b4debef3cc9601df4d91995649548f8ab3a097"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1-6Xv6LHHmZ"
      },
      "source": [
        "## Load Data from Github Repository\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eI7hrAaVHL44"
      },
      "outputs": [],
      "source": [
        "## Remove PIC-PAPER-01 folder:\n",
        "!rm -rf PIC-PAPER-01\n",
        "\n",
        "# # Download Github Repo (Private) https://stackoverflow.com/questions/74532852/clone-github-repo-with-fine-grained-token/78280453#78280453\n",
        "# !git clone --no-checkout https://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@github.com/danimp94/PIC-PAPER-01.git\n",
        "\n",
        "# # To clone data folder only:\n",
        "# %cd PIC-PAPER-01 # Navigate to the repository directory\n",
        "# !git sparse-checkout init --cone # Initialize sparse-checkout\n",
        "# !git sparse-checkout set data # Set the sparse-checkout to include only the data/ folder\n",
        "# !git checkout # Checkout the specified folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hAY8BfVX9XCK"
      },
      "outputs": [],
      "source": [
        "def load_data_from_directory(input_path):\n",
        "    data_frames = []\n",
        "    for file in os.listdir(input_path):\n",
        "        if file.endswith('.csv'):\n",
        "            df = pd.read_csv(os.path.join(input_path, file), delimiter=';', header=0)\n",
        "            data_frames.append(df)\n",
        "    data = pd.concat(data_frames, ignore_index=True)\n",
        "\n",
        "    print(data)\n",
        "    print(data.shape)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiCTw-qcKXhn"
      },
      "source": [
        "## Preprocessing Data\n",
        "Define a function to preprocess the data. This includes encoding categorical labels and standardizing the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zJoZqi5LDPl8"
      },
      "outputs": [],
      "source": [
        "def calculate_averages_and_dispersion(data, data_percentage):\n",
        "    df = data\n",
        "    results = []\n",
        "    for (sample, freq), group in df.groupby(['Sample', 'Frequency (GHz)']):\n",
        "        window_size = max(1, int(len(group) * data_percentage / 100))\n",
        "        # print(f\"Processing sample: {sample}, frequency: {freq} with window size: {window_size}\")\n",
        "        for start in range(0, len(group), window_size):\n",
        "            window_data = group.iloc[start:start + window_size]\n",
        "            mean_values = window_data[['LG (mV)', 'HG (mV)']].mean()\n",
        "            std_deviation_values = window_data[['LG (mV)', 'HG (mV)']].std()\n",
        "            results.append({\n",
        "                'Frequency (GHz)': freq,\n",
        "                'LG (mV) mean': mean_values['LG (mV)'],\n",
        "                'HG (mV) mean': mean_values['HG (mV)'],\n",
        "                'LG (mV) std deviation': std_deviation_values['LG (mV)'],\n",
        "                'HG (mV) std deviation': std_deviation_values['HG (mV)'],\n",
        "                'Thickness (mm)': window_data['Thickness (mm)'].iloc[0],\n",
        "                'Sample': sample,\n",
        "            })\n",
        "    results_df = pd.DataFrame(results)\n",
        "    # results_df.to_csv(output_file, sep=';', index=False)\n",
        "    # print(f\"Processed {input_file} and saved to {output_file}\")\n",
        "    print(results_df)\n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KZPdMwkkKmm7"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data, data_percentage):\n",
        "    # Windowing the data\n",
        "    data = calculate_averages_and_dispersion(data, data_percentage)\n",
        "    print(data.shape)\n",
        "\n",
        "    # Assuming the last column is the target\n",
        "    X = data.iloc[:, :-1].values\n",
        "    y = data.iloc[:, -1].values\n",
        "\n",
        "    # Encode the target variable if it's categorical\n",
        "    if y.dtype == 'object':\n",
        "        le = LabelEncoder()\n",
        "        y = le.fit_transform(y)\n",
        "\n",
        "    # le is the fitted LabelEncoder\n",
        "    joblib.dump(le, 'label_encoder.pkl')\n",
        "\n",
        "    # Get the original labels and their encoded values\n",
        "    original_labels = le.classes_\n",
        "    encoded_values = le.transform(original_labels)\n",
        "\n",
        "    # Create a DataFrame to display the mapping\n",
        "    label_mapping_df = pd.DataFrame({\n",
        "        'Original Label': original_labels,\n",
        "        'Encoded Value': encoded_values\n",
        "    })\n",
        "\n",
        "    # Display the DataFrame\n",
        "    print(label_mapping_df)\n",
        "\n",
        "    # # Standardize the features\n",
        "    # scaler = StandardScaler()\n",
        "    # X = scaler.fit_transform(X)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X = torch.tensor(X, dtype=torch.float32)\n",
        "    y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTXvxCzu6MBA",
        "outputId": "282676d4-9b88-4add-b901-d072dd1aa79a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Sample  Frequency (GHz)     LG (mV)    HG (mV)  Thickness (mm)\n",
            "0           A1            100.0   -7.080942  -0.854611             0.2\n",
            "1           A1            100.0   67.024785   0.244141             0.2\n",
            "2           A1            100.0  124.893178  -1.098776             0.2\n",
            "3           A1            100.0   91.075571   0.000000             0.2\n",
            "4           A1            100.0   48.956174   0.122094             0.2\n",
            "...        ...              ...         ...        ...             ...\n",
            "2737958    REF            600.0    0.366256  16.237333             0.0\n",
            "2737959    REF            600.0    0.000000  -7.080942             0.0\n",
            "2737960    REF            600.0   -0.244170  15.260652             0.0\n",
            "2737961    REF            600.0    0.366256  20.021975             0.0\n",
            "2737962    REF            600.0    0.122085  13.185203             0.0\n",
            "\n",
            "[2737963 rows x 5 columns]\n",
            "(2737963, 5)\n",
            "       Frequency (GHz)  LG (mV) mean  HG (mV) mean  LG (mV) std deviation  \\\n",
            "0                100.0     54.879155     -0.022198              29.958659   \n",
            "1                100.0     54.511665      0.048093              28.096155   \n",
            "2                100.0     55.099894     -0.118380              26.833871   \n",
            "3                100.0     48.387674      0.103588              28.843498   \n",
            "4                100.0     49.932853     -0.071525              23.093397   \n",
            "...                ...           ...           ...                    ...   \n",
            "24271            600.0     -0.006143     10.237498               0.890999   \n",
            "24272            600.0     -0.006910     10.949278               0.858148   \n",
            "24273            600.0      0.029178     10.547702               0.842082   \n",
            "24274            600.0      0.065266     10.051683               0.891632   \n",
            "24275            600.0     -0.228910      9.766817               0.856929   \n",
            "\n",
            "       HG (mV) std deviation  Thickness (mm) Sample  \n",
            "0                   0.644211             0.2     A1  \n",
            "1                   0.704742             0.2     A1  \n",
            "2                   0.755493             0.2     A1  \n",
            "3                   0.854655             0.2     A1  \n",
            "4                   0.650231             0.2     A1  \n",
            "...                      ...             ...    ...  \n",
            "24271               8.205224             0.0    REF  \n",
            "24272               8.679982             0.0    REF  \n",
            "24273               8.802972             0.0    REF  \n",
            "24274               8.365464             0.0    REF  \n",
            "24275               9.725526             0.0    REF  \n",
            "\n",
            "[24276 rows x 7 columns]\n",
            "(24276, 7)\n",
            "   Original Label  Encoded Value\n",
            "0              A1              0\n",
            "1              B1              1\n",
            "2              C1              2\n",
            "3              D1              3\n",
            "4              E1              4\n",
            "5              E2              5\n",
            "6              E3              6\n",
            "7              F1              7\n",
            "8              G1              8\n",
            "9              H1              9\n",
            "10             I1             10\n",
            "11             J1             11\n",
            "12             K1             12\n",
            "13             L1             13\n",
            "14             M1             14\n",
            "15             N1             15\n",
            "16            REF             16\n"
          ]
        }
      ],
      "source": [
        "input_path = '/content/drive/MyDrive/PhD/Colab Notebooks/training_data/'\n",
        "data = load_data_from_directory(input_path)\n",
        "\n",
        "# Load and preprocess data\n",
        "X, y = preprocess_data(data, data_percentage=3.7) # 1s window size\n",
        "\n",
        "\n",
        "# print(le.classes_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZjha9Cx6MBB"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLRhwd8cKKJH",
        "outputId": "664020c9-43f5-493a-e702-9d5171913050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epochs': 100, 'seed': 48, 'classes': 17, 'k_folds': 3, 'batch_size': 32, 'sequence_length': 1, 'learning_rate': 0.001, 'dataset': 'experiment_1', 'architecture': 'LSTM', 'hidden_dim': 32}\n"
          ]
        }
      ],
      "source": [
        "config = dict(\n",
        "    epochs=100,\n",
        "    seed = 48,\n",
        "    classes = data['Sample'].nunique(), # Each different sample is a different class\n",
        "    k_folds = 3,  # Number of folds for cross-validation\n",
        "    batch_size=32,\n",
        "    sequence_length = 1,\n",
        "    learning_rate=0.001,\n",
        "    dataset=\"experiment_1\",\n",
        "    architecture=\"LSTM\",\n",
        "    hidden_dim = 32\n",
        ")\n",
        "\n",
        "print(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI96M_V6KpyH"
      },
      "source": [
        "## Define Model\n",
        "Define the LSTM model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "h6j-_U_RKxlR"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        print(f'input_dim: {input_dim}, hidden_dim: {hidden_dim}, output_dim: {output_dim}')\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers = 1, batch_first=True)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        # self.softmax = nn.Softmax(dim=1) # Add softmax for multi-class classification\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden and cell states with zeros\n",
        "        # (num_layers * num_directions, batch, hidden_size)\n",
        "        # num_layers * num_directions = 1 in this case\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_dim).to(x.device)  # Use x.size(0) for batch size\n",
        "        c0 = torch.zeros(1, x.size(0), self.hidden_dim).to(x.device)  # Use x.size(0) for batch size\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))  # Pass h0 and c0 to LSTM\n",
        "\n",
        "        # Apply dropout to the output and take only the hidden state of the last timestep\n",
        "        out = self.dropout(out[:, -1, :])  # Keep sequence dimension for proper input to fc layer\n",
        "\n",
        "        # Decode the hidden state of the last timestep\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Check how to pass input data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afvxx1KzKzYp"
      },
      "source": [
        "## Train Model\n",
        "Define a function to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-3a9IuRVK3Wq"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, device, config):\n",
        "    num_epochs = config.epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "              X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "              outputs = model(X_batch)\n",
        "              loss = criterion(outputs, y_batch)\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "              running_loss += loss.item()\n",
        "\n",
        "        val_loss = 0.0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                outputs = model(X_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        # Log metrics to W&B\n",
        "        wandb.log({\"epoch\": epoch, \"train_loss\": running_loss / len(train_loader), \"val_loss\": val_loss / len(val_loader)})\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1bDpP8JK5jp"
      },
      "source": [
        "## Evaluate Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TIDNXJGRK9af"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the model on the test set: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KBOELYWD6MBB"
      },
      "outputs": [],
      "source": [
        "def make(config, X, y):\n",
        "    # K-Fold Cross-Validation\n",
        "    kfold = KFold(n_splits=config.k_folds, shuffle=True, random_state=config.seed)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
        "        print(f'Fold {fold+1}/{config.k_folds}')\n",
        "\n",
        "        # Create DataLoader for training and validation sets\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        # Convert data to tensors and add sequence length dimension\n",
        "        X_train = torch.tensor(X_train).float()\n",
        "        X_val = torch.tensor(X_val).float()\n",
        "        y_train = torch.tensor(y_train).long()\n",
        "        y_val = torch.tensor(y_val).long()\n",
        "        print(X_train.shape)\n",
        "        print(y_train.shape)\n",
        "\n",
        "        # Reshape input tensors to (batch_size, sequence_length, num_features)\n",
        "        X_train = X_train.reshape( X_train.shape[0], config.sequence_length, X_train.shape[-1])\n",
        "        X_val = X_val.reshape( X_val.shape[0],config.sequence_length, X_val.shape[-1])\n",
        "        print(X_train.shape)\n",
        "        print(y_train.shape)\n",
        "\n",
        "        train_dataset = TensorDataset(X_train, y_train)\n",
        "        val_dataset = TensorDataset(X_val, y_val)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
        "\n",
        "        print(X_train.shape)\n",
        "        print(y_train.shape)\n",
        "        # Initialize the model, loss function, and optimizer\n",
        "        input_dim = X_train.shape[-1] # The number of expected features in the input x\n",
        "        hidden_dim = config.hidden_dim\n",
        "        output_dim = config.classes\n",
        "        model = LSTMModel(input_dim, hidden_dim, output_dim).to(device)\n",
        "        # criterion = nn.CrossEntropyLoss()\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "        yield model, train_loader, val_loader, criterion, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SlnfXqu-6MBB"
      },
      "outputs": [],
      "source": [
        "def model_pipeline(hyperparameters):\n",
        "    with wandb.init(project=\"PIC-PAPER-01-LSTM\", config=hyperparameters):\n",
        "        config = wandb.config\n",
        "\n",
        "        # Set seed for reproducibility\n",
        "        set_seed(config.seed)\n",
        "\n",
        "        # Split the data into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=config.seed)\n",
        "\n",
        "        # K-Fold Cross-Validation\n",
        "        for model, train_loader, val_loader, criterion, optimizer in make(config, X_train, y_train):\n",
        "            print(model)\n",
        "\n",
        "            # Train the model\n",
        "            train_model(model, train_loader, val_loader, criterion, optimizer, device, config)\n",
        "\n",
        "            # Evaluate the model on the validation set\n",
        "            evaluate_model(model, val_loader, device)\n",
        "\n",
        "        # Evaluate the final model on the test set\n",
        "        X_test = torch.tensor(X_test).float()\n",
        "        print('X_test shape:', X_test.shape)\n",
        "        # Reshape input tensors to (batch_size, sequence_length, num_features)\n",
        "        X_test = X_test.reshape(X_test.shape[0],config.sequence_length, X_test.shape[-1])\n",
        "        print('X_test shape:', X_test.shape)\n",
        "        y_test = torch.tensor(y_test).long()\n",
        "        test_dataset = TensorDataset(X_test, y_test)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
        "        print('test_loader shape:', test_loader)\n",
        "        # Evaluate the final model on the test set\n",
        "        evaluate_model(model, test_loader, device)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEtZPi2fLQGL"
      },
      "source": [
        "## Run Training\n",
        "\n",
        "Do not use it if just want to run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tScgXvxA6MBC",
        "outputId": "810f5abe-3dc9-4eed-a8aa-f4f14ae5a1e2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241104_124054-e7j80c91</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-LSTM/runs/e7j80c91' target=\"_blank\">wandering-voice-2</a></strong> to <a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-LSTM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-LSTM' target=\"_blank\">https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-LSTM</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-LSTM/runs/e7j80c91' target=\"_blank\">https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-LSTM/runs/e7j80c91</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/3\n",
            "torch.Size([14565, 6])\n",
            "torch.Size([14565])\n",
            "torch.Size([14565, 1, 6])\n",
            "torch.Size([14565])\n",
            "torch.Size([14565, 1, 6])\n",
            "torch.Size([14565])\n",
            "input_dim: 6, hidden_dim: 32, output_dim: 17\n",
            "LSTMModel(\n",
            "  (lstm): LSTM(6, 32, batch_first=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=32, out_features=17, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-632b0d8f4136>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_train = torch.tensor(X_train).float()\n",
            "<ipython-input-17-632b0d8f4136>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_val = torch.tensor(X_val).float()\n",
            "<ipython-input-17-632b0d8f4136>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_train = torch.tensor(y_train).long()\n",
            "<ipython-input-17-632b0d8f4136>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_val = torch.tensor(y_val).long()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Loss: 2.8271, Val Loss: 2.7973\n",
            "Epoch [2/100], Train Loss: 2.7670, Val Loss: 2.7060\n",
            "Epoch [3/100], Train Loss: 2.6490, Val Loss: 2.5533\n",
            "Epoch [4/100], Train Loss: 2.4989, Val Loss: 2.4021\n",
            "Epoch [5/100], Train Loss: 2.3909, Val Loss: 2.3145\n",
            "Epoch [6/100], Train Loss: 2.3239, Val Loss: 2.2650\n",
            "Epoch [7/100], Train Loss: 2.2877, Val Loss: 2.2230\n",
            "Epoch [8/100], Train Loss: 2.2529, Val Loss: 2.1960\n",
            "Epoch [9/100], Train Loss: 2.2186, Val Loss: 2.1552\n",
            "Epoch [10/100], Train Loss: 2.1898, Val Loss: 2.1284\n",
            "Epoch [11/100], Train Loss: 2.1605, Val Loss: 2.0728\n",
            "Epoch [12/100], Train Loss: 2.1225, Val Loss: 2.0403\n",
            "Epoch [13/100], Train Loss: 2.0859, Val Loss: 2.0295\n",
            "Epoch [14/100], Train Loss: 2.0536, Val Loss: 1.9704\n",
            "Epoch [15/100], Train Loss: 2.0276, Val Loss: 1.9244\n",
            "Epoch [16/100], Train Loss: 1.9910, Val Loss: 1.8893\n",
            "Epoch [17/100], Train Loss: 1.9609, Val Loss: 1.8637\n",
            "Epoch [18/100], Train Loss: 1.9351, Val Loss: 1.8396\n",
            "Epoch [19/100], Train Loss: 1.9050, Val Loss: 1.7846\n",
            "Epoch [20/100], Train Loss: 1.8884, Val Loss: 1.7707\n",
            "Epoch [21/100], Train Loss: 1.8553, Val Loss: 1.7592\n",
            "Epoch [22/100], Train Loss: 1.8371, Val Loss: 1.7254\n",
            "Epoch [23/100], Train Loss: 1.8078, Val Loss: 1.6925\n",
            "Epoch [24/100], Train Loss: 1.7948, Val Loss: 1.6723\n",
            "Epoch [25/100], Train Loss: 1.7667, Val Loss: 1.6281\n",
            "Epoch [26/100], Train Loss: 1.7543, Val Loss: 1.6280\n",
            "Epoch [27/100], Train Loss: 1.7341, Val Loss: 1.5960\n",
            "Epoch [28/100], Train Loss: 1.7222, Val Loss: 1.5684\n",
            "Epoch [29/100], Train Loss: 1.6999, Val Loss: 1.5547\n",
            "Epoch [30/100], Train Loss: 1.6842, Val Loss: 1.5388\n",
            "Epoch [31/100], Train Loss: 1.6763, Val Loss: 1.5216\n",
            "Epoch [32/100], Train Loss: 1.6648, Val Loss: 1.5106\n",
            "Epoch [33/100], Train Loss: 1.6468, Val Loss: 1.4921\n",
            "Epoch [34/100], Train Loss: 1.6309, Val Loss: 1.4835\n",
            "Epoch [35/100], Train Loss: 1.6192, Val Loss: 1.4635\n",
            "Epoch [36/100], Train Loss: 1.6114, Val Loss: 1.4428\n",
            "Epoch [37/100], Train Loss: 1.5904, Val Loss: 1.4305\n",
            "Epoch [38/100], Train Loss: 1.5855, Val Loss: 1.4108\n",
            "Epoch [39/100], Train Loss: 1.5714, Val Loss: 1.4146\n",
            "Epoch [40/100], Train Loss: 1.5543, Val Loss: 1.3820\n",
            "Epoch [41/100], Train Loss: 1.5374, Val Loss: 1.3740\n",
            "Epoch [42/100], Train Loss: 1.5253, Val Loss: 1.3519\n",
            "Epoch [43/100], Train Loss: 1.5068, Val Loss: 1.3362\n",
            "Epoch [44/100], Train Loss: 1.4898, Val Loss: 1.3234\n",
            "Epoch [45/100], Train Loss: 1.4721, Val Loss: 1.3126\n",
            "Epoch [46/100], Train Loss: 1.4732, Val Loss: 1.3150\n",
            "Epoch [47/100], Train Loss: 1.4674, Val Loss: 1.3107\n",
            "Epoch [48/100], Train Loss: 1.4520, Val Loss: 1.2790\n",
            "Epoch [49/100], Train Loss: 1.4375, Val Loss: 1.2651\n",
            "Epoch [50/100], Train Loss: 1.4351, Val Loss: 1.2790\n",
            "Epoch [51/100], Train Loss: 1.4300, Val Loss: 1.2383\n",
            "Epoch [52/100], Train Loss: 1.4115, Val Loss: 1.2613\n",
            "Epoch [53/100], Train Loss: 1.4114, Val Loss: 1.2139\n",
            "Epoch [54/100], Train Loss: 1.4001, Val Loss: 1.2183\n",
            "Epoch [55/100], Train Loss: 1.3824, Val Loss: 1.2229\n",
            "Epoch [56/100], Train Loss: 1.3762, Val Loss: 1.1926\n",
            "Epoch [57/100], Train Loss: 1.3774, Val Loss: 1.1908\n",
            "Epoch [58/100], Train Loss: 1.3580, Val Loss: 1.1740\n",
            "Epoch [59/100], Train Loss: 1.3632, Val Loss: 1.1779\n",
            "Epoch [60/100], Train Loss: 1.3551, Val Loss: 1.1717\n",
            "Epoch [61/100], Train Loss: 1.3414, Val Loss: 1.1537\n",
            "Epoch [62/100], Train Loss: 1.3448, Val Loss: 1.1436\n",
            "Epoch [63/100], Train Loss: 1.3391, Val Loss: 1.1450\n",
            "Epoch [64/100], Train Loss: 1.3254, Val Loss: 1.1299\n",
            "Epoch [65/100], Train Loss: 1.3192, Val Loss: 1.1128\n",
            "Epoch [66/100], Train Loss: 1.3246, Val Loss: 1.1423\n",
            "Epoch [67/100], Train Loss: 1.3100, Val Loss: 1.1027\n",
            "Epoch [68/100], Train Loss: 1.3047, Val Loss: 1.1162\n",
            "Epoch [69/100], Train Loss: 1.2964, Val Loss: 1.1157\n",
            "Epoch [70/100], Train Loss: 1.2958, Val Loss: 1.0981\n",
            "Epoch [71/100], Train Loss: 1.2917, Val Loss: 1.0788\n",
            "Epoch [72/100], Train Loss: 1.2894, Val Loss: 1.0720\n",
            "Epoch [73/100], Train Loss: 1.2848, Val Loss: 1.0905\n",
            "Epoch [74/100], Train Loss: 1.2740, Val Loss: 1.0682\n",
            "Epoch [75/100], Train Loss: 1.2758, Val Loss: 1.0686\n",
            "Epoch [76/100], Train Loss: 1.2573, Val Loss: 1.0493\n",
            "Epoch [77/100], Train Loss: 1.2524, Val Loss: 1.0481\n",
            "Epoch [78/100], Train Loss: 1.2474, Val Loss: 1.0387\n",
            "Epoch [79/100], Train Loss: 1.2506, Val Loss: 1.0503\n",
            "Epoch [80/100], Train Loss: 1.2384, Val Loss: 1.0334\n",
            "Epoch [81/100], Train Loss: 1.2547, Val Loss: 1.0318\n",
            "Epoch [82/100], Train Loss: 1.2299, Val Loss: 1.0215\n",
            "Epoch [83/100], Train Loss: 1.2282, Val Loss: 1.0056\n",
            "Epoch [84/100], Train Loss: 1.2196, Val Loss: 1.0059\n",
            "Epoch [85/100], Train Loss: 1.2185, Val Loss: 1.0259\n",
            "Epoch [86/100], Train Loss: 1.2234, Val Loss: 0.9985\n",
            "Epoch [87/100], Train Loss: 1.2115, Val Loss: 0.9711\n",
            "Epoch [88/100], Train Loss: 1.2121, Val Loss: 0.9915\n",
            "Epoch [89/100], Train Loss: 1.2129, Val Loss: 1.0055\n",
            "Epoch [90/100], Train Loss: 1.1929, Val Loss: 0.9787\n",
            "Epoch [91/100], Train Loss: 1.2013, Val Loss: 0.9731\n",
            "Epoch [92/100], Train Loss: 1.1976, Val Loss: 0.9865\n",
            "Epoch [93/100], Train Loss: 1.1881, Val Loss: 0.9725\n",
            "Epoch [94/100], Train Loss: 1.1752, Val Loss: 0.9543\n",
            "Epoch [95/100], Train Loss: 1.1857, Val Loss: 0.9518\n",
            "Epoch [96/100], Train Loss: 1.1823, Val Loss: 0.9571\n",
            "Epoch [97/100], Train Loss: 1.1829, Val Loss: 0.9542\n",
            "Epoch [98/100], Train Loss: 1.1585, Val Loss: 0.9460\n",
            "Epoch [99/100], Train Loss: 1.1537, Val Loss: 0.9518\n",
            "Epoch [100/100], Train Loss: 1.1495, Val Loss: 0.9264\n",
            "Accuracy of the model on the test set: 66.88%\n",
            "Fold 2/3\n",
            "torch.Size([14565, 6])\n",
            "torch.Size([14565])\n",
            "torch.Size([14565, 1, 6])\n",
            "torch.Size([14565])\n",
            "torch.Size([14565, 1, 6])\n",
            "torch.Size([14565])\n",
            "input_dim: 6, hidden_dim: 32, output_dim: 17\n",
            "LSTMModel(\n",
            "  (lstm): LSTM(6, 32, batch_first=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=32, out_features=17, bias=True)\n",
            ")\n",
            "Epoch [1/100], Train Loss: 2.8227, Val Loss: 2.7990\n",
            "Epoch [2/100], Train Loss: 2.7768, Val Loss: 2.7393\n",
            "Epoch [3/100], Train Loss: 2.6730, Val Loss: 2.5805\n",
            "Epoch [4/100], Train Loss: 2.5162, Val Loss: 2.4263\n",
            "Epoch [5/100], Train Loss: 2.3971, Val Loss: 2.3411\n",
            "Epoch [6/100], Train Loss: 2.3412, Val Loss: 2.2911\n",
            "Epoch [7/100], Train Loss: 2.3022, Val Loss: 2.2539\n",
            "Epoch [8/100], Train Loss: 2.2730, Val Loss: 2.2229\n",
            "Epoch [9/100], Train Loss: 2.2444, Val Loss: 2.1866\n",
            "Epoch [10/100], Train Loss: 2.2082, Val Loss: 2.1673\n",
            "Epoch [11/100], Train Loss: 2.1769, Val Loss: 2.1162\n",
            "Epoch [12/100], Train Loss: 2.1438, Val Loss: 2.0887\n",
            "Epoch [13/100], Train Loss: 2.1150, Val Loss: 2.0472\n",
            "Epoch [14/100], Train Loss: 2.0817, Val Loss: 2.0011\n",
            "Epoch [15/100], Train Loss: 2.0513, Val Loss: 1.9805\n",
            "Epoch [16/100], Train Loss: 2.0173, Val Loss: 1.9333\n",
            "Epoch [17/100], Train Loss: 1.9913, Val Loss: 1.9014\n",
            "Epoch [18/100], Train Loss: 1.9690, Val Loss: 1.8732\n",
            "Epoch [19/100], Train Loss: 1.9431, Val Loss: 1.8469\n",
            "Epoch [20/100], Train Loss: 1.9197, Val Loss: 1.8268\n",
            "Epoch [21/100], Train Loss: 1.8975, Val Loss: 1.7841\n",
            "Epoch [22/100], Train Loss: 1.8819, Val Loss: 1.7592\n",
            "Epoch [23/100], Train Loss: 1.8652, Val Loss: 1.7345\n",
            "Epoch [24/100], Train Loss: 1.8383, Val Loss: 1.7078\n",
            "Epoch [25/100], Train Loss: 1.8163, Val Loss: 1.6907\n",
            "Epoch [26/100], Train Loss: 1.7968, Val Loss: 1.6630\n",
            "Epoch [27/100], Train Loss: 1.7781, Val Loss: 1.6615\n",
            "Epoch [28/100], Train Loss: 1.7667, Val Loss: 1.6534\n",
            "Epoch [29/100], Train Loss: 1.7574, Val Loss: 1.6231\n",
            "Epoch [30/100], Train Loss: 1.7384, Val Loss: 1.5921\n",
            "Epoch [31/100], Train Loss: 1.7283, Val Loss: 1.6109\n",
            "Epoch [32/100], Train Loss: 1.7070, Val Loss: 1.5690\n",
            "Epoch [33/100], Train Loss: 1.6993, Val Loss: 1.5455\n",
            "Epoch [34/100], Train Loss: 1.6764, Val Loss: 1.5425\n",
            "Epoch [35/100], Train Loss: 1.6707, Val Loss: 1.5232\n",
            "Epoch [36/100], Train Loss: 1.6640, Val Loss: 1.5164\n",
            "Epoch [37/100], Train Loss: 1.6429, Val Loss: 1.4986\n",
            "Epoch [38/100], Train Loss: 1.6368, Val Loss: 1.4719\n",
            "Epoch [39/100], Train Loss: 1.6251, Val Loss: 1.4646\n",
            "Epoch [40/100], Train Loss: 1.6117, Val Loss: 1.4598\n",
            "Epoch [41/100], Train Loss: 1.6012, Val Loss: 1.4537\n",
            "Epoch [42/100], Train Loss: 1.5948, Val Loss: 1.4412\n",
            "Epoch [43/100], Train Loss: 1.5870, Val Loss: 1.4411\n",
            "Epoch [44/100], Train Loss: 1.5751, Val Loss: 1.4183\n",
            "Epoch [45/100], Train Loss: 1.5597, Val Loss: 1.4019\n",
            "Epoch [46/100], Train Loss: 1.5615, Val Loss: 1.3881\n",
            "Epoch [47/100], Train Loss: 1.5404, Val Loss: 1.3881\n",
            "Epoch [48/100], Train Loss: 1.5427, Val Loss: 1.3720\n",
            "Epoch [49/100], Train Loss: 1.5365, Val Loss: 1.3673\n",
            "Epoch [50/100], Train Loss: 1.5268, Val Loss: 1.3598\n",
            "Epoch [51/100], Train Loss: 1.5120, Val Loss: 1.3587\n",
            "Epoch [52/100], Train Loss: 1.4972, Val Loss: 1.3419\n",
            "Epoch [53/100], Train Loss: 1.4956, Val Loss: 1.3192\n",
            "Epoch [54/100], Train Loss: 1.4793, Val Loss: 1.3109\n",
            "Epoch [55/100], Train Loss: 1.4661, Val Loss: 1.3127\n",
            "Epoch [56/100], Train Loss: 1.4649, Val Loss: 1.3046\n",
            "Epoch [57/100], Train Loss: 1.4452, Val Loss: 1.2806\n",
            "Epoch [58/100], Train Loss: 1.4469, Val Loss: 1.2998\n",
            "Epoch [59/100], Train Loss: 1.4377, Val Loss: 1.2747\n",
            "Epoch [60/100], Train Loss: 1.4341, Val Loss: 1.2707\n",
            "Epoch [61/100], Train Loss: 1.4343, Val Loss: 1.2605\n",
            "Epoch [62/100], Train Loss: 1.4345, Val Loss: 1.2263\n",
            "Epoch [63/100], Train Loss: 1.4213, Val Loss: 1.2499\n",
            "Epoch [64/100], Train Loss: 1.4114, Val Loss: 1.2127\n",
            "Epoch [65/100], Train Loss: 1.3971, Val Loss: 1.2178\n",
            "Epoch [66/100], Train Loss: 1.3938, Val Loss: 1.2213\n",
            "Epoch [67/100], Train Loss: 1.3853, Val Loss: 1.2076\n",
            "Epoch [68/100], Train Loss: 1.3823, Val Loss: 1.1847\n",
            "Epoch [69/100], Train Loss: 1.3756, Val Loss: 1.1939\n",
            "Epoch [70/100], Train Loss: 1.3756, Val Loss: 1.1854\n",
            "Epoch [71/100], Train Loss: 1.3767, Val Loss: 1.1909\n",
            "Epoch [72/100], Train Loss: 1.3699, Val Loss: 1.1661\n",
            "Epoch [73/100], Train Loss: 1.3672, Val Loss: 1.1610\n",
            "Epoch [74/100], Train Loss: 1.3489, Val Loss: 1.1398\n",
            "Epoch [75/100], Train Loss: 1.3423, Val Loss: 1.1571\n",
            "Epoch [76/100], Train Loss: 1.3495, Val Loss: 1.1584\n",
            "Epoch [77/100], Train Loss: 1.3332, Val Loss: 1.1311\n",
            "Epoch [78/100], Train Loss: 1.3295, Val Loss: 1.1444\n",
            "Epoch [79/100], Train Loss: 1.3333, Val Loss: 1.1321\n",
            "Epoch [80/100], Train Loss: 1.3214, Val Loss: 1.1229\n",
            "Epoch [81/100], Train Loss: 1.3181, Val Loss: 1.1181\n",
            "Epoch [82/100], Train Loss: 1.3211, Val Loss: 1.1514\n",
            "Epoch [83/100], Train Loss: 1.3081, Val Loss: 1.0861\n",
            "Epoch [84/100], Train Loss: 1.3139, Val Loss: 1.0911\n",
            "Epoch [85/100], Train Loss: 1.3097, Val Loss: 1.1153\n",
            "Epoch [86/100], Train Loss: 1.2963, Val Loss: 1.0958\n",
            "Epoch [87/100], Train Loss: 1.2923, Val Loss: 1.0998\n",
            "Epoch [88/100], Train Loss: 1.2938, Val Loss: 1.1233\n",
            "Epoch [89/100], Train Loss: 1.2951, Val Loss: 1.0798\n",
            "Epoch [90/100], Train Loss: 1.2827, Val Loss: 1.0907\n",
            "Epoch [91/100], Train Loss: 1.2754, Val Loss: 1.0966\n",
            "Epoch [92/100], Train Loss: 1.2845, Val Loss: 1.1191\n",
            "Epoch [93/100], Train Loss: 1.2735, Val Loss: 1.0593\n",
            "Epoch [94/100], Train Loss: 1.2623, Val Loss: 1.0538\n",
            "Epoch [95/100], Train Loss: 1.2638, Val Loss: 1.0435\n",
            "Epoch [96/100], Train Loss: 1.2588, Val Loss: 1.0573\n",
            "Epoch [97/100], Train Loss: 1.2451, Val Loss: 1.0686\n",
            "Epoch [98/100], Train Loss: 1.2514, Val Loss: 1.0538\n",
            "Epoch [99/100], Train Loss: 1.2394, Val Loss: 1.0349\n",
            "Epoch [100/100], Train Loss: 1.2282, Val Loss: 1.0303\n",
            "Accuracy of the model on the test set: 66.62%\n",
            "Fold 3/3\n",
            "torch.Size([14566, 6])\n",
            "torch.Size([14566])\n",
            "torch.Size([14566, 1, 6])\n",
            "torch.Size([14566])\n",
            "torch.Size([14566, 1, 6])\n",
            "torch.Size([14566])\n",
            "input_dim: 6, hidden_dim: 32, output_dim: 17\n",
            "LSTMModel(\n",
            "  (lstm): LSTM(6, 32, batch_first=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=32, out_features=17, bias=True)\n",
            ")\n",
            "Epoch [1/100], Train Loss: 2.8245, Val Loss: 2.7815\n",
            "Epoch [2/100], Train Loss: 2.6768, Val Loss: 2.4933\n",
            "Epoch [3/100], Train Loss: 2.4035, Val Loss: 2.3271\n",
            "Epoch [4/100], Train Loss: 2.3099, Val Loss: 2.2624\n",
            "Epoch [5/100], Train Loss: 2.2530, Val Loss: 2.2124\n",
            "Epoch [6/100], Train Loss: 2.1991, Val Loss: 2.1251\n",
            "Epoch [7/100], Train Loss: 2.1208, Val Loss: 2.0554\n",
            "Epoch [8/100], Train Loss: 2.0444, Val Loss: 1.9661\n",
            "Epoch [9/100], Train Loss: 1.9858, Val Loss: 1.9161\n",
            "Epoch [10/100], Train Loss: 1.9227, Val Loss: 1.8492\n",
            "Epoch [11/100], Train Loss: 1.8768, Val Loss: 1.8463\n",
            "Epoch [12/100], Train Loss: 1.8308, Val Loss: 1.7536\n",
            "Epoch [13/100], Train Loss: 1.8018, Val Loss: 1.7123\n",
            "Epoch [14/100], Train Loss: 1.7740, Val Loss: 1.7031\n",
            "Epoch [15/100], Train Loss: 1.7406, Val Loss: 1.6470\n",
            "Epoch [16/100], Train Loss: 1.7185, Val Loss: 1.6177\n",
            "Epoch [17/100], Train Loss: 1.6950, Val Loss: 1.5947\n",
            "Epoch [18/100], Train Loss: 1.6698, Val Loss: 1.5599\n",
            "Epoch [19/100], Train Loss: 1.6431, Val Loss: 1.5342\n",
            "Epoch [20/100], Train Loss: 1.6319, Val Loss: 1.5339\n",
            "Epoch [21/100], Train Loss: 1.6262, Val Loss: 1.5769\n",
            "Epoch [22/100], Train Loss: 1.5965, Val Loss: 1.5051\n",
            "Epoch [23/100], Train Loss: 1.5819, Val Loss: 1.4601\n",
            "Epoch [24/100], Train Loss: 1.5624, Val Loss: 1.4929\n",
            "Epoch [25/100], Train Loss: 1.5479, Val Loss: 1.4261\n",
            "Epoch [26/100], Train Loss: 1.5304, Val Loss: 1.4314\n",
            "Epoch [27/100], Train Loss: 1.5149, Val Loss: 1.3792\n",
            "Epoch [28/100], Train Loss: 1.4990, Val Loss: 1.3582\n",
            "Epoch [29/100], Train Loss: 1.4954, Val Loss: 1.3255\n",
            "Epoch [30/100], Train Loss: 1.4654, Val Loss: 1.3446\n",
            "Epoch [31/100], Train Loss: 1.4636, Val Loss: 1.3039\n",
            "Epoch [32/100], Train Loss: 1.4381, Val Loss: 1.2953\n",
            "Epoch [33/100], Train Loss: 1.4266, Val Loss: 1.2711\n",
            "Epoch [34/100], Train Loss: 1.4144, Val Loss: 1.2742\n",
            "Epoch [35/100], Train Loss: 1.4115, Val Loss: 1.2290\n",
            "Epoch [36/100], Train Loss: 1.4007, Val Loss: 1.2179\n",
            "Epoch [37/100], Train Loss: 1.3781, Val Loss: 1.2084\n",
            "Epoch [38/100], Train Loss: 1.3744, Val Loss: 1.2075\n",
            "Epoch [39/100], Train Loss: 1.3759, Val Loss: 1.2549\n",
            "Epoch [40/100], Train Loss: 1.3526, Val Loss: 1.1872\n",
            "Epoch [41/100], Train Loss: 1.3516, Val Loss: 1.1592\n",
            "Epoch [42/100], Train Loss: 1.3465, Val Loss: 1.1657\n",
            "Epoch [43/100], Train Loss: 1.3502, Val Loss: 1.1327\n",
            "Epoch [44/100], Train Loss: 1.3248, Val Loss: 1.1707\n",
            "Epoch [45/100], Train Loss: 1.3520, Val Loss: 1.1773\n",
            "Epoch [46/100], Train Loss: 1.3326, Val Loss: 1.1954\n",
            "Epoch [47/100], Train Loss: 1.3246, Val Loss: 1.1534\n",
            "Epoch [48/100], Train Loss: 1.3159, Val Loss: 1.2479\n",
            "Epoch [49/100], Train Loss: 1.3197, Val Loss: 1.0790\n",
            "Epoch [50/100], Train Loss: 1.2948, Val Loss: 1.1514\n",
            "Epoch [51/100], Train Loss: 1.3119, Val Loss: 1.0812\n",
            "Epoch [52/100], Train Loss: 1.2821, Val Loss: 1.0648\n",
            "Epoch [53/100], Train Loss: 1.2817, Val Loss: 1.0647\n",
            "Epoch [54/100], Train Loss: 1.2828, Val Loss: 1.0529\n",
            "Epoch [55/100], Train Loss: 1.2616, Val Loss: 1.0866\n",
            "Epoch [56/100], Train Loss: 1.2760, Val Loss: 1.0327\n",
            "Epoch [57/100], Train Loss: 1.2807, Val Loss: 1.0674\n",
            "Epoch [58/100], Train Loss: 1.2447, Val Loss: 1.0185\n",
            "Epoch [59/100], Train Loss: 1.2770, Val Loss: 1.0583\n",
            "Epoch [60/100], Train Loss: 1.2595, Val Loss: 1.0325\n",
            "Epoch [61/100], Train Loss: 1.2591, Val Loss: 1.0444\n",
            "Epoch [62/100], Train Loss: 1.2369, Val Loss: 1.0012\n",
            "Epoch [63/100], Train Loss: 1.2298, Val Loss: 1.0101\n",
            "Epoch [64/100], Train Loss: 1.2368, Val Loss: 1.0440\n",
            "Epoch [65/100], Train Loss: 1.2313, Val Loss: 0.9943\n",
            "Epoch [66/100], Train Loss: 1.2409, Val Loss: 1.0187\n",
            "Epoch [67/100], Train Loss: 1.2182, Val Loss: 1.0019\n",
            "Epoch [68/100], Train Loss: 1.2044, Val Loss: 0.9718\n",
            "Epoch [69/100], Train Loss: 1.2080, Val Loss: 0.9796\n",
            "Epoch [70/100], Train Loss: 1.2070, Val Loss: 0.9924\n",
            "Epoch [71/100], Train Loss: 1.2023, Val Loss: 0.9646\n",
            "Epoch [72/100], Train Loss: 1.2071, Val Loss: 1.3713\n",
            "Epoch [73/100], Train Loss: 1.2208, Val Loss: 1.0051\n",
            "Epoch [74/100], Train Loss: 1.2007, Val Loss: 0.9442\n",
            "Epoch [75/100], Train Loss: 1.2022, Val Loss: 0.9625\n",
            "Epoch [76/100], Train Loss: 1.2198, Val Loss: 0.9544\n",
            "Epoch [77/100], Train Loss: 1.1870, Val Loss: 0.9486\n",
            "Epoch [78/100], Train Loss: 1.1984, Val Loss: 0.9643\n",
            "Epoch [79/100], Train Loss: 1.1737, Val Loss: 0.9328\n",
            "Epoch [80/100], Train Loss: 1.1679, Val Loss: 0.9338\n",
            "Epoch [81/100], Train Loss: 1.1666, Val Loss: 0.9682\n",
            "Epoch [82/100], Train Loss: 1.1625, Val Loss: 0.9284\n",
            "Epoch [83/100], Train Loss: 1.1572, Val Loss: 0.9484\n",
            "Epoch [84/100], Train Loss: 1.1723, Val Loss: 0.9072\n",
            "Epoch [85/100], Train Loss: 1.1759, Val Loss: 0.9088\n",
            "Epoch [86/100], Train Loss: 1.1562, Val Loss: 0.9081\n",
            "Epoch [87/100], Train Loss: 1.1394, Val Loss: 0.9389\n",
            "Epoch [88/100], Train Loss: 1.1488, Val Loss: 0.8932\n",
            "Epoch [89/100], Train Loss: 1.1522, Val Loss: 0.8749\n",
            "Epoch [90/100], Train Loss: 1.1358, Val Loss: 0.9302\n",
            "Epoch [91/100], Train Loss: 1.1498, Val Loss: 1.0307\n",
            "Epoch [92/100], Train Loss: 1.1473, Val Loss: 0.9332\n",
            "Epoch [93/100], Train Loss: 1.1410, Val Loss: 0.9148\n",
            "Epoch [94/100], Train Loss: 1.1392, Val Loss: 0.8528\n",
            "Epoch [95/100], Train Loss: 1.1171, Val Loss: 0.9116\n",
            "Epoch [96/100], Train Loss: 1.1350, Val Loss: 0.9007\n",
            "Epoch [97/100], Train Loss: 1.1240, Val Loss: 0.8799\n",
            "Epoch [98/100], Train Loss: 1.1314, Val Loss: 0.8547\n",
            "Epoch [99/100], Train Loss: 1.1151, Val Loss: 1.0079\n",
            "Epoch [100/100], Train Loss: 1.1544, Val Loss: 0.9513\n",
            "Accuracy of the model on the test set: 65.83%\n",
            "X_test shape: torch.Size([2428, 6])\n",
            "X_test shape: torch.Size([2428, 1, 6])\n",
            "test_loader shape: <torch.utils.data.dataloader.DataLoader object at 0x7d5173734340>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-c9df2445940e>:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_test = torch.tensor(X_test).float()\n",
            "<ipython-input-21-c9df2445940e>:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_test = torch.tensor(y_test).long()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test set: 64.58%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▅▅▆▇▇▇▇▁▂▂▂▂▄▄▄▅▅▆▇▇██▁▂▂▂▂▃▄▄▅▆▆▇▇▇█</td></tr><tr><td>train_loss</td><td>▅▄▄▃▃▂▂▂▂▂▂▂▂▁█▅▃▃▃▃▃▂▂▂▂▂▂▅▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇▆▄▃▂▂▂▂▂▂▂█▆▆▅▄▃▃▃▃▂▂▂▂▂██▄▄▃▂▂▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>train_loss</td><td>1.15444</td></tr><tr><td>val_loss</td><td>0.95134</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wandering-voice-2</strong> at: <a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-LSTM/runs/e7j80c91' target=\"_blank\">https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-LSTM/runs/e7j80c91</a><br/> View project at: <a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-LSTM' target=\"_blank\">https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-01-LSTM</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241104_124054-e7j80c91/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = model_pipeline(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn-KrUz59yds"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": true,
        "id": "AwFKsiy3LVKk"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), 'lstm_model.pth')\n",
        "\n",
        "# # Save the model as onnx\n",
        "# torch.onnx.export(model, X_train, 'lstm_model.onnx')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_test_data(data, data_percentage):\n",
        "    # Windowing the data\n",
        "    data = calculate_averages_and_dispersion(data, data_percentage)\n",
        "    print(data.shape)\n",
        "\n",
        "    # Assuming the last column is the target\n",
        "    X = data.iloc[:, :-1].values\n",
        "    y = data.iloc[:, -1].values\n",
        "\n",
        "    # Encode labeling of target data using presaved pkl file\n",
        "    # Load label encoder\n",
        "    label_encoder_path = '/content/drive/MyDrive/PhD/Colab Notebooks/label_encoder.pkl'\n",
        "    le = joblib.load(label_encoder_path)\n",
        "    y = le.transform(y)\n",
        "    print('y: ', y)\n",
        "\n",
        "    # # Standardize the features\n",
        "    # scaler = StandardScaler()\n",
        "    # X = scaler.fit_transform(X)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X = torch.tensor(X, dtype=torch.float32)\n",
        "    y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "0ggQaw0lISGG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXfBY8sm1Z43"
      },
      "source": [
        "## Load New Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ArtddUGY1Z43",
        "outputId": "931acbd2-b4a7-4a26-bf7f-52586c08b183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['REF_1.csv', '.ipynb_checkpoints']\n",
            "      Sample  Frequency (GHz)    LG (mV)    HG (mV)  Thickness (mm)\n",
            "0        REF              100  32.718837  -0.854611               0\n",
            "1        REF              100  19.289465   0.122118               0\n",
            "2        REF              100  29.178366   0.244164               0\n",
            "3        REF              100  42.729825   0.854540               0\n",
            "4        REF              100  54.083751   0.366259               0\n",
            "...      ...              ...        ...        ...             ...\n",
            "63784    REF              600  -0.244170  33.207178               0\n",
            "63785    REF              600  -0.854596  41.753143               0\n",
            "63786    REF              600   0.854596  21.242827               0\n",
            "63787    REF              600   0.000000  36.747649               0\n",
            "63788    REF              600   0.244170  27.225003               0\n",
            "\n",
            "[63789 rows x 5 columns]\n",
            "(63789, 5)\n",
            "     Frequency (GHz)  LG (mV) mean  HG (mV) mean  LG (mV) std deviation  \\\n",
            "0                100     51.378981     -0.063946              20.119522   \n",
            "1                100     43.726854      0.056686              18.066434   \n",
            "2                100     44.811088      0.074118              18.031144   \n",
            "3                100     46.787705      0.005805              20.747336   \n",
            "4                100     46.867642     -0.040696              21.492982   \n",
            "..               ...           ...           ...                    ...   \n",
            "658              600      0.112186     33.592131               0.810680   \n",
            "659              600      0.103387     32.597852               0.896565   \n",
            "660              600      0.276067     33.182981               0.890577   \n",
            "661              600     -0.067092     33.749412               1.053275   \n",
            "662              600     -0.313933     35.073338               0.821568   \n",
            "\n",
            "     HG (mV) std deviation  Thickness (mm) Sample  \n",
            "0                 0.738717               0    REF  \n",
            "1                 0.700541               0    REF  \n",
            "2                 0.717143               0    REF  \n",
            "3                 0.783414               0    REF  \n",
            "4                 0.716474               0    REF  \n",
            "..                     ...             ...    ...  \n",
            "658               8.833808               0    REF  \n",
            "659               9.363379               0    REF  \n",
            "660               9.369229               0    REF  \n",
            "661               9.338285               0    REF  \n",
            "662               8.520847               0    REF  \n",
            "\n",
            "[663 rows x 7 columns]\n",
            "(663, 7)\n",
            "y:  [16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16]\n"
          ]
        }
      ],
      "source": [
        "# Load new data\n",
        "input_data_test = '/content/drive/MyDrive/PhD/Colab Notebooks/test_data/'\n",
        "print(os.listdir(input_data_test))\n",
        "\n",
        "data_test = load_data_from_directory(input_data_test)\n",
        "\n",
        "# Load and preprocess data\n",
        "X_test, y_test = preprocess_test_data(data_test, data_percentage=8.33) # 1s window size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faC6EfmDMsRL"
      },
      "source": [
        "## Run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxNfBFvFowYp",
        "outputId": "d9d01106-2720-4a7b-9a25-e438099a9887",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "input_dim: 6, hidden_dim: 32, output_dim: 17\n",
            "Model outputs shape: tensor([[-2.8383, -6.4346, -8.5649,  ...,  8.5910,  7.9805,  8.1263],\n",
            "        [-2.9892, -6.2118, -8.7234,  ...,  8.5426,  8.1412,  7.2641],\n",
            "        [-2.9792, -6.1843, -8.7145,  ...,  8.6078,  8.1719,  7.2878],\n",
            "        ...,\n",
            "        [-2.5043, -6.1008, -9.3995,  ...,  7.8632,  7.9418,  7.5823],\n",
            "        [-2.4409, -6.2118, -9.8413,  ...,  7.9821,  7.9717,  7.7778],\n",
            "        [-2.5919, -5.9724, -9.0061,  ...,  7.7440,  7.9905,  7.0651]],\n",
            "       device='cuda:0')\n",
            "tensor([14, 14, 14, 14, 14, 16, 14, 14, 14, 14, 14, 14, 14, 14, 14, 16, 14, 14,\n",
            "        14, 14, 16, 16, 16, 16, 14, 16, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
            "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 16, 16,\n",
            "        16, 16, 16, 16, 14, 14, 14, 14, 16, 14, 14, 16, 14, 16, 16, 16, 16, 15,\n",
            "        16, 16, 16, 16, 16, 15, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 14, 14,\n",
            "        14, 15, 16, 15, 15, 16, 15, 16, 15, 15, 16, 16, 15, 15, 16, 16, 16, 16,\n",
            "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
            "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,  0, 16,\n",
            "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
            "        16, 16, 16, 16, 16, 16, 16, 14, 16, 16, 14, 14, 14, 15, 15, 15, 15, 15,\n",
            "        15, 14, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
            "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
            "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
            "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
            "        16, 14, 16, 14, 14, 14, 14, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
            "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
            "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
            "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
            "        16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 14, 14, 14, 14, 14, 14, 14, 14,\n",
            "        14, 14, 14, 14, 14, 15, 15, 15, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 14, 15, 14, 14, 14, 14, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
            "        15, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
            "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 16, 16, 16, 16, 16, 16, 16, 16,\n",
            "        16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
            "        14, 14, 14, 14, 15, 14, 14, 14, 14, 14, 14, 14, 15, 14, 14, 15, 15, 15,\n",
            "        15, 14, 15, 14, 15, 15, 15, 14, 15, 15, 15, 15, 14, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 14, 15, 14, 14, 15, 14, 14, 15, 14, 15, 15, 14, 15, 15,\n",
            "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 14, 14, 14,\n",
            "        14, 15, 14, 15, 14, 14, 14, 14, 16, 14, 15, 15, 15, 14, 14, 14, 14, 14,\n",
            "        14, 15, 14, 14, 14, 14, 14, 15, 14, 14, 14, 14, 14, 14, 15, 14, 14, 14,\n",
            "        14, 16, 15, 16, 16, 16, 16, 16, 14, 16, 16, 16, 14, 15, 15, 15, 15, 15,\n",
            "        15, 14, 15, 15, 15, 15,  0, 15, 14, 15, 15, 15, 15, 15, 15, 14, 15, 14,\n",
            "        14,  0, 14, 14, 15, 15, 15, 15, 15, 14, 15, 15, 15, 14, 15],\n",
            "       device='cuda:0')\n",
            "Predicted labels: ['M1' 'M1' 'M1' 'M1' 'M1' 'REF' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1'\n",
            " 'M1' 'REF' 'M1' 'M1' 'M1' 'M1' 'REF' 'REF' 'REF' 'REF' 'M1' 'REF' 'M1'\n",
            " 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1'\n",
            " 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'REF' 'REF' 'REF'\n",
            " 'REF' 'REF' 'REF' 'M1' 'M1' 'M1' 'M1' 'REF' 'M1' 'M1' 'REF' 'M1' 'REF'\n",
            " 'REF' 'REF' 'REF' 'N1' 'REF' 'REF' 'REF' 'REF' 'REF' 'N1' 'M1' 'M1' 'M1'\n",
            " 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'N1' 'M1' 'M1' 'M1' 'N1' 'REF' 'N1' 'N1'\n",
            " 'REF' 'N1' 'REF' 'N1' 'N1' 'REF' 'REF' 'N1' 'N1' 'REF' 'REF' 'REF' 'REF'\n",
            " 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF'\n",
            " 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF'\n",
            " 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'A1' 'REF'\n",
            " 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF'\n",
            " 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF'\n",
            " 'REF' 'M1' 'REF' 'REF' 'M1' 'M1' 'M1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'M1'\n",
            " 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF'\n",
            " 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF'\n",
            " 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF'\n",
            " 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF'\n",
            " 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF'\n",
            " 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'M1'\n",
            " 'REF' 'M1' 'M1' 'M1' 'M1' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF'\n",
            " 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF'\n",
            " 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF'\n",
            " 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF'\n",
            " 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF'\n",
            " 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF'\n",
            " 'REF' 'REF' 'N1' 'N1' 'N1' 'N1' 'N1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1'\n",
            " 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'N1' 'N1' 'N1' 'M1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'M1' 'N1' 'M1' 'M1' 'M1' 'M1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1'\n",
            " 'M1' 'M1' 'M1' 'N1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1'\n",
            " 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1'\n",
            " 'M1' 'M1' 'M1' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF'\n",
            " 'REF' 'REF' 'REF' 'REF' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1'\n",
            " 'M1' 'N1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'N1' 'M1' 'M1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'M1' 'N1' 'M1' 'N1' 'N1' 'N1' 'M1' 'N1' 'N1' 'N1' 'N1' 'M1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'M1' 'N1' 'M1' 'M1' 'N1'\n",
            " 'M1' 'M1' 'N1' 'M1' 'N1' 'N1' 'M1' 'N1' 'N1' 'M1' 'M1' 'M1' 'M1' 'M1'\n",
            " 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'N1' 'M1' 'M1' 'M1' 'M1'\n",
            " 'N1' 'M1' 'N1' 'M1' 'M1' 'M1' 'M1' 'REF' 'M1' 'N1' 'N1' 'N1' 'M1' 'M1'\n",
            " 'M1' 'M1' 'M1' 'M1' 'N1' 'M1' 'M1' 'M1' 'M1' 'M1' 'N1' 'M1' 'M1' 'M1'\n",
            " 'M1' 'M1' 'M1' 'N1' 'M1' 'M1' 'M1' 'M1' 'REF' 'N1' 'REF' 'REF' 'REF'\n",
            " 'REF' 'REF' 'M1' 'REF' 'REF' 'REF' 'M1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'M1' 'N1' 'N1' 'N1' 'N1' 'A1' 'N1' 'M1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'M1' 'N1' 'M1' 'M1' 'A1' 'M1' 'M1' 'N1' 'N1' 'N1' 'N1' 'N1' 'M1' 'N1'\n",
            " 'N1' 'N1' 'M1' 'N1']\n",
            "Classes in label encoder: ['A1' 'B1' 'C1' 'D1' 'E1' 'E2' 'E3' 'F1' 'G1' 'H1' 'I1' 'J1' 'K1' 'L1'\n",
            " 'M1' 'N1' 'REF']\n",
            "Number of classes: 17\n",
            "Number of correct predictions: 260/663\n",
            "Accuracy: 39.22%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          A1       0.00      0.00      0.00         0\n",
            "          M1       0.00      0.00      0.00         0\n",
            "          N1       0.00      0.00      0.00         0\n",
            "         REF       1.00      0.39      0.56       663\n",
            "\n",
            "    accuracy                           0.39       663\n",
            "   macro avg       0.25      0.10      0.14       663\n",
            "weighted avg       1.00      0.39      0.56       663\n",
            "\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  3   0   0   0   0   0   0   0   0   0   0   0   0   0 221 179 260]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-a586c42a3c29>:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "<ipython-input-41-a586c42a3c29>:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_test = torch.tensor(y_test).long().to(device)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "# Initialize the model\n",
        "input_dim = X_test.shape[-1]\n",
        "hidden_dim = config['hidden_dim']  # Replace with the hidden dimension used during training\n",
        "output_dim = config['classes']  # Replace with the number of output classes used during training\n",
        "model = LSTMModel(input_dim, hidden_dim, output_dim).to(device)\n",
        "\n",
        "# # Load label encoder\n",
        "label_encoder_path = '/content/drive/MyDrive/PhD/Colab Notebooks/label_encoder.pkl'\n",
        "le = joblib.load(label_encoder_path)\n",
        "\n",
        "# Load pretrained model\n",
        "model_path = '/content/drive/MyDrive/PhD/Colab Notebooks/lstm_model.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    X_test = X_test.unsqueeze(1).to(device)\n",
        "    y_test = torch.tensor(y_test).long().to(device)\n",
        "    outputs = model(X_test)\n",
        "    print(\"Model outputs shape:\", outputs.data)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "# Decode the predicted labels\n",
        "\n",
        "print(predicted)\n",
        "\n",
        "predicted_labels = le.inverse_transform(predicted.cpu().numpy())\n",
        "\n",
        "# Print the results\n",
        "print(\"Predicted labels:\", predicted_labels)\n",
        "print(\"Classes in label encoder:\", le.classes_)\n",
        "print(\"Number of classes:\", len(le.classes_))\n",
        "\n",
        "# Calculate percentage of correct predictions\n",
        "correct_predictions = (predicted == y_test).sum().item()\n",
        "total_samples = len(y_test)\n",
        "print(f\"Number of correct predictions: {correct_predictions}/{total_samples}\")\n",
        "accuracy = correct_predictions / total_samples * 100\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "# Calculate the classification report\n",
        "print(classification_report(le.inverse_transform(y_test.cpu().numpy()), predicted_labels))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(le.inverse_transform(y_test.cpu().numpy()), predicted_labels, labels=le.classes_)\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Original Label | Encoded Value |\n",
        "|----------------|---------------|\n",
        "| A1             | 0             |\n",
        "| B1             | 1             |\n",
        "| C1             | 2             |\n",
        "| D1             | 3             |\n",
        "| E1             | 4             |\n",
        "| E2             | 5             |\n",
        "| E3             | 6             |\n",
        "| F1             | 7             |\n",
        "| G1             | 8             |\n",
        "| H1             | 9             |\n",
        "| I1             | 10            |\n",
        "| J1             | 11            |\n",
        "| K1             | 12            |\n",
        "| L1             | 13            |\n",
        "| M1             | 14            |\n",
        "| N1             | 15            |\n",
        "| REF            | 16            |"
      ],
      "metadata": {
        "id": "ktsgpY21NcOV"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}