{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peUcVKtbGPfD"
      },
      "source": [
        "# Multi-Layer Perceptron Classification Model\n",
        "\n",
        "This notebook demonstrates how to load data, preprocess it, define an MLP model, train the model, and evaluate its performance. The data is assumed to be in CSV format and stored in a directory.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, we need to install the necessary libraries. Run the following cell to install them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAZ2n50rGS-C",
        "outputId": "4137938f-d942-479e-f9d7-8c3264501cb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install torch torchvision torchaudio\n",
        "%pip install pandas scikit-learn\n",
        "%pip install wandb onnx -Uq\n",
        "%pip install joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4d3KxhxGML8"
      },
      "source": [
        "## Import Libraries and seed\n",
        "Import the necessary libraries for data processing, model building, training, and evaluation. Adding a seed ensures reproducibility by making sure that the random number generation is consistent across different runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a9HvzrNG8iw",
        "outputId": "11c53dcd-34cc-4d3c-b3fe-d29bb29ccb54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import wandb\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUQtUPxSBzq4",
        "outputId": "0f76e76f-ddb8-4a21-cbb2-804c07137a5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoCyZSTt7qqS",
        "outputId": "dfdc6556-67cf-48b8-be1c-87f8fa79b502"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanimp94\u001b[0m (\u001b[33mdanimp94-university-carlos-iii-of-madrid\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "wandb.login()\n",
        "#94b4debef3cc9601df4d91995649548f8ab3a097"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1-6Xv6LHHmZ"
      },
      "source": [
        "## Load Data from Github Repository\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI7hrAaVHL44"
      },
      "outputs": [],
      "source": [
        "## Remove PIC-PAPER-01 folder:\n",
        "!rm -rf PIC-PAPER-01\n",
        "\n",
        "# # Download Github Repo (Private) https://stackoverflow.com/questions/74532852/clone-github-repo-with-fine-grained-token/78280453#78280453\n",
        "# !git clone --no-checkout https://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@github.com/danimp94/PIC-PAPER-01.git\n",
        "\n",
        "# # To clone data folder only:\n",
        "# %cd PIC-PAPER-01 # Navigate to the repository directory\n",
        "# !git sparse-checkout init --cone # Initialize sparse-checkout\n",
        "# !git sparse-checkout set data # Set the sparse-checkout to include only the data/ folder\n",
        "# !git checkout # Checkout the specified folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAY8BfVX9XCK"
      },
      "outputs": [],
      "source": [
        "def load_data_from_directory(input_path):\n",
        "    data_frames = []\n",
        "    for file in os.listdir(input_path):\n",
        "        if file.endswith('.csv'):\n",
        "            df = pd.read_csv(os.path.join(input_path, file), delimiter=';', header=0)\n",
        "            data_frames.append(df)\n",
        "    data = pd.concat(data_frames, ignore_index=True)\n",
        "\n",
        "    print(data)\n",
        "    print(data.shape)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiCTw-qcKXhn"
      },
      "source": [
        "## Preprocessing Data\n",
        "Define a function to preprocess the data. This includes encoding categorical labels and standardizing the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJoZqi5LDPl8"
      },
      "outputs": [],
      "source": [
        "def calculate_averages_and_dispersion(data, data_percentage):\n",
        "    df = data\n",
        "    results = []\n",
        "    for (sample, freq), group in df.groupby(['Sample', 'Frequency (GHz)']):\n",
        "        window_size = max(1, int(len(group) * data_percentage / 100))\n",
        "        # print(f\"Processing sample: {sample}, frequency: {freq} with window size: {window_size}\")\n",
        "        for start in range(0, len(group), window_size):\n",
        "            window_data = group.iloc[start:start + window_size]\n",
        "            mean_values = window_data[['LG (mV)', 'HG (mV)']].mean()\n",
        "            std_deviation_values = window_data[['LG (mV)', 'HG (mV)']].std()\n",
        "            results.append({\n",
        "                'Frequency (GHz)': freq,\n",
        "                'LG (mV) mean': mean_values['LG (mV)'],\n",
        "                'HG (mV) mean': mean_values['HG (mV)'],\n",
        "                'LG (mV) std deviation': std_deviation_values['LG (mV)'],\n",
        "                'HG (mV) std deviation': std_deviation_values['HG (mV)'],\n",
        "                'Thickness (mm)': window_data['Thickness (mm)'].iloc[0], # iloc[0]\n",
        "                'Sample': sample,\n",
        "            })\n",
        "    results_df = pd.DataFrame(results)\n",
        "    # results_df.to_csv(output_file, sep=';', index=False)\n",
        "    # print(f\"Processed {input_file} and saved to {output_file}\")\n",
        "    print(results_df)\n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZPdMwkkKmm7"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data, data_percentage):\n",
        "    # Windowing the data\n",
        "    data = calculate_averages_and_dispersion(data, data_percentage)\n",
        "    print(data.shape)\n",
        "\n",
        "    # Assuming the last column is the target\n",
        "    X = data.iloc[:, :-1].values\n",
        "    y = data.iloc[:, -1].values\n",
        "\n",
        "    # Encode the target variable if it's categorical\n",
        "    if y.dtype == 'object':\n",
        "        le = LabelEncoder()\n",
        "        y = le.fit_transform(y)\n",
        "\n",
        "    # le is the fitted LabelEncoder\n",
        "    joblib.dump(le, 'label_encoder.pkl')\n",
        "\n",
        "    # Get the original labels and their encoded values\n",
        "    original_labels = le.classes_\n",
        "    encoded_values = le.transform(original_labels)\n",
        "\n",
        "    # Create a DataFrame to display the mapping\n",
        "    label_mapping_df = pd.DataFrame({\n",
        "        'Original Label': original_labels,\n",
        "        'Encoded Value': encoded_values\n",
        "    })\n",
        "\n",
        "    # Display the DataFrame\n",
        "    print(label_mapping_df)\n",
        "\n",
        "    # Standardize the features\n",
        "    # print('prestandarization: ',X)\n",
        "    # scaler = StandardScaler()\n",
        "    # X = scaler.fit_transform(X)\n",
        "    # print('post-std: ', X)\n",
        "\n",
        "    # # Convert to PyTorch tensors\n",
        "    # X = torch.tensor(X, dtype=torch.float32)\n",
        "    # y = torch.tensor(y, dtype=torch.long)\n",
        "    # print(X.shape)\n",
        "    # print(y.shape)\n",
        "\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTXvxCzu6MBA",
        "outputId": "a780fa85-4467-4f42-d87a-b14b7b3a2429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Sample  Frequency (GHz)     LG (mV)    HG (mV)  Thickness (mm)\n",
            "0           A1            100.0   -7.080942  -0.854611             0.2\n",
            "1           A1            100.0   67.024785   0.244141             0.2\n",
            "2           A1            100.0  124.893178  -1.098776             0.2\n",
            "3           A1            100.0   91.075571   0.000000             0.2\n",
            "4           A1            100.0   48.956174   0.122094             0.2\n",
            "...        ...              ...         ...        ...             ...\n",
            "2737958    REF            600.0    0.366256  16.237333             0.0\n",
            "2737959    REF            600.0    0.000000  -7.080942             0.0\n",
            "2737960    REF            600.0   -0.244170  15.260652             0.0\n",
            "2737961    REF            600.0    0.366256  20.021975             0.0\n",
            "2737962    REF            600.0    0.122085  13.185203             0.0\n",
            "\n",
            "[2737963 rows x 5 columns]\n",
            "(2737963, 5)\n",
            "       Frequency (GHz)  LG (mV) mean  HG (mV) mean  LG (mV) std deviation  \\\n",
            "0                100.0     54.879155     -0.022198              29.958659   \n",
            "1                100.0     54.511665      0.048093              28.096155   \n",
            "2                100.0     55.099894     -0.118380              26.833871   \n",
            "3                100.0     48.387674      0.103588              28.843498   \n",
            "4                100.0     49.932853     -0.071525              23.093397   \n",
            "...                ...           ...           ...                    ...   \n",
            "24271            600.0     -0.006143     10.237498               0.890999   \n",
            "24272            600.0     -0.006910     10.949278               0.858148   \n",
            "24273            600.0      0.029178     10.547702               0.842082   \n",
            "24274            600.0      0.065266     10.051683               0.891632   \n",
            "24275            600.0     -0.228910      9.766817               0.856929   \n",
            "\n",
            "       HG (mV) std deviation  Thickness (mm) Sample  \n",
            "0                   0.644211             0.2     A1  \n",
            "1                   0.704742             0.2     A1  \n",
            "2                   0.755493             0.2     A1  \n",
            "3                   0.854655             0.2     A1  \n",
            "4                   0.650231             0.2     A1  \n",
            "...                      ...             ...    ...  \n",
            "24271               8.205224             0.0    REF  \n",
            "24272               8.679982             0.0    REF  \n",
            "24273               8.802972             0.0    REF  \n",
            "24274               8.365464             0.0    REF  \n",
            "24275               9.725526             0.0    REF  \n",
            "\n",
            "[24276 rows x 7 columns]\n",
            "(24276, 7)\n",
            "   Original Label  Encoded Value\n",
            "0              A1              0\n",
            "1              B1              1\n",
            "2              C1              2\n",
            "3              D1              3\n",
            "4              E1              4\n",
            "5              E2              5\n",
            "6              E3              6\n",
            "7              F1              7\n",
            "8              G1              8\n",
            "9              H1              9\n",
            "10             I1             10\n",
            "11             J1             11\n",
            "12             K1             12\n",
            "13             L1             13\n",
            "14             M1             14\n",
            "15             N1             15\n",
            "16            REF             16\n"
          ]
        }
      ],
      "source": [
        "input_path = '/content/drive/MyDrive/PhD/Colab Notebooks/training_data/'\n",
        "data = load_data_from_directory(input_path)\n",
        "\n",
        "# Load and preprocess data\n",
        "X, y = preprocess_data(data, data_percentage=3.7) # 1s window size\n",
        "\n",
        "\n",
        "# print(le.classes_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZjha9Cx6MBB"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLRhwd8cKKJH",
        "outputId": "6214f77f-6ff4-4fae-cb1f-a78fa3d3a89c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epochs': 500, 'seed': 40, 'classes': 17, 'learning_rate': 0.001, 'dataset': 'experiment_1', 'architecture': 'MLP', 'hidden_dim': 64, 'batch_size': 32}\n"
          ]
        }
      ],
      "source": [
        "config = dict(\n",
        "    epochs = 500,\n",
        "    seed = 40,\n",
        "    classes = 17,\n",
        "    learning_rate = 0.001,\n",
        "    dataset = \"experiment_1\",\n",
        "    architecture = \"MLP\",\n",
        "    hidden_dim = 64,\n",
        "    batch_size = 32\n",
        ")\n",
        "\n",
        "print(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN Model"
      ],
      "metadata": {
        "id": "73lbgWbceGk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Multiclass(nn.Module):\n",
        "    ''' Multiclass classification\n",
        "        input_size: number of features\n",
        "        hidden_size: number of neurons in the hidden layer\n",
        "        num_classes: number of classes to classify\n",
        "    '''\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(Multiclass, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)  # First fully connected layer\n",
        "        self.relu = nn.ReLU() # Activation layer (ReLU)\n",
        "        self.fc_out = nn.Linear(hidden_size, num_classes) # Last fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc_out(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "nGtMP0a_d1zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEtZPi2fLQGL"
      },
      "source": [
        "## Run Training\n",
        "\n",
        "Do not use it if just want to run inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(X_train, y_train, X_test, y_test, config):\n",
        "    # Set random seed\n",
        "    set_seed(config['seed'])\n",
        "\n",
        "    # Initialize the model\n",
        "    input_dim = X.shape[-1]\n",
        "    hidden_dim = config['hidden_dim']\n",
        "    output_dim = config['classes']\n",
        "    print(input_dim, hidden_dim, output_dim)\n",
        "    model = Multiclass(input_dim, hidden_dim, output_dim).to(device)\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "    # Create TensorDataset and DataLoader for training data\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)  # Shuffle for better training\n",
        "\n",
        "    # Create TensorDataset and DataLoader for test data\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(config['epochs']):\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, target)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluate on test data\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                outputs = model(data)\n",
        "                test_loss += criterion(outputs, target).item()  # Sum up batch loss\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct += (predicted == target).sum().item()\n",
        "\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "        print(f'Epoch: {epoch+1}, Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "1OR7_UmMhT7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split trainig data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=True)\n",
        "\n",
        "model = train_model(X_train, y_train, X_test, y_test, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC8fnd6ni4sI",
        "outputId": "ec32f907-d844-4baa-f72e-05e33f7f527e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6 64 17\n",
            "y_train:  [ 8  8  4 ... 10 10  8]\n",
            "y_train:  tensor([ 8,  8,  4,  ..., 10, 10,  8])\n",
            "y_shape:  torch.Size([19420])\n",
            "X_train:  tensor([[ 1.6000e+02,  3.0245e+01,  8.0642e-02,  3.2782e+00,  7.2364e-01,\n",
            "          1.0000e-01],\n",
            "        [ 3.1000e+02,  1.1193e+00,  5.0588e+02,  8.0973e-01,  4.0931e+01,\n",
            "          1.0000e-01],\n",
            "        [ 1.4000e+02,  6.9175e+01, -9.6320e-02,  1.2824e+01,  8.2268e-01,\n",
            "          1.0000e-01],\n",
            "        ...,\n",
            "        [ 1.7000e+02,  1.9603e+01,  1.0464e-01,  1.5905e+00,  7.3450e-01,\n",
            "          3.6000e-01],\n",
            "        [ 1.9000e+02,  1.0697e+01,  2.1643e-01,  2.3917e+00,  7.5252e-01,\n",
            "          3.6000e-01],\n",
            "        [ 1.5000e+02,  6.5257e+01,  1.5205e-01,  7.2693e+00,  7.7181e-01,\n",
            "          1.0000e-01]])\n",
            "X_shape:  torch.Size([19420, 6])\n",
            "Epoch: 1, Test Loss: 0.0969, Accuracy: 12.31%\n",
            "Epoch: 2, Test Loss: 0.0934, Accuracy: 12.21%\n",
            "Epoch: 3, Test Loss: 0.0901, Accuracy: 13.10%\n",
            "Epoch: 4, Test Loss: 0.0802, Accuracy: 16.64%\n",
            "Epoch: 5, Test Loss: 0.0869, Accuracy: 15.47%\n",
            "Epoch: 6, Test Loss: 0.0894, Accuracy: 19.05%\n",
            "Epoch: 7, Test Loss: 0.0780, Accuracy: 22.12%\n",
            "Epoch: 8, Test Loss: 0.0752, Accuracy: 21.64%\n",
            "Epoch: 9, Test Loss: 0.0737, Accuracy: 21.27%\n",
            "Epoch: 10, Test Loss: 0.0708, Accuracy: 24.94%\n",
            "Epoch: 11, Test Loss: 0.0681, Accuracy: 25.25%\n",
            "Epoch: 12, Test Loss: 0.0693, Accuracy: 25.95%\n",
            "Epoch: 13, Test Loss: 0.0661, Accuracy: 30.05%\n",
            "Epoch: 14, Test Loss: 0.0666, Accuracy: 32.08%\n",
            "Epoch: 15, Test Loss: 0.0619, Accuracy: 30.79%\n",
            "Epoch: 16, Test Loss: 0.0665, Accuracy: 28.77%\n",
            "Epoch: 17, Test Loss: 0.0616, Accuracy: 33.81%\n",
            "Epoch: 18, Test Loss: 0.0645, Accuracy: 32.64%\n",
            "Epoch: 19, Test Loss: 0.0577, Accuracy: 37.05%\n",
            "Epoch: 20, Test Loss: 0.0584, Accuracy: 36.33%\n",
            "Epoch: 21, Test Loss: 0.0546, Accuracy: 40.79%\n",
            "Epoch: 22, Test Loss: 0.0547, Accuracy: 38.49%\n",
            "Epoch: 23, Test Loss: 0.0529, Accuracy: 42.96%\n",
            "Epoch: 24, Test Loss: 0.0530, Accuracy: 37.36%\n",
            "Epoch: 25, Test Loss: 0.0476, Accuracy: 47.41%\n",
            "Epoch: 26, Test Loss: 0.0481, Accuracy: 42.34%\n",
            "Epoch: 27, Test Loss: 0.0466, Accuracy: 46.27%\n",
            "Epoch: 28, Test Loss: 0.0433, Accuracy: 52.37%\n",
            "Epoch: 29, Test Loss: 0.0423, Accuracy: 54.61%\n",
            "Epoch: 30, Test Loss: 0.0413, Accuracy: 53.05%\n",
            "Epoch: 31, Test Loss: 0.0406, Accuracy: 54.47%\n",
            "Epoch: 32, Test Loss: 0.0396, Accuracy: 52.16%\n",
            "Epoch: 33, Test Loss: 0.0379, Accuracy: 56.88%\n",
            "Epoch: 34, Test Loss: 0.0367, Accuracy: 56.98%\n",
            "Epoch: 35, Test Loss: 0.0362, Accuracy: 58.03%\n",
            "Epoch: 36, Test Loss: 0.0350, Accuracy: 60.71%\n",
            "Epoch: 37, Test Loss: 0.0347, Accuracy: 61.59%\n",
            "Epoch: 38, Test Loss: 0.0326, Accuracy: 62.25%\n",
            "Epoch: 39, Test Loss: 0.0349, Accuracy: 58.67%\n",
            "Epoch: 40, Test Loss: 0.0331, Accuracy: 64.35%\n",
            "Epoch: 41, Test Loss: 0.0307, Accuracy: 66.27%\n",
            "Epoch: 42, Test Loss: 0.0330, Accuracy: 60.69%\n",
            "Epoch: 43, Test Loss: 0.0305, Accuracy: 66.43%\n",
            "Epoch: 44, Test Loss: 0.0299, Accuracy: 65.36%\n",
            "Epoch: 45, Test Loss: 0.0291, Accuracy: 66.95%\n",
            "Epoch: 46, Test Loss: 0.0298, Accuracy: 62.11%\n",
            "Epoch: 47, Test Loss: 0.0277, Accuracy: 69.65%\n",
            "Epoch: 48, Test Loss: 0.0276, Accuracy: 68.93%\n",
            "Epoch: 49, Test Loss: 0.0267, Accuracy: 68.55%\n",
            "Epoch: 50, Test Loss: 0.0287, Accuracy: 62.97%\n",
            "Epoch: 51, Test Loss: 0.0272, Accuracy: 67.61%\n",
            "Epoch: 52, Test Loss: 0.0279, Accuracy: 66.35%\n",
            "Epoch: 53, Test Loss: 0.0274, Accuracy: 67.15%\n",
            "Epoch: 54, Test Loss: 0.0271, Accuracy: 65.57%\n",
            "Epoch: 55, Test Loss: 0.0261, Accuracy: 69.17%\n",
            "Epoch: 56, Test Loss: 0.0269, Accuracy: 67.15%\n",
            "Epoch: 57, Test Loss: 0.0250, Accuracy: 69.42%\n",
            "Epoch: 58, Test Loss: 0.0243, Accuracy: 70.10%\n",
            "Epoch: 59, Test Loss: 0.0252, Accuracy: 68.68%\n",
            "Epoch: 60, Test Loss: 0.0240, Accuracy: 72.82%\n",
            "Epoch: 61, Test Loss: 0.0248, Accuracy: 70.57%\n",
            "Epoch: 62, Test Loss: 0.0230, Accuracy: 73.62%\n",
            "Epoch: 63, Test Loss: 0.0237, Accuracy: 68.49%\n",
            "Epoch: 64, Test Loss: 0.0236, Accuracy: 68.70%\n",
            "Epoch: 65, Test Loss: 0.0222, Accuracy: 73.50%\n",
            "Epoch: 66, Test Loss: 0.0233, Accuracy: 69.58%\n",
            "Epoch: 67, Test Loss: 0.0219, Accuracy: 74.01%\n",
            "Epoch: 68, Test Loss: 0.0233, Accuracy: 70.76%\n",
            "Epoch: 69, Test Loss: 0.0220, Accuracy: 72.43%\n",
            "Epoch: 70, Test Loss: 0.0238, Accuracy: 70.04%\n",
            "Epoch: 71, Test Loss: 0.0230, Accuracy: 70.98%\n",
            "Epoch: 72, Test Loss: 0.0211, Accuracy: 73.93%\n",
            "Epoch: 73, Test Loss: 0.0208, Accuracy: 74.73%\n",
            "Epoch: 74, Test Loss: 0.0208, Accuracy: 74.40%\n",
            "Epoch: 75, Test Loss: 0.0203, Accuracy: 74.59%\n",
            "Epoch: 76, Test Loss: 0.0219, Accuracy: 73.19%\n",
            "Epoch: 77, Test Loss: 0.0218, Accuracy: 72.41%\n",
            "Epoch: 78, Test Loss: 0.0209, Accuracy: 73.17%\n",
            "Epoch: 79, Test Loss: 0.0200, Accuracy: 74.34%\n",
            "Epoch: 80, Test Loss: 0.0214, Accuracy: 72.01%\n",
            "Epoch: 81, Test Loss: 0.0192, Accuracy: 76.83%\n",
            "Epoch: 82, Test Loss: 0.0184, Accuracy: 77.00%\n",
            "Epoch: 83, Test Loss: 0.0189, Accuracy: 76.65%\n",
            "Epoch: 84, Test Loss: 0.0200, Accuracy: 73.74%\n",
            "Epoch: 85, Test Loss: 0.0198, Accuracy: 73.27%\n",
            "Epoch: 86, Test Loss: 0.0185, Accuracy: 77.20%\n",
            "Epoch: 87, Test Loss: 0.0195, Accuracy: 73.74%\n",
            "Epoch: 88, Test Loss: 0.0190, Accuracy: 75.47%\n",
            "Epoch: 89, Test Loss: 0.0219, Accuracy: 70.92%\n",
            "Epoch: 90, Test Loss: 0.0184, Accuracy: 76.21%\n",
            "Epoch: 91, Test Loss: 0.0186, Accuracy: 75.66%\n",
            "Epoch: 92, Test Loss: 0.0205, Accuracy: 72.08%\n",
            "Epoch: 93, Test Loss: 0.0183, Accuracy: 75.78%\n",
            "Epoch: 94, Test Loss: 0.0174, Accuracy: 76.98%\n",
            "Epoch: 95, Test Loss: 0.0173, Accuracy: 76.65%\n",
            "Epoch: 96, Test Loss: 0.0180, Accuracy: 75.99%\n",
            "Epoch: 97, Test Loss: 0.0176, Accuracy: 76.01%\n",
            "Epoch: 98, Test Loss: 0.0186, Accuracy: 75.39%\n",
            "Epoch: 99, Test Loss: 0.0172, Accuracy: 77.24%\n",
            "Epoch: 100, Test Loss: 0.0187, Accuracy: 74.61%\n",
            "Epoch: 101, Test Loss: 0.0172, Accuracy: 77.29%\n",
            "Epoch: 102, Test Loss: 0.0170, Accuracy: 77.43%\n",
            "Epoch: 103, Test Loss: 0.0171, Accuracy: 76.44%\n",
            "Epoch: 104, Test Loss: 0.0175, Accuracy: 76.96%\n",
            "Epoch: 105, Test Loss: 0.0178, Accuracy: 76.73%\n",
            "Epoch: 106, Test Loss: 0.0175, Accuracy: 76.19%\n",
            "Epoch: 107, Test Loss: 0.0173, Accuracy: 76.46%\n",
            "Epoch: 108, Test Loss: 0.0171, Accuracy: 76.75%\n",
            "Epoch: 109, Test Loss: 0.0175, Accuracy: 75.00%\n",
            "Epoch: 110, Test Loss: 0.0161, Accuracy: 77.88%\n",
            "Epoch: 111, Test Loss: 0.0176, Accuracy: 75.12%\n",
            "Epoch: 112, Test Loss: 0.0165, Accuracy: 78.15%\n",
            "Epoch: 113, Test Loss: 0.0167, Accuracy: 75.56%\n",
            "Epoch: 114, Test Loss: 0.0164, Accuracy: 77.16%\n",
            "Epoch: 115, Test Loss: 0.0174, Accuracy: 74.36%\n",
            "Epoch: 116, Test Loss: 0.0170, Accuracy: 75.39%\n",
            "Epoch: 117, Test Loss: 0.0160, Accuracy: 77.64%\n",
            "Epoch: 118, Test Loss: 0.0171, Accuracy: 76.63%\n",
            "Epoch: 119, Test Loss: 0.0167, Accuracy: 75.68%\n",
            "Epoch: 120, Test Loss: 0.0166, Accuracy: 76.28%\n",
            "Epoch: 121, Test Loss: 0.0173, Accuracy: 76.57%\n",
            "Epoch: 122, Test Loss: 0.0164, Accuracy: 76.57%\n",
            "Epoch: 123, Test Loss: 0.0163, Accuracy: 78.07%\n",
            "Epoch: 124, Test Loss: 0.0165, Accuracy: 77.29%\n",
            "Epoch: 125, Test Loss: 0.0151, Accuracy: 78.38%\n",
            "Epoch: 126, Test Loss: 0.0160, Accuracy: 77.74%\n",
            "Epoch: 127, Test Loss: 0.0154, Accuracy: 78.01%\n",
            "Epoch: 128, Test Loss: 0.0160, Accuracy: 77.55%\n",
            "Epoch: 129, Test Loss: 0.0159, Accuracy: 76.83%\n",
            "Epoch: 130, Test Loss: 0.0156, Accuracy: 76.98%\n",
            "Epoch: 131, Test Loss: 0.0157, Accuracy: 78.05%\n",
            "Epoch: 132, Test Loss: 0.0156, Accuracy: 78.09%\n",
            "Epoch: 133, Test Loss: 0.0158, Accuracy: 78.03%\n",
            "Epoch: 134, Test Loss: 0.0145, Accuracy: 78.73%\n",
            "Epoch: 135, Test Loss: 0.0158, Accuracy: 78.62%\n",
            "Epoch: 136, Test Loss: 0.0151, Accuracy: 78.73%\n",
            "Epoch: 137, Test Loss: 0.0150, Accuracy: 79.39%\n",
            "Epoch: 138, Test Loss: 0.0150, Accuracy: 78.05%\n",
            "Epoch: 139, Test Loss: 0.0156, Accuracy: 77.43%\n",
            "Epoch: 140, Test Loss: 0.0151, Accuracy: 77.94%\n",
            "Epoch: 141, Test Loss: 0.0151, Accuracy: 78.07%\n",
            "Epoch: 142, Test Loss: 0.0162, Accuracy: 77.06%\n",
            "Epoch: 143, Test Loss: 0.0152, Accuracy: 78.67%\n",
            "Epoch: 144, Test Loss: 0.0153, Accuracy: 77.12%\n",
            "Epoch: 145, Test Loss: 0.0142, Accuracy: 79.37%\n",
            "Epoch: 146, Test Loss: 0.0142, Accuracy: 80.02%\n",
            "Epoch: 147, Test Loss: 0.0154, Accuracy: 77.55%\n",
            "Epoch: 148, Test Loss: 0.0151, Accuracy: 77.64%\n",
            "Epoch: 149, Test Loss: 0.0161, Accuracy: 76.52%\n",
            "Epoch: 150, Test Loss: 0.0142, Accuracy: 79.88%\n",
            "Epoch: 151, Test Loss: 0.0155, Accuracy: 78.01%\n",
            "Epoch: 152, Test Loss: 0.0144, Accuracy: 78.40%\n",
            "Epoch: 153, Test Loss: 0.0157, Accuracy: 76.01%\n",
            "Epoch: 154, Test Loss: 0.0141, Accuracy: 78.69%\n",
            "Epoch: 155, Test Loss: 0.0143, Accuracy: 79.28%\n",
            "Epoch: 156, Test Loss: 0.0143, Accuracy: 78.52%\n",
            "Epoch: 157, Test Loss: 0.0175, Accuracy: 75.49%\n",
            "Epoch: 158, Test Loss: 0.0146, Accuracy: 78.38%\n",
            "Epoch: 159, Test Loss: 0.0190, Accuracy: 72.06%\n",
            "Epoch: 160, Test Loss: 0.0143, Accuracy: 78.23%\n",
            "Epoch: 161, Test Loss: 0.0149, Accuracy: 77.45%\n",
            "Epoch: 162, Test Loss: 0.0148, Accuracy: 77.88%\n",
            "Epoch: 163, Test Loss: 0.0158, Accuracy: 79.06%\n",
            "Epoch: 164, Test Loss: 0.0143, Accuracy: 79.86%\n",
            "Epoch: 165, Test Loss: 0.0153, Accuracy: 76.98%\n",
            "Epoch: 166, Test Loss: 0.0146, Accuracy: 78.11%\n",
            "Epoch: 167, Test Loss: 0.0150, Accuracy: 78.73%\n",
            "Epoch: 168, Test Loss: 0.0170, Accuracy: 73.93%\n",
            "Epoch: 169, Test Loss: 0.0160, Accuracy: 79.74%\n",
            "Epoch: 170, Test Loss: 0.0141, Accuracy: 79.00%\n",
            "Epoch: 171, Test Loss: 0.0143, Accuracy: 78.73%\n",
            "Epoch: 172, Test Loss: 0.0141, Accuracy: 78.03%\n",
            "Epoch: 173, Test Loss: 0.0162, Accuracy: 77.31%\n",
            "Epoch: 174, Test Loss: 0.0166, Accuracy: 77.08%\n",
            "Epoch: 175, Test Loss: 0.0141, Accuracy: 78.77%\n",
            "Epoch: 176, Test Loss: 0.0143, Accuracy: 77.90%\n",
            "Epoch: 177, Test Loss: 0.0137, Accuracy: 79.92%\n",
            "Epoch: 178, Test Loss: 0.0149, Accuracy: 77.80%\n",
            "Epoch: 179, Test Loss: 0.0142, Accuracy: 80.31%\n",
            "Epoch: 180, Test Loss: 0.0135, Accuracy: 79.39%\n",
            "Epoch: 181, Test Loss: 0.0139, Accuracy: 79.41%\n",
            "Epoch: 182, Test Loss: 0.0148, Accuracy: 78.89%\n",
            "Epoch: 183, Test Loss: 0.0136, Accuracy: 79.00%\n",
            "Epoch: 184, Test Loss: 0.0145, Accuracy: 78.03%\n",
            "Epoch: 185, Test Loss: 0.0135, Accuracy: 80.23%\n",
            "Epoch: 186, Test Loss: 0.0137, Accuracy: 80.07%\n",
            "Epoch: 187, Test Loss: 0.0138, Accuracy: 79.10%\n",
            "Epoch: 188, Test Loss: 0.0166, Accuracy: 75.12%\n",
            "Epoch: 189, Test Loss: 0.0143, Accuracy: 79.76%\n",
            "Epoch: 190, Test Loss: 0.0138, Accuracy: 79.86%\n",
            "Epoch: 191, Test Loss: 0.0163, Accuracy: 77.97%\n",
            "Epoch: 192, Test Loss: 0.0140, Accuracy: 79.43%\n",
            "Epoch: 193, Test Loss: 0.0137, Accuracy: 78.69%\n",
            "Epoch: 194, Test Loss: 0.0142, Accuracy: 78.07%\n",
            "Epoch: 195, Test Loss: 0.0137, Accuracy: 79.30%\n",
            "Epoch: 196, Test Loss: 0.0174, Accuracy: 75.21%\n",
            "Epoch: 197, Test Loss: 0.0132, Accuracy: 80.13%\n",
            "Epoch: 198, Test Loss: 0.0137, Accuracy: 79.04%\n",
            "Epoch: 199, Test Loss: 0.0150, Accuracy: 77.84%\n",
            "Epoch: 200, Test Loss: 0.0137, Accuracy: 79.16%\n",
            "Epoch: 201, Test Loss: 0.0136, Accuracy: 79.80%\n",
            "Epoch: 202, Test Loss: 0.0127, Accuracy: 80.37%\n",
            "Epoch: 203, Test Loss: 0.0128, Accuracy: 80.15%\n",
            "Epoch: 204, Test Loss: 0.0151, Accuracy: 77.76%\n",
            "Epoch: 205, Test Loss: 0.0128, Accuracy: 80.87%\n",
            "Epoch: 206, Test Loss: 0.0134, Accuracy: 79.65%\n",
            "Epoch: 207, Test Loss: 0.0142, Accuracy: 78.95%\n",
            "Epoch: 208, Test Loss: 0.0139, Accuracy: 79.96%\n",
            "Epoch: 209, Test Loss: 0.0139, Accuracy: 80.29%\n",
            "Epoch: 210, Test Loss: 0.0132, Accuracy: 80.15%\n",
            "Epoch: 211, Test Loss: 0.0152, Accuracy: 78.07%\n",
            "Epoch: 212, Test Loss: 0.0187, Accuracy: 74.79%\n",
            "Epoch: 213, Test Loss: 0.0155, Accuracy: 78.11%\n",
            "Epoch: 214, Test Loss: 0.0135, Accuracy: 80.27%\n",
            "Epoch: 215, Test Loss: 0.0157, Accuracy: 78.44%\n",
            "Epoch: 216, Test Loss: 0.0137, Accuracy: 79.65%\n",
            "Epoch: 217, Test Loss: 0.0135, Accuracy: 80.23%\n",
            "Epoch: 218, Test Loss: 0.0148, Accuracy: 78.17%\n",
            "Epoch: 219, Test Loss: 0.0128, Accuracy: 80.33%\n",
            "Epoch: 220, Test Loss: 0.0128, Accuracy: 80.83%\n",
            "Epoch: 221, Test Loss: 0.0128, Accuracy: 81.51%\n",
            "Epoch: 222, Test Loss: 0.0140, Accuracy: 79.20%\n",
            "Epoch: 223, Test Loss: 0.0137, Accuracy: 79.49%\n",
            "Epoch: 224, Test Loss: 0.0127, Accuracy: 80.17%\n",
            "Epoch: 225, Test Loss: 0.0159, Accuracy: 76.05%\n",
            "Epoch: 226, Test Loss: 0.0136, Accuracy: 80.19%\n",
            "Epoch: 227, Test Loss: 0.0133, Accuracy: 80.48%\n",
            "Epoch: 228, Test Loss: 0.0135, Accuracy: 79.88%\n",
            "Epoch: 229, Test Loss: 0.0130, Accuracy: 81.43%\n",
            "Epoch: 230, Test Loss: 0.0136, Accuracy: 80.37%\n",
            "Epoch: 231, Test Loss: 0.0143, Accuracy: 78.50%\n",
            "Epoch: 232, Test Loss: 0.0133, Accuracy: 81.88%\n",
            "Epoch: 233, Test Loss: 0.0126, Accuracy: 80.85%\n",
            "Epoch: 234, Test Loss: 0.0132, Accuracy: 79.26%\n",
            "Epoch: 235, Test Loss: 0.0133, Accuracy: 80.07%\n",
            "Epoch: 236, Test Loss: 0.0123, Accuracy: 81.63%\n",
            "Epoch: 237, Test Loss: 0.0134, Accuracy: 80.21%\n",
            "Epoch: 238, Test Loss: 0.0145, Accuracy: 78.21%\n",
            "Epoch: 239, Test Loss: 0.0145, Accuracy: 78.75%\n",
            "Epoch: 240, Test Loss: 0.0147, Accuracy: 77.72%\n",
            "Epoch: 241, Test Loss: 0.0141, Accuracy: 79.76%\n",
            "Epoch: 242, Test Loss: 0.0130, Accuracy: 80.31%\n",
            "Epoch: 243, Test Loss: 0.0141, Accuracy: 79.63%\n",
            "Epoch: 244, Test Loss: 0.0135, Accuracy: 80.33%\n",
            "Epoch: 245, Test Loss: 0.0154, Accuracy: 77.22%\n",
            "Epoch: 246, Test Loss: 0.0158, Accuracy: 77.90%\n",
            "Epoch: 247, Test Loss: 0.0137, Accuracy: 79.35%\n",
            "Epoch: 248, Test Loss: 0.0139, Accuracy: 79.74%\n",
            "Epoch: 249, Test Loss: 0.0137, Accuracy: 78.97%\n",
            "Epoch: 250, Test Loss: 0.0124, Accuracy: 81.65%\n",
            "Epoch: 251, Test Loss: 0.0133, Accuracy: 80.64%\n",
            "Epoch: 252, Test Loss: 0.0132, Accuracy: 79.80%\n",
            "Epoch: 253, Test Loss: 0.0128, Accuracy: 81.10%\n",
            "Epoch: 254, Test Loss: 0.0127, Accuracy: 81.69%\n",
            "Epoch: 255, Test Loss: 0.0142, Accuracy: 80.09%\n",
            "Epoch: 256, Test Loss: 0.0119, Accuracy: 82.52%\n",
            "Epoch: 257, Test Loss: 0.0135, Accuracy: 79.51%\n",
            "Epoch: 258, Test Loss: 0.0138, Accuracy: 79.41%\n",
            "Epoch: 259, Test Loss: 0.0139, Accuracy: 78.67%\n",
            "Epoch: 260, Test Loss: 0.0125, Accuracy: 81.86%\n",
            "Epoch: 261, Test Loss: 0.0141, Accuracy: 78.46%\n",
            "Epoch: 262, Test Loss: 0.0128, Accuracy: 80.85%\n",
            "Epoch: 263, Test Loss: 0.0165, Accuracy: 75.06%\n",
            "Epoch: 264, Test Loss: 0.0126, Accuracy: 80.85%\n",
            "Epoch: 265, Test Loss: 0.0125, Accuracy: 81.26%\n",
            "Epoch: 266, Test Loss: 0.0158, Accuracy: 77.14%\n",
            "Epoch: 267, Test Loss: 0.0135, Accuracy: 80.29%\n",
            "Epoch: 268, Test Loss: 0.0140, Accuracy: 79.59%\n",
            "Epoch: 269, Test Loss: 0.0120, Accuracy: 82.58%\n",
            "Epoch: 270, Test Loss: 0.0135, Accuracy: 79.96%\n",
            "Epoch: 271, Test Loss: 0.0122, Accuracy: 82.72%\n",
            "Epoch: 272, Test Loss: 0.0136, Accuracy: 80.35%\n",
            "Epoch: 273, Test Loss: 0.0141, Accuracy: 79.59%\n",
            "Epoch: 274, Test Loss: 0.0142, Accuracy: 78.87%\n",
            "Epoch: 275, Test Loss: 0.0137, Accuracy: 79.39%\n",
            "Epoch: 276, Test Loss: 0.0126, Accuracy: 81.03%\n",
            "Epoch: 277, Test Loss: 0.0122, Accuracy: 80.95%\n",
            "Epoch: 278, Test Loss: 0.0132, Accuracy: 79.35%\n",
            "Epoch: 279, Test Loss: 0.0130, Accuracy: 80.33%\n",
            "Epoch: 280, Test Loss: 0.0123, Accuracy: 81.24%\n",
            "Epoch: 281, Test Loss: 0.0132, Accuracy: 79.84%\n",
            "Epoch: 282, Test Loss: 0.0121, Accuracy: 82.02%\n",
            "Epoch: 283, Test Loss: 0.0128, Accuracy: 81.01%\n",
            "Epoch: 284, Test Loss: 0.0134, Accuracy: 80.02%\n",
            "Epoch: 285, Test Loss: 0.0137, Accuracy: 80.25%\n",
            "Epoch: 286, Test Loss: 0.0131, Accuracy: 80.00%\n",
            "Epoch: 287, Test Loss: 0.0133, Accuracy: 79.76%\n",
            "Epoch: 288, Test Loss: 0.0149, Accuracy: 78.56%\n",
            "Epoch: 289, Test Loss: 0.0123, Accuracy: 81.69%\n",
            "Epoch: 290, Test Loss: 0.0118, Accuracy: 82.35%\n",
            "Epoch: 291, Test Loss: 0.0130, Accuracy: 80.89%\n",
            "Epoch: 292, Test Loss: 0.0128, Accuracy: 80.66%\n",
            "Epoch: 293, Test Loss: 0.0129, Accuracy: 79.18%\n",
            "Epoch: 294, Test Loss: 0.0137, Accuracy: 79.90%\n",
            "Epoch: 295, Test Loss: 0.0134, Accuracy: 79.78%\n",
            "Epoch: 296, Test Loss: 0.0130, Accuracy: 81.12%\n",
            "Epoch: 297, Test Loss: 0.0125, Accuracy: 80.95%\n",
            "Epoch: 298, Test Loss: 0.0126, Accuracy: 81.01%\n",
            "Epoch: 299, Test Loss: 0.0133, Accuracy: 79.76%\n",
            "Epoch: 300, Test Loss: 0.0140, Accuracy: 79.47%\n",
            "Epoch: 301, Test Loss: 0.0129, Accuracy: 82.25%\n",
            "Epoch: 302, Test Loss: 0.0128, Accuracy: 81.12%\n",
            "Epoch: 303, Test Loss: 0.0128, Accuracy: 80.79%\n",
            "Epoch: 304, Test Loss: 0.0123, Accuracy: 82.00%\n",
            "Epoch: 305, Test Loss: 0.0134, Accuracy: 81.36%\n",
            "Epoch: 306, Test Loss: 0.0127, Accuracy: 81.22%\n",
            "Epoch: 307, Test Loss: 0.0147, Accuracy: 80.21%\n",
            "Epoch: 308, Test Loss: 0.0121, Accuracy: 81.69%\n",
            "Epoch: 309, Test Loss: 0.0120, Accuracy: 82.00%\n",
            "Epoch: 310, Test Loss: 0.0127, Accuracy: 80.62%\n",
            "Epoch: 311, Test Loss: 0.0145, Accuracy: 79.10%\n",
            "Epoch: 312, Test Loss: 0.0132, Accuracy: 80.66%\n",
            "Epoch: 313, Test Loss: 0.0141, Accuracy: 78.91%\n",
            "Epoch: 314, Test Loss: 0.0138, Accuracy: 80.27%\n",
            "Epoch: 315, Test Loss: 0.0127, Accuracy: 80.64%\n",
            "Epoch: 316, Test Loss: 0.0128, Accuracy: 81.24%\n",
            "Epoch: 317, Test Loss: 0.0134, Accuracy: 80.00%\n",
            "Epoch: 318, Test Loss: 0.0124, Accuracy: 81.57%\n",
            "Epoch: 319, Test Loss: 0.0128, Accuracy: 80.81%\n",
            "Epoch: 320, Test Loss: 0.0131, Accuracy: 80.44%\n",
            "Epoch: 321, Test Loss: 0.0118, Accuracy: 82.10%\n",
            "Epoch: 322, Test Loss: 0.0128, Accuracy: 81.67%\n",
            "Epoch: 323, Test Loss: 0.0129, Accuracy: 80.97%\n",
            "Epoch: 324, Test Loss: 0.0128, Accuracy: 81.07%\n",
            "Epoch: 325, Test Loss: 0.0130, Accuracy: 81.57%\n",
            "Epoch: 326, Test Loss: 0.0125, Accuracy: 80.79%\n",
            "Epoch: 327, Test Loss: 0.0132, Accuracy: 80.37%\n",
            "Epoch: 328, Test Loss: 0.0126, Accuracy: 80.62%\n",
            "Epoch: 329, Test Loss: 0.0136, Accuracy: 80.27%\n",
            "Epoch: 330, Test Loss: 0.0124, Accuracy: 81.65%\n",
            "Epoch: 331, Test Loss: 0.0131, Accuracy: 81.22%\n",
            "Epoch: 332, Test Loss: 0.0124, Accuracy: 82.10%\n",
            "Epoch: 333, Test Loss: 0.0125, Accuracy: 80.93%\n",
            "Epoch: 334, Test Loss: 0.0133, Accuracy: 81.75%\n",
            "Epoch: 335, Test Loss: 0.0150, Accuracy: 78.25%\n",
            "Epoch: 336, Test Loss: 0.0135, Accuracy: 80.99%\n",
            "Epoch: 337, Test Loss: 0.0138, Accuracy: 79.26%\n",
            "Epoch: 338, Test Loss: 0.0128, Accuracy: 80.35%\n",
            "Epoch: 339, Test Loss: 0.0121, Accuracy: 81.88%\n",
            "Epoch: 340, Test Loss: 0.0131, Accuracy: 80.00%\n",
            "Epoch: 341, Test Loss: 0.0128, Accuracy: 81.45%\n",
            "Epoch: 342, Test Loss: 0.0131, Accuracy: 80.21%\n",
            "Epoch: 343, Test Loss: 0.0131, Accuracy: 79.88%\n",
            "Epoch: 344, Test Loss: 0.0125, Accuracy: 81.30%\n",
            "Epoch: 345, Test Loss: 0.0129, Accuracy: 81.36%\n",
            "Epoch: 346, Test Loss: 0.0128, Accuracy: 80.95%\n",
            "Epoch: 347, Test Loss: 0.0128, Accuracy: 82.00%\n",
            "Epoch: 348, Test Loss: 0.0144, Accuracy: 78.75%\n",
            "Epoch: 349, Test Loss: 0.0119, Accuracy: 82.52%\n",
            "Epoch: 350, Test Loss: 0.0135, Accuracy: 79.80%\n",
            "Epoch: 351, Test Loss: 0.0124, Accuracy: 82.10%\n",
            "Epoch: 352, Test Loss: 0.0137, Accuracy: 80.87%\n",
            "Epoch: 353, Test Loss: 0.0117, Accuracy: 82.74%\n",
            "Epoch: 354, Test Loss: 0.0131, Accuracy: 80.68%\n",
            "Epoch: 355, Test Loss: 0.0155, Accuracy: 76.81%\n",
            "Epoch: 356, Test Loss: 0.0132, Accuracy: 79.78%\n",
            "Epoch: 357, Test Loss: 0.0125, Accuracy: 81.67%\n",
            "Epoch: 358, Test Loss: 0.0119, Accuracy: 81.34%\n",
            "Epoch: 359, Test Loss: 0.0128, Accuracy: 80.46%\n",
            "Epoch: 360, Test Loss: 0.0133, Accuracy: 81.36%\n",
            "Epoch: 361, Test Loss: 0.0119, Accuracy: 82.10%\n",
            "Epoch: 362, Test Loss: 0.0133, Accuracy: 81.55%\n",
            "Epoch: 363, Test Loss: 0.0120, Accuracy: 82.13%\n",
            "Epoch: 364, Test Loss: 0.0133, Accuracy: 80.35%\n",
            "Epoch: 365, Test Loss: 0.0117, Accuracy: 82.33%\n",
            "Epoch: 366, Test Loss: 0.0120, Accuracy: 81.20%\n",
            "Epoch: 367, Test Loss: 0.0142, Accuracy: 79.84%\n",
            "Epoch: 368, Test Loss: 0.0123, Accuracy: 81.73%\n",
            "Epoch: 369, Test Loss: 0.0122, Accuracy: 81.75%\n",
            "Epoch: 370, Test Loss: 0.0127, Accuracy: 81.57%\n",
            "Epoch: 371, Test Loss: 0.0120, Accuracy: 81.71%\n",
            "Epoch: 372, Test Loss: 0.0118, Accuracy: 82.76%\n",
            "Epoch: 373, Test Loss: 0.0125, Accuracy: 81.45%\n",
            "Epoch: 374, Test Loss: 0.0124, Accuracy: 82.10%\n",
            "Epoch: 375, Test Loss: 0.0123, Accuracy: 81.71%\n",
            "Epoch: 376, Test Loss: 0.0133, Accuracy: 80.85%\n",
            "Epoch: 377, Test Loss: 0.0132, Accuracy: 80.44%\n",
            "Epoch: 378, Test Loss: 0.0174, Accuracy: 75.82%\n",
            "Epoch: 379, Test Loss: 0.0123, Accuracy: 82.08%\n",
            "Epoch: 380, Test Loss: 0.0130, Accuracy: 80.54%\n",
            "Epoch: 381, Test Loss: 0.0133, Accuracy: 81.24%\n",
            "Epoch: 382, Test Loss: 0.0124, Accuracy: 82.33%\n",
            "Epoch: 383, Test Loss: 0.0122, Accuracy: 82.10%\n",
            "Epoch: 384, Test Loss: 0.0124, Accuracy: 81.78%\n",
            "Epoch: 385, Test Loss: 0.0116, Accuracy: 82.43%\n",
            "Epoch: 386, Test Loss: 0.0124, Accuracy: 81.14%\n",
            "Epoch: 387, Test Loss: 0.0139, Accuracy: 80.83%\n",
            "Epoch: 388, Test Loss: 0.0267, Accuracy: 71.15%\n",
            "Epoch: 389, Test Loss: 0.0138, Accuracy: 80.81%\n",
            "Epoch: 390, Test Loss: 0.0127, Accuracy: 81.86%\n",
            "Epoch: 391, Test Loss: 0.0120, Accuracy: 81.53%\n",
            "Epoch: 392, Test Loss: 0.0128, Accuracy: 80.54%\n",
            "Epoch: 393, Test Loss: 0.0131, Accuracy: 81.38%\n",
            "Epoch: 394, Test Loss: 0.0143, Accuracy: 79.16%\n",
            "Epoch: 395, Test Loss: 0.0159, Accuracy: 78.67%\n",
            "Epoch: 396, Test Loss: 0.0132, Accuracy: 80.31%\n",
            "Epoch: 397, Test Loss: 0.0122, Accuracy: 82.23%\n",
            "Epoch: 398, Test Loss: 0.0178, Accuracy: 77.20%\n",
            "Epoch: 399, Test Loss: 0.0134, Accuracy: 80.25%\n",
            "Epoch: 400, Test Loss: 0.0121, Accuracy: 82.04%\n",
            "Epoch: 401, Test Loss: 0.0154, Accuracy: 78.73%\n",
            "Epoch: 402, Test Loss: 0.0151, Accuracy: 80.19%\n",
            "Epoch: 403, Test Loss: 0.0121, Accuracy: 82.00%\n",
            "Epoch: 404, Test Loss: 0.0117, Accuracy: 82.70%\n",
            "Epoch: 405, Test Loss: 0.0119, Accuracy: 81.61%\n",
            "Epoch: 406, Test Loss: 0.0123, Accuracy: 81.69%\n",
            "Epoch: 407, Test Loss: 0.0124, Accuracy: 81.34%\n",
            "Epoch: 408, Test Loss: 0.0130, Accuracy: 81.63%\n",
            "Epoch: 409, Test Loss: 0.0133, Accuracy: 79.98%\n",
            "Epoch: 410, Test Loss: 0.0143, Accuracy: 82.10%\n",
            "Epoch: 411, Test Loss: 0.0137, Accuracy: 79.90%\n",
            "Epoch: 412, Test Loss: 0.0121, Accuracy: 82.02%\n",
            "Epoch: 413, Test Loss: 0.0131, Accuracy: 81.65%\n",
            "Epoch: 414, Test Loss: 0.0124, Accuracy: 82.35%\n",
            "Epoch: 415, Test Loss: 0.0120, Accuracy: 82.74%\n",
            "Epoch: 416, Test Loss: 0.0129, Accuracy: 80.75%\n",
            "Epoch: 417, Test Loss: 0.0121, Accuracy: 82.06%\n",
            "Epoch: 418, Test Loss: 0.0131, Accuracy: 80.66%\n",
            "Epoch: 419, Test Loss: 0.0115, Accuracy: 83.48%\n",
            "Epoch: 420, Test Loss: 0.0127, Accuracy: 81.16%\n",
            "Epoch: 421, Test Loss: 0.0124, Accuracy: 82.62%\n",
            "Epoch: 422, Test Loss: 0.0121, Accuracy: 82.10%\n",
            "Epoch: 423, Test Loss: 0.0130, Accuracy: 81.24%\n",
            "Epoch: 424, Test Loss: 0.0125, Accuracy: 82.29%\n",
            "Epoch: 425, Test Loss: 0.0139, Accuracy: 81.05%\n",
            "Epoch: 426, Test Loss: 0.0123, Accuracy: 81.24%\n",
            "Epoch: 427, Test Loss: 0.0123, Accuracy: 83.13%\n",
            "Epoch: 428, Test Loss: 0.0126, Accuracy: 82.29%\n",
            "Epoch: 429, Test Loss: 0.0120, Accuracy: 82.45%\n",
            "Epoch: 430, Test Loss: 0.0131, Accuracy: 81.26%\n",
            "Epoch: 431, Test Loss: 0.0136, Accuracy: 81.45%\n",
            "Epoch: 432, Test Loss: 0.0125, Accuracy: 82.08%\n",
            "Epoch: 433, Test Loss: 0.0132, Accuracy: 80.15%\n",
            "Epoch: 434, Test Loss: 0.0121, Accuracy: 81.90%\n",
            "Epoch: 435, Test Loss: 0.0129, Accuracy: 82.39%\n",
            "Epoch: 436, Test Loss: 0.0118, Accuracy: 82.60%\n",
            "Epoch: 437, Test Loss: 0.0119, Accuracy: 82.83%\n",
            "Epoch: 438, Test Loss: 0.0133, Accuracy: 81.61%\n",
            "Epoch: 439, Test Loss: 0.0123, Accuracy: 81.51%\n",
            "Epoch: 440, Test Loss: 0.0116, Accuracy: 83.18%\n",
            "Epoch: 441, Test Loss: 0.0114, Accuracy: 83.05%\n",
            "Epoch: 442, Test Loss: 0.0122, Accuracy: 81.82%\n",
            "Epoch: 443, Test Loss: 0.0122, Accuracy: 81.84%\n",
            "Epoch: 444, Test Loss: 0.0121, Accuracy: 82.21%\n",
            "Epoch: 445, Test Loss: 0.0117, Accuracy: 82.95%\n",
            "Epoch: 446, Test Loss: 0.0124, Accuracy: 82.06%\n",
            "Epoch: 447, Test Loss: 0.0131, Accuracy: 81.57%\n",
            "Epoch: 448, Test Loss: 0.0119, Accuracy: 82.93%\n",
            "Epoch: 449, Test Loss: 0.0123, Accuracy: 82.80%\n",
            "Epoch: 450, Test Loss: 0.0125, Accuracy: 82.50%\n",
            "Epoch: 451, Test Loss: 0.0130, Accuracy: 80.40%\n",
            "Epoch: 452, Test Loss: 0.0126, Accuracy: 81.92%\n",
            "Epoch: 453, Test Loss: 0.0131, Accuracy: 81.18%\n",
            "Epoch: 454, Test Loss: 0.0126, Accuracy: 82.48%\n",
            "Epoch: 455, Test Loss: 0.0122, Accuracy: 82.48%\n",
            "Epoch: 456, Test Loss: 0.0129, Accuracy: 81.84%\n",
            "Epoch: 457, Test Loss: 0.0123, Accuracy: 82.54%\n",
            "Epoch: 458, Test Loss: 0.0123, Accuracy: 82.97%\n",
            "Epoch: 459, Test Loss: 0.0129, Accuracy: 81.71%\n",
            "Epoch: 460, Test Loss: 0.0126, Accuracy: 81.71%\n",
            "Epoch: 461, Test Loss: 0.0141, Accuracy: 79.90%\n",
            "Epoch: 462, Test Loss: 0.0144, Accuracy: 79.76%\n",
            "Epoch: 463, Test Loss: 0.0133, Accuracy: 81.05%\n",
            "Epoch: 464, Test Loss: 0.0121, Accuracy: 82.80%\n",
            "Epoch: 465, Test Loss: 0.0275, Accuracy: 72.28%\n",
            "Epoch: 466, Test Loss: 0.0126, Accuracy: 81.57%\n",
            "Epoch: 467, Test Loss: 0.0123, Accuracy: 82.43%\n",
            "Epoch: 468, Test Loss: 0.0125, Accuracy: 82.31%\n",
            "Epoch: 469, Test Loss: 0.0130, Accuracy: 81.96%\n",
            "Epoch: 470, Test Loss: 0.0122, Accuracy: 82.54%\n",
            "Epoch: 471, Test Loss: 0.0126, Accuracy: 82.80%\n",
            "Epoch: 472, Test Loss: 0.0128, Accuracy: 80.97%\n",
            "Epoch: 473, Test Loss: 0.0129, Accuracy: 81.55%\n",
            "Epoch: 474, Test Loss: 0.0124, Accuracy: 82.80%\n",
            "Epoch: 475, Test Loss: 0.0121, Accuracy: 81.86%\n",
            "Epoch: 476, Test Loss: 0.0121, Accuracy: 81.67%\n",
            "Epoch: 477, Test Loss: 0.0161, Accuracy: 79.45%\n",
            "Epoch: 478, Test Loss: 0.0118, Accuracy: 82.93%\n",
            "Epoch: 479, Test Loss: 0.0133, Accuracy: 81.59%\n",
            "Epoch: 480, Test Loss: 0.0129, Accuracy: 81.82%\n",
            "Epoch: 481, Test Loss: 0.0140, Accuracy: 80.68%\n",
            "Epoch: 482, Test Loss: 0.0148, Accuracy: 81.69%\n",
            "Epoch: 483, Test Loss: 0.0130, Accuracy: 82.56%\n",
            "Epoch: 484, Test Loss: 0.0131, Accuracy: 80.75%\n",
            "Epoch: 485, Test Loss: 0.0120, Accuracy: 83.30%\n",
            "Epoch: 486, Test Loss: 0.0127, Accuracy: 82.91%\n",
            "Epoch: 487, Test Loss: 0.0125, Accuracy: 81.78%\n",
            "Epoch: 488, Test Loss: 0.0181, Accuracy: 76.26%\n",
            "Epoch: 489, Test Loss: 0.0163, Accuracy: 77.45%\n",
            "Epoch: 490, Test Loss: 0.0121, Accuracy: 82.25%\n",
            "Epoch: 491, Test Loss: 0.0129, Accuracy: 82.19%\n",
            "Epoch: 492, Test Loss: 0.0130, Accuracy: 81.45%\n",
            "Epoch: 493, Test Loss: 0.0129, Accuracy: 81.69%\n",
            "Epoch: 494, Test Loss: 0.0122, Accuracy: 81.96%\n",
            "Epoch: 495, Test Loss: 0.0116, Accuracy: 83.30%\n",
            "Epoch: 496, Test Loss: 0.0124, Accuracy: 82.68%\n",
            "Epoch: 497, Test Loss: 0.0124, Accuracy: 82.23%\n",
            "Epoch: 498, Test Loss: 0.0140, Accuracy: 79.59%\n",
            "Epoch: 499, Test Loss: 0.0133, Accuracy: 81.20%\n",
            "Epoch: 500, Test Loss: 0.0123, Accuracy: 82.85%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "acwT3zcgjWPj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R3-PgjKIjVfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn-KrUz59yds"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AwFKsiy3LVKk"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), 'lstm_model.pth')\n",
        "\n",
        "# # Save the model as onnx\n",
        "# torch.onnx.export(model, X_train, 'lstm_model.onnx')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_test_data(data, data_percentage):\n",
        "    # Windowing the data\n",
        "    data = calculate_averages_and_dispersion(data, data_percentage)\n",
        "\n",
        "    # Assuming the last column is the target\n",
        "    X = data.iloc[:, :-1].values\n",
        "    y = data.iloc[:, -1].values\n",
        "\n",
        "    # Encode labeling of target data using presaved pkl file\n",
        "    # Load label encoder\n",
        "    label_encoder_path = '/content/drive/MyDrive/PhD/Colab Notebooks/label_encoder.pkl'\n",
        "    le = joblib.load(label_encoder_path)\n",
        "    y = le.transform(y)\n",
        "    print('y: ', y)    # Encode the target variable if it's categorical\n",
        "\n",
        "    # # Standardize the features\n",
        "    # scaler = StandardScaler()\n",
        "    # X = scaler.fit_transform(X)\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "0ggQaw0lISGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXfBY8sm1Z43"
      },
      "source": [
        "## Load New Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ArtddUGY1Z43",
        "outputId": "b16a06bc-a1c2-49bc-f518-3db82d76cf09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['H1_1 - Copy.csv', '.ipynb_checkpoints']\n",
            "      Sample  Frequency (GHz)    LG (mV)    HG (mV)  Thickness (mm)\n",
            "0         H1              100  69.100232   0.244141            0.07\n",
            "1         H1              100  53.229153   0.366211            0.07\n",
            "2         H1              100  62.019289   1.587129            0.07\n",
            "3         H1              100  67.268954  -0.244141            0.07\n",
            "4         H1              100  75.326578   1.220798            0.07\n",
            "...      ...              ...        ...        ...             ...\n",
            "63947     H1              600   0.244170  24.417043            0.07\n",
            "63948     H1              600  -0.732511  12.086436            0.07\n",
            "63949     H1              600   0.122085  29.300451            0.07\n",
            "63950     H1              600  -0.244170   1.220852            0.07\n",
            "63951     H1              600  -0.610426  33.573434            0.07\n",
            "\n",
            "[63952 rows x 5 columns]\n",
            "(63952, 5)\n",
            "     Frequency (GHz)  LG (mV) mean  HG (mV) mean  LG (mV) std deviation  \\\n",
            "0                100     66.101648     -0.055687              24.633872   \n",
            "1                100     65.467662      0.083531              24.145444   \n",
            "2                100     65.403406     -0.113516              25.154737   \n",
            "3                100     66.551436     -0.074966              25.552439   \n",
            "4                100     67.174713      0.023559              28.480576   \n",
            "..               ...           ...           ...                    ...   \n",
            "658              600      0.150426     26.175288               0.893086   \n",
            "659              600      0.041422     24.880312               0.788236   \n",
            "660              600      0.021801     24.905384               0.957551   \n",
            "661              600     -0.106825     25.121213               0.909939   \n",
            "662              600     -0.366256     19.045293               0.386067   \n",
            "\n",
            "     HG (mV) std deviation  Thickness (mm) Sample  \n",
            "0                 0.634816            0.07     H1  \n",
            "1                 0.696428            0.07     H1  \n",
            "2                 0.695544            0.07     H1  \n",
            "3                 0.687035            0.07     H1  \n",
            "4                 0.729388            0.07     H1  \n",
            "..                     ...             ...    ...  \n",
            "658               9.735454            0.07     H1  \n",
            "659               9.709209            0.07     H1  \n",
            "660               9.699742            0.07     H1  \n",
            "661               9.328112            0.07     H1  \n",
            "662              15.081684            0.07     H1  \n",
            "\n",
            "[663 rows x 7 columns]\n",
            "y:  [9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
            " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\n",
            "torch.Size([663, 6])\n",
            "torch.Size([663])\n"
          ]
        }
      ],
      "source": [
        "# Load new data\n",
        "input_data_test = '/content/drive/MyDrive/PhD/Colab Notebooks/test_data/'\n",
        "print(os.listdir(input_data_test))\n",
        "\n",
        "data_test = load_data_from_directory(input_data_test)\n",
        "\n",
        "# Load and preprocess data\n",
        "X_test, y_test = preprocess_test_data(data_test, data_percentage=8.33) # 1s window size\n",
        "#X_test, y_test = preprocess_test_data(data_test, data_percentage=100) # 1s window size\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faC6EfmDMsRL"
      },
      "source": [
        "## Run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxNfBFvFowYp",
        "outputId": "30aaf77f-5f65-4811-a35c-7e70c5c9a3dc",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "tensor([ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4, 11,  4,  4,\n",
            "         4,  4,  4,  4,  4,  4,  4,  4, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,  8,  8,  8,\n",
            "         8,  8,  8,  8,  8, 15,  8,  8,  8,  8,  0, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15,  8, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15,  6, 15,  6,  6, 15,  6,  6, 15, 15, 15,  6,  6,  6, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,  9, 15, 15, 15,  9, 15, 15,\n",
            "        15, 15, 15,  9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,  9, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,  9, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
            "        11,  0, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,  0,  6,  6,  6,\n",
            "         6,  6,  4,  4,  4, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,  0,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,  0,  4,  4,  4,  4,  4,\n",
            "         4,  4,  4,  4,  4,  4,  4,  0, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15,  6,  4,  4,  4,  4,  4,  4, 15,  4, 15,  4, 15, 15,  6,  6,\n",
            "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15,  9,  9,  9,  9,  9, 15, 15, 15, 15,  9, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 14, 14, 15, 15, 15, 15, 15, 15,\n",
            "        14, 15, 15,  9, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 11, 11, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 15, 11, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15,  0, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 14,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15,  6, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 11,  6,\n",
            "         6,  6,  6, 15,  6,  6,  6,  6, 15,  6, 15,  6, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15,  6, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15,  6, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,  6])\n",
            "Predicted labels: ['E1' 'E1' 'E1' 'E1' 'E1' 'E1' 'E1' 'E1' 'E1' 'E1' 'E1' 'E1' 'E1' 'E1'\n",
            " 'E1' 'J1' 'E1' 'E1' 'E1' 'E1' 'E1' 'E1' 'E1' 'E1' 'E1' 'E1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'M1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'G1' 'G1' 'G1' 'G1' 'G1'\n",
            " 'G1' 'G1' 'G1' 'N1' 'G1' 'G1' 'G1' 'G1' 'A1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'G1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'E3' 'N1' 'E3' 'E3' 'N1' 'E3' 'E3'\n",
            " 'N1' 'N1' 'N1' 'E3' 'E3' 'E3' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'H1' 'N1' 'N1' 'N1' 'H1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'H1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'H1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'H1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'J1' 'J1' 'J1' 'J1' 'J1' 'J1' 'J1' 'J1' 'J1' 'J1' 'J1' 'A1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'A1' 'E3'\n",
            " 'E3' 'E3' 'E3' 'E3' 'E1' 'E1' 'E1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'A1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'A1' 'E1' 'E1' 'E1' 'E1' 'E1'\n",
            " 'E1' 'E1' 'E1' 'E1' 'E1' 'E1' 'E1' 'A1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'E3' 'E1' 'E1' 'E1' 'E1' 'E1' 'E1'\n",
            " 'N1' 'E1' 'N1' 'E1' 'N1' 'N1' 'E3' 'E3' 'E3' 'E3' 'E3' 'E3' 'E3' 'E3'\n",
            " 'E3' 'E3' 'E3' 'E3' 'E3' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'H1' 'H1' 'H1' 'H1' 'H1' 'N1' 'N1' 'N1' 'N1' 'H1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'M1' 'M1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'M1' 'N1' 'N1' 'H1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'M1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'J1' 'J1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'J1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'A1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'M1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'E3' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'J1' 'E3' 'E3' 'E3' 'E3' 'N1'\n",
            " 'E3' 'E3' 'E3' 'E3' 'N1' 'E3' 'N1' 'E3' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'E3' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'N1' 'E3' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
            " 'N1' 'N1' 'N1' 'N1' 'E3']\n",
            "Classes in label encoder: ['A1' 'B1' 'C1' 'D1' 'E1' 'E2' 'E3' 'F1' 'G1' 'H1' 'I1' 'J1' 'K1' 'L1'\n",
            " 'M1' 'N1' 'REF']\n",
            "Number of classes: 17\n",
            "Accuracy: 1.81%\n",
            "Number of predictions that are class 9: 12\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          A1       0.00      0.00      0.00         0\n",
            "          E1       0.00      0.00      0.00         0\n",
            "          E3       0.00      0.00      0.00         0\n",
            "          G1       0.00      0.00      0.00         0\n",
            "          H1       1.00      0.02      0.04       663\n",
            "          J1       0.00      0.00      0.00         0\n",
            "          M1       0.00      0.00      0.00         0\n",
            "          N1       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.02       663\n",
            "   macro avg       0.12      0.00      0.00       663\n",
            "weighted avg       1.00      0.02      0.04       663\n",
            "\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  7   0   0   0  48   0  41   0  13  12   0  16   0   0   6 520   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-167-521afbafd517>:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "# Initialize the model\n",
        "input_dim = X_test.shape[-1]\n",
        "hidden_dim = config['hidden_dim']  # Replace with the hidden dimension used during training\n",
        "output_dim = config['classes']  # Replace with the number of output classes used during training\n",
        "model = Multiclass(input_dim, hidden_dim, output_dim).to(device)\n",
        "\n",
        "# Load label encoder\n",
        "label_encoder_path = '/content/drive/MyDrive/PhD/Colab Notebooks/label_encoder.pkl'\n",
        "le = joblib.load(label_encoder_path)\n",
        "\n",
        "# Load pretrained model\n",
        "model_path = '/content/drive/MyDrive/PhD/Colab Notebooks/MLP_model.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "\n",
        "# Run inferences\n",
        "with torch.no_grad():\n",
        "    X_test = X_test.to(device)\n",
        "    outputs = model(X_test)\n",
        "    _, predicted = torch.max(outputs.data, 1) #\n",
        "\n",
        "# Convert predicted labels to original labels\n",
        "# Decode the predicted labels\n",
        "# Now perform the inverse transform\n",
        "\n",
        "print(predicted)\n",
        "\n",
        "predicted_labels = le.inverse_transform(predicted.cpu().numpy())\n",
        "\n",
        "# Print the results\n",
        "print(\"Predicted labels:\", predicted_labels)\n",
        "print(\"Classes in label encoder:\", le.classes_)\n",
        "print(\"Number of classes:\", len(le.classes_))\n",
        "\n",
        "# Calculate percentage of correct predictions\n",
        "correct_predictions = (predicted == y_test).sum().item()\n",
        "total_samples = len(y_test)\n",
        "accuracy = correct_predictions / total_samples * 100\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Calculate number of predictions that are class = 9\n",
        "num_class_9_predictions = (predicted == 9).sum().item()\n",
        "print(f\"Number of predictions that are class 9: {num_class_9_predictions}\")\n",
        "\n",
        "\n",
        "# Calculate the classification report\n",
        "print(classification_report(le.inverse_transform(y_test.cpu().numpy()), predicted_labels))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(le.inverse_transform(y_test.cpu().numpy()), predicted_labels, labels=le.classes_)\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Original Label | Encoded Value |\n",
        "|----------------|---------------|\n",
        "| A1             | 0             |\n",
        "| B1             | 1             |\n",
        "| C1             | 2             |\n",
        "| D1             | 3             |\n",
        "| E1             | 4             |\n",
        "| E2             | 5             |\n",
        "| E3             | 6             |\n",
        "| F1             | 7             |\n",
        "| G1             | 8             |\n",
        "| H1             | 9             |\n",
        "| I1             | 10            |\n",
        "| J1             | 11            |\n",
        "| K1             | 12            |\n",
        "| L1             | 13            |\n",
        "| M1             | 14            |\n",
        "| N1             | 15            |\n",
        "| REF            | 16            |"
      ],
      "metadata": {
        "id": "ktsgpY21NcOV"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}