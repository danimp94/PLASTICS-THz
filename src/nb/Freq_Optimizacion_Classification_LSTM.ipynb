{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peUcVKtbGPfD"
   },
   "source": [
    "# LSTM Classification Model\n",
    "\n",
    "This notebook demonstrates how to load data, preprocess it, define an LSTM model, train the model, and evaluate its performance. The data is assumed to be in CSV format and stored in a directory.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we need to install the necessary libraries. Run the following cell to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61837,
     "status": "ok",
     "timestamp": 1730988731870,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "aAZ2n50rGS-C",
    "outputId": "784a4900-1f7f-4693-f530-d141bc6460c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio\n",
    "%pip install pandas scikit-learn\n",
    "%pip install wandb onnx -Uq\n",
    "%pip install joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4d3KxhxGML8"
   },
   "source": [
    "## Import Libraries and seed\n",
    "Import the necessary libraries for data processing, model building, training, and evaluation. Adding a seed ensures reproducibility by making sure that the random number generation is consistent across different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26369,
     "status": "ok",
     "timestamp": 1730988758235,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "9a9HvzrNG8iw",
    "outputId": "6ae16851-8701-4f82-cd15-02866950d23c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import wandb\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23260,
     "status": "ok",
     "timestamp": 1730988781491,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "SUQtUPxSBzq4",
    "outputId": "50b7f26f-9d99-468c-fc71-938adb100bfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "executionInfo": {
     "elapsed": 23743,
     "status": "ok",
     "timestamp": 1730988805231,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "AoCyZSTt7qqS",
    "outputId": "837dcd63-164a-4e17-8b69-b2387b098e97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n",
    "#94b4debef3cc9601df4d91995649548f8ab3a097"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1-6Xv6LHHmZ"
   },
   "source": [
    "## Load Data from Github Repository\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1730988805587,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "eI7hrAaVHL44"
   },
   "outputs": [],
   "source": [
    "## Remove PIC-PAPER-01 folder:\n",
    "!rm -rf PIC-PAPER-01\n",
    "\n",
    "# # Download Github Repo (Private) https://stackoverflow.com/questions/74532852/clone-github-repo-with-fine-grained-token/78280453#78280453\n",
    "# !git clone --no-checkout https://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@github.com/danimp94/PIC-PAPER-01.git\n",
    "\n",
    "# # To clone data folder only:\n",
    "# %cd PIC-PAPER-01 # Navigate to the repository directory\n",
    "# !git sparse-checkout init --cone # Initialize sparse-checkout\n",
    "# !git sparse-checkout set data # Set the sparse-checkout to include only the data/ folder\n",
    "# !git checkout # Checkout the specified folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1730988805587,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "hAY8BfVX9XCK"
   },
   "outputs": [],
   "source": [
    "def load_data_from_directory(input_path):\n",
    "    data_frames = []\n",
    "    for file in os.listdir(input_path):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(input_path, file), delimiter=';', header=0)\n",
    "            data_frames.append(df)\n",
    "    data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    print(data)\n",
    "    print(data.shape)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiCTw-qcKXhn"
   },
   "source": [
    "## Preprocessing Data\n",
    "Define a function to preprocess the data. This includes encoding categorical labels and standardizing the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1730989220248,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "zJoZqi5LDPl8"
   },
   "outputs": [],
   "source": [
    "def calculate_averages_and_dispersion(data, data_percentage):\n",
    "    df = data\n",
    "    results = []\n",
    "    for (sample, freq), group in df.groupby(['Sample', 'Frequency (GHz)']):\n",
    "        window_size = max(1, int(len(group) * data_percentage / 100))\n",
    "        # print(f\"Processing sample: {sample}, frequency: {freq} with window size: {window_size}\")\n",
    "        for start in range(0, len(group), window_size):\n",
    "            window_data = group.iloc[start:start + window_size]\n",
    "            mean_values = window_data[['LG (mV)', 'HG (mV)']].mean()\n",
    "            std_deviation_values = window_data[['LG (mV)', 'HG (mV)']].std()\n",
    "            results.append({\n",
    "                'Frequency (GHz)': freq,\n",
    "                'LG (mV) mean': mean_values['LG (mV)'],\n",
    "                'HG (mV) mean': mean_values['HG (mV)'],\n",
    "                'LG (mV) std deviation': std_deviation_values['LG (mV)'],\n",
    "                'HG (mV) std deviation': std_deviation_values['HG (mV)'],\n",
    "                # 'Thickness (mm)': window_data['Thickness (mm)'].iloc[0], ## COMMENT\n",
    "                'Sample': sample,\n",
    "            })\n",
    "    results_df = pd.DataFrame(results)\n",
    "    # results_df.to_csv(output_file, sep=';', index=False)\n",
    "    # print(f\"Processed {input_file} and saved to {output_file}\")\n",
    "    # print(results_df)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1730989298179,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "KZPdMwkkKmm7"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(data, data_percentage):\n",
    "    # Windowing the data\n",
    "    data = calculate_averages_and_dispersion(data, data_percentage)\n",
    "    print(data.shape)\n",
    "\n",
    "    # Assuming the last column is the target\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "\n",
    "    # Encode the target variable if it's categorical\n",
    "    if y.dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "    # le is the fitted LabelEncoder - Saving Encoder\n",
    "    joblib.dump(le, 'label_encoder.pkl')\n",
    "\n",
    "    # Comment\n",
    "    # Get the original labels and their encoded values\n",
    "    original_labels = le.classes_\n",
    "    encoded_values = le.transform(original_labels)\n",
    "\n",
    "    # Create a DataFrame to display the mapping\n",
    "    # label_mapping_df = pd.DataFrame({\n",
    "    #     'Original Label': original_labels,\n",
    "    #     'Encoded Value': encoded_values\n",
    "    # })\n",
    "\n",
    "    # Display the DataFrame\n",
    "    # print(label_mapping_df)\n",
    "\n",
    "\n",
    "    # Normalization TBD\n",
    "    # # Standardize the features\n",
    "    # scaler = StandardScaler()\n",
    "    # X = scaler.fit_transform(X)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXiN30xA1kKX"
   },
   "source": [
    "## Group Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1730992185234,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "Kijzt73J1nyo"
   },
   "outputs": [],
   "source": [
    "def group_frequencies(data, num_frequencies):\n",
    "    # Ensure the data is sorted by frequency\n",
    "    # data = data.sort_values(by='Frequency (GHz)')\n",
    "\n",
    "    # Create a data frame with the data of each group\n",
    "    data_groups = []\n",
    "    unique_frequencies = data['Frequency (GHz)'].unique()\n",
    "    for freq in unique_frequencies:\n",
    "        data_groups.append(data[data['Frequency (GHz)'] == freq])\n",
    "    # return data_groups\n",
    "\n",
    "    grouped_data = []\n",
    "\n",
    "    num_unique_frequencies = len(unique_frequencies)\n",
    "\n",
    "    for i in range(0, num_unique_frequencies, num_frequencies):\n",
    "        grouped_data.append(pd.concat(data_groups[i:i + num_frequencies]))\n",
    "    return grouped_data\n",
    "\n",
    "\n",
    "\n",
    "    # Preprocess the data for each group\n",
    "def preprocess_data_groups(data_groups, data_percentage):\n",
    "    X_groups = []\n",
    "    y_groups = []\n",
    "    for data in data_groups:\n",
    "        # Skip if the DataFrame is empty\n",
    "        if data.empty:\n",
    "            continue\n",
    "        X, y = preprocess_data(data, data_percentage)\n",
    "        X_groups.append(X)\n",
    "        y_groups.append(y)\n",
    "    return X_groups, y_groups\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40971,
     "status": "ok",
     "timestamp": 1730992227525,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "UTXvxCzu6MBA",
    "outputId": "7886a9f8-5ffc-4e3f-8e75-0b300463e0a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Sample  Frequency (GHz)     LG (mV)    HG (mV)  Thickness (mm)\n",
      "0           A1            100.0   -7.080942  -0.854611             0.2\n",
      "1           A1            100.0   67.024785   0.244141             0.2\n",
      "2           A1            100.0  124.893178  -1.098776             0.2\n",
      "3           A1            100.0   91.075571   0.000000             0.2\n",
      "4           A1            100.0   48.956174   0.122094             0.2\n",
      "...        ...              ...         ...        ...             ...\n",
      "2420620    REF            600.0    0.366256  16.237333             0.0\n",
      "2420621    REF            600.0    0.000000  -7.080942             0.0\n",
      "2420622    REF            600.0   -0.244170  15.260652             0.0\n",
      "2420623    REF            600.0    0.366256  20.021975             0.0\n",
      "2420624    REF            600.0    0.122085  13.185203             0.0\n",
      "\n",
      "[2420625 rows x 5 columns]\n",
      "(2420625, 5)\n",
      "[        Sample  Frequency (GHz)     LG (mV)   HG (mV)  Thickness (mm)\n",
      "0           A1            100.0   -7.080942 -0.854611             0.2\n",
      "1           A1            100.0   67.024785  0.244141             0.2\n",
      "2           A1            100.0  124.893178 -1.098776             0.2\n",
      "3           A1            100.0   91.075571  0.000000             0.2\n",
      "4           A1            100.0   48.956174  0.122094             0.2\n",
      "...        ...              ...         ...       ...             ...\n",
      "2281019    REF            140.0   69.344401 -0.610447             0.0\n",
      "2281020    REF            140.0   81.675008 -0.610447             0.0\n",
      "2281021    REF            140.0   99.377364  0.854588             0.0\n",
      "2281022    REF            140.0   73.006958  1.587129             0.0\n",
      "2281023    REF            140.0   71.786106 -0.610423             0.0\n",
      "\n",
      "[215758 rows x 5 columns],         Sample  Frequency (GHz)    LG (mV)   HG (mV)  Thickness (mm)\n",
      "14710       A1            150.0  66.536441  0.488353             0.2\n",
      "14711       A1            150.0  60.798436  0.366282             0.2\n",
      "14712       A1            150.0  52.862898 -0.122070             0.2\n",
      "14713       A1            150.0  62.385544 -0.122118             0.2\n",
      "14714       A1            150.0  63.118055  0.244212             0.2\n",
      "...        ...              ...        ...       ...             ...\n",
      "2296058    REF            190.0  11.598095 -0.610447             0.0\n",
      "2296059    REF            190.0   8.179709 -1.587081             0.0\n",
      "2296060    REF            190.0   8.668050 -0.122070             0.0\n",
      "2296061    REF            190.0  14.772311 -0.610352             0.0\n",
      "2296062    REF            190.0  14.283970 -0.366282             0.0\n",
      "\n",
      "[223126 rows x 5 columns],         Sample  Frequency (GHz)   LG (mV)   HG (mV)  Thickness (mm)\n",
      "30122       A1            200.0  3.052130  0.732493             0.2\n",
      "30123       A1            200.0  7.569283 -0.244141             0.2\n",
      "30124       A1            200.0  2.319619 -0.976706             0.2\n",
      "30125       A1            200.0 -0.366256 -0.732493             0.2\n",
      "30126       A1            200.0  1.342937  1.342916             0.2\n",
      "...        ...              ...       ...       ...             ...\n",
      "2310386    REF            240.0  5.860090 -0.122046             0.0\n",
      "2310387    REF            240.0  6.348431 -1.587105             0.0\n",
      "2310388    REF            240.0  5.127579  0.366259             0.0\n",
      "2310389    REF            240.0  8.545965 -1.464987             0.0\n",
      "2310390    REF            240.0  7.447198 -1.220846             0.0\n",
      "\n",
      "[284451 rows x 5 columns],         Sample  Frequency (GHz)   LG (mV)     HG (mV)  Thickness (mm)\n",
      "58277       A1            250.0  1.587108   -0.732493             0.2\n",
      "58278       A1            250.0  2.075449    0.488353             0.2\n",
      "58279       A1            250.0  1.709193   -0.366282             0.2\n",
      "58280       A1            250.0  2.563789   -0.122070             0.2\n",
      "58281       A1            250.0  2.319619   -0.854635             0.2\n",
      "...        ...              ...       ...         ...             ...\n",
      "2325737    REF            290.0  3.174216  737.516809             0.0\n",
      "2325738    REF            290.0 -0.732511  669.759536             0.0\n",
      "2325739    REF            290.0  2.075449  661.579800             0.0\n",
      "2325740    REF            290.0  0.854596  689.171052             0.0\n",
      "2325741    REF            290.0  1.709193  718.105268             0.0\n",
      "\n",
      "[227701 rows x 5 columns],         Sample  Frequency (GHz)   LG (mV)     HG (mV)  Thickness (mm)\n",
      "73406       A1            300.0  2.197534   -0.000048             0.2\n",
      "73407       A1            300.0  3.296301   -0.488329             0.2\n",
      "73408       A1            300.0  2.319619    0.732517             0.2\n",
      "73409       A1            300.0  3.174216    0.488305             0.2\n",
      "73410       A1            300.0  2.685875    0.976706             0.2\n",
      "...        ...              ...       ...         ...             ...\n",
      "2341385    REF            340.0 -1.098767  454.156995             0.0\n",
      "2341386    REF            340.0 -0.122085  445.366907             0.0\n",
      "2341387    REF            340.0  3.052130  463.557601             0.0\n",
      "2341388    REF            340.0 -0.488341  486.143374             0.0\n",
      "2341389    REF            340.0  0.488341  507.508278             0.0\n",
      "\n",
      "[228605 rows x 5 columns],         Sample  Frequency (GHz)   LG (mV)     HG (mV)  Thickness (mm)\n",
      "88880       A1            350.0  1.220852  536.198246             0.2\n",
      "88881       A1            350.0  1.831278  545.843005             0.2\n",
      "88882       A1            350.0  0.732511  532.657754             0.2\n",
      "88883       A1            350.0  2.197534  569.405413             0.2\n",
      "88884       A1            350.0  1.709193  516.542542             0.2\n",
      "...        ...              ...       ...         ...             ...\n",
      "2357014    REF            390.0  2.197534  658.771849             0.0\n",
      "2357015    REF            390.0 -0.610426  717.372727             0.0\n",
      "2357016    REF            390.0  0.854596  659.626460             0.0\n",
      "2357017    REF            390.0  0.488341  642.656565             0.0\n",
      "2357018    REF            390.0  1.831278  673.544049             0.0\n",
      "\n",
      "[226697 rows x 5 columns],         Sample  Frequency (GHz)   LG (mV)     HG (mV)  Thickness (mm)\n",
      "104313      A1            400.0  0.488341  223.049694             0.2\n",
      "104314      A1            400.0  0.854596  240.752035             0.2\n",
      "104315      A1            400.0 -1.342937  231.107306             0.2\n",
      "104316      A1            400.0 -0.488341  239.897442             0.2\n",
      "104317      A1            400.0 -0.610426  239.287025             0.2\n",
      "...        ...              ...       ...         ...             ...\n",
      "2371796    REF            440.0  1.220852  250.396800             0.0\n",
      "2371797    REF            440.0  1.953363  221.950978             0.0\n",
      "2371798    REF            440.0  0.000000  246.978420             0.0\n",
      "2371799    REF            440.0  1.709193  219.631332             0.0\n",
      "2371800    REF            440.0  0.854596  223.782223             0.0\n",
      "\n",
      "[226187 rows x 5 columns],         Sample  Frequency (GHz)   LG (mV)     HG (mV)  Thickness (mm)\n",
      "119669      A1            450.0 -0.488341  143.083870             0.2\n",
      "119670      A1            450.0  1.342937  140.520081             0.2\n",
      "119671      A1            450.0 -0.366256  151.507750             0.2\n",
      "119672      A1            450.0 -0.854596  134.782076             0.2\n",
      "119673      A1            450.0  0.976682  134.537905             0.2\n",
      "...        ...              ...       ...         ...             ...\n",
      "2385415    REF            490.0  0.244170   87.290932             0.0\n",
      "2385416    REF            490.0  0.976682   74.105735             0.0\n",
      "2385417    REF            490.0 -1.342937   74.471983             0.0\n",
      "2385418    REF            490.0  1.342937   53.839581             0.0\n",
      "2385419    REF            490.0  1.342937   67.391041             0.0\n",
      "\n",
      "[224461 rows x 5 columns],         Sample  Frequency (GHz)   LG (mV)    HG (mV)  Thickness (mm)\n",
      "134107      A1            500.0 -1.342937  42.973995             0.2\n",
      "134108      A1            500.0 -0.366256  30.887559             0.2\n",
      "134109      A1            500.0 -1.098767  46.026126             0.2\n",
      "134110      A1            500.0  0.488341  20.021975             0.2\n",
      "134111      A1            500.0  0.122085  36.015138             0.2\n",
      "...        ...              ...       ...        ...             ...\n",
      "2400847    REF            540.0 -0.366256   8.057624             0.0\n",
      "2400848    REF            540.0  0.488341  21.975338             0.0\n",
      "2400849    REF            540.0  1.098767   8.423880             0.0\n",
      "2400850    REF            540.0 -0.610426  10.499328             0.0\n",
      "2400851    REF            540.0 -1.220852  -2.197534             0.0\n",
      "\n",
      "[227901 rows x 5 columns],         Sample  Frequency (GHz)   LG (mV)    HG (mV)  Thickness (mm)\n",
      "148792      A1            550.0  0.000000  15.138566             0.2\n",
      "148793      A1            550.0  0.000000  18.556952             0.2\n",
      "148794      A1            550.0 -0.854596  18.923208             0.2\n",
      "148795      A1            550.0 -1.465023  10.010988             0.2\n",
      "148796      A1            550.0 -1.709193  16.359419             0.2\n",
      "...        ...              ...       ...        ...             ...\n",
      "2416319    REF            590.0 -0.610426  11.720181             0.0\n",
      "2416320    REF            590.0 -0.610426   1.220852             0.0\n",
      "2416321    REF            590.0  0.854596   9.400561             0.0\n",
      "2416322    REF            590.0  0.976682   4.272982             0.0\n",
      "2416323    REF            590.0  0.976682  31.742156             0.0\n",
      "\n",
      "[224569 rows x 5 columns],         Sample  Frequency (GHz)    LG (mV)    HG (mV)  Thickness (mm)\n",
      "163988      A1            600.0  60.676354  -0.976634             0.2\n",
      "163989      A1            600.0  65.926018   0.976706             0.2\n",
      "163990      A1            600.0  29.056284   1.464987             0.2\n",
      "163991      A1            600.0  82.163352   0.976682             0.2\n",
      "163992      A1            600.0  68.001464   0.244188             0.2\n",
      "...        ...              ...        ...        ...             ...\n",
      "2420620    REF            600.0   0.366256  16.237333             0.0\n",
      "2420621    REF            600.0   0.000000  -7.080942             0.0\n",
      "2420622    REF            600.0  -0.244170  15.260652             0.0\n",
      "2420623    REF            600.0   0.366256  20.021975             0.0\n",
      "2420624    REF            600.0   0.122085  13.185203             0.0\n",
      "\n",
      "[111169 rows x 5 columns]]\n"
     ]
    }
   ],
   "source": [
    "input_path = '/content/drive/MyDrive/PhD/Colab Notebooks/training_data/'\n",
    "data = load_data_from_directory(input_path)\n",
    "\n",
    "# # Load and preprocess data\n",
    "# X, y = preprocess_data(data, data_percentage=3.7) # 1s window size\n",
    "\n",
    "# Split the data in groups of frequencies i.e. 100-140, 150-190 and so on\n",
    "data_groups = group_frequencies(data, num_frequencies=5)\n",
    "X_groups, y_groups = preprocess_data_groups(data_groups, data_percentage=3.7)\n",
    "\n",
    "print(data_groups)\n",
    "\n",
    "# Number of groups\n",
    "num_groups = len(data_groups)\n",
    "print(f\"Number of groups: {num_groups}\")\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# X_groups = torch.tensor(X_groups, dtype=torch.float32)\n",
    "# y_groups = torch.tensor(y_groups, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZjha9Cx6MBB"
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1730990362723,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "oLRhwd8cKKJH"
   },
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    epochs=15,\n",
    "    seed = 48,\n",
    "    classes = data['Sample'].nunique(), # Each different sample is a different class\n",
    "    k_folds = 4,  # Number of folds for cross-validation\n",
    "    batch_size=128,\n",
    "    sequence_length = 1, #Check\n",
    "    learning_rate=0.001,\n",
    "    dataset=\"experiment_1\",\n",
    "    architecture=\"LSTM\",\n",
    "    hidden_dim = 512\n",
    "    )\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RI96M_V6KpyH"
   },
   "source": [
    "## Define Model\n",
    "Define the LSTM model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6j-_U_RKxlR"
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        print(f'input_dim: {input_dim}, hidden_dim: {hidden_dim}, output_dim: {output_dim}')\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers = 1, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        # self.softmax = nn.Softmax(dim=1) # Add softmax for multi-class classification\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states with zeros\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        # num_layers * num_directions = 1 in this case\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_dim).to(x.device)  # Use x.size(0) for batch size\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_dim).to(x.device)  # Use x.size(0) for batch size\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # Pass h0 and c0 to LSTM\n",
    "\n",
    "        # Apply dropout to the output and take only the hidden state of the last timestep\n",
    "        out = self.dropout(out[:, -1, :])  # Keep sequence dimension for proper input to fc layer\n",
    "\n",
    "        # Decode the hidden state of the last timestep\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Check how to pass input data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afvxx1KzKzYp"
   },
   "source": [
    "## Train Model\n",
    "Define a function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3a9IuRVK3Wq"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, config):\n",
    "    num_epochs = config.epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "              X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "              outputs = model(X_batch)\n",
    "              loss = criterion(outputs, y_batch)\n",
    "\n",
    "              optimizer.zero_grad()\n",
    "              loss.backward()\n",
    "              optimizer.step()\n",
    "\n",
    "              running_loss += loss.item()\n",
    "\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Log metrics to W&B\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": running_loss / len(train_loader), \"val_loss\": val_loss / len(val_loader)})\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1bDpP8JK5jp"
   },
   "source": [
    "## Evaluate Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIDNXJGRK9af"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on the test set: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBOELYWD6MBB"
   },
   "outputs": [],
   "source": [
    "def make(config, X, y):\n",
    "    # K-Fold Cross-Validation\n",
    "    kfold = KFold(n_splits=config.k_folds, shuffle=True, random_state=config.seed)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
    "        print(f'Fold {fold+1}/{config.k_folds}')\n",
    "\n",
    "        # Create DataLoader for training and validation sets\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # Convert data to tensors and add sequence length dimension\n",
    "        X_train = torch.tensor(X_train).float()\n",
    "        X_val = torch.tensor(X_val).float()\n",
    "        y_train = torch.tensor(y_train).long()\n",
    "        y_val = torch.tensor(y_val).long()\n",
    "        print(X_train.shape)\n",
    "        print(y_train.shape)\n",
    "\n",
    "        # Reshape input tensors to (batch_size, sequence_length, num_features)\n",
    "        X_train = X_train.reshape( X_train.shape[0], config.sequence_length, X_train.shape[-1])\n",
    "        X_val = X_val.reshape( X_val.shape[0],config.sequence_length, X_val.shape[-1])\n",
    "        print(X_train.shape)\n",
    "        print(y_train.shape)\n",
    "\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        val_dataset = TensorDataset(X_val, y_val)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "        print(X_train.shape)\n",
    "        print(y_train.shape)\n",
    "        # Initialize the model, loss function, and optimizer\n",
    "        input_dim = X_train.shape[-1] # The number of expected features in the input x\n",
    "        hidden_dim = config.hidden_dim\n",
    "        output_dim = config.classes\n",
    "        model = LSTMModel(input_dim, hidden_dim, output_dim).to(device)\n",
    "        # criterion = nn.CrossEntropyLoss()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "        yield model, train_loader, val_loader, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlnfXqu-6MBB"
   },
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "    with wandb.init(project=\"PIC-PAPER-03-Freq-OPTIM\", config=hyperparameters):\n",
    "        config = wandb.config\n",
    "\n",
    "        # Set seed for reproducibility\n",
    "        set_seed(config.seed)\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=config.seed)\n",
    "\n",
    "        # K-Fold Cross-Validation\n",
    "        for model, train_loader, val_loader, criterion, optimizer in make(config, X_train, y_train):\n",
    "            print(model)\n",
    "\n",
    "            # Train the model\n",
    "            train_model(model, train_loader, val_loader, criterion, optimizer, device, config)\n",
    "\n",
    "            # Evaluate the model on the validation set\n",
    "            evaluate_model(model, val_loader, device)\n",
    "\n",
    "        # Evaluate the final model on the test set\n",
    "        X_test = torch.tensor(X_test).float()\n",
    "        print('X_test shape:', X_test.shape)\n",
    "        # Reshape input tensors to (batch_size, sequence_length, num_features)\n",
    "        X_test = X_test.reshape(X_test.shape[0],config.sequence_length, X_test.shape[-1])\n",
    "        print('X_test shape:', X_test.shape)\n",
    "        y_test = torch.tensor(y_test).long()\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "        print('test_loader shape:', test_loader)\n",
    "        # Evaluate the final model on the test set\n",
    "        evaluate_model(model, test_loader, device)\n",
    "\n",
    "        # Save the model using the wandb run name # Moved inside the function\n",
    "        run_name = wandb.run.name\n",
    "        print(f\"Wandb model name: {run_name}\")\n",
    "        torch.save(model.state_dict(), f'/content/drive/MyDrive/PhD/Colab Notebooks/trained_models/{run_name}.pth')\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEtZPi2fLQGL"
   },
   "source": [
    "## Run Training\n",
    "\n",
    "Do not use it if just want to run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tScgXvxA6MBC",
    "outputId": "ca945612-a5cf-4d3a-efe4-3b83e98c3e92"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241105_204134-tl45o9km</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-02-LSTM/runs/tl45o9km' target=\"_blank\">colorful-bush-17</a></strong> to <a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-02-LSTM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-02-LSTM' target=\"_blank\">https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-02-LSTM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-02-LSTM/runs/tl45o9km' target=\"_blank\">https://wandb.ai/danimp94-university-carlos-iii-of-madrid/PIC-PAPER-02-LSTM/runs/tl45o9km</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/4\n",
      "torch.Size([14458, 5])\n",
      "torch.Size([14458])\n",
      "torch.Size([14458, 1, 5])\n",
      "torch.Size([14458])\n",
      "torch.Size([14458, 1, 5])\n",
      "torch.Size([14458])\n",
      "input_dim: 5, hidden_dim: 512, output_dim: 15\n",
      "LSTMModel(\n",
      "  (lstm): LSTM(5, 512, batch_first=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=15, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-632b0d8f4136>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train).float()\n",
      "<ipython-input-46-632b0d8f4136>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val).float()\n",
      "<ipython-input-46-632b0d8f4136>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train).long()\n",
      "<ipython-input-46-632b0d8f4136>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val = torch.tensor(y_val).long()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1500], Train Loss: 2.6806, Val Loss: 2.6509\n",
      "Epoch [2/1500], Train Loss: 2.6385, Val Loss: 2.6177\n",
      "Epoch [3/1500], Train Loss: 2.6078, Val Loss: 2.5897\n",
      "Epoch [4/1500], Train Loss: 2.5786, Val Loss: 2.5651\n",
      "Epoch [5/1500], Train Loss: 2.5555, Val Loss: 2.5392\n",
      "Epoch [6/1500], Train Loss: 2.5354, Val Loss: 2.5133\n",
      "Epoch [7/1500], Train Loss: 2.5076, Val Loss: 2.4864\n",
      "Epoch [8/1500], Train Loss: 2.4812, Val Loss: 2.4660\n",
      "Epoch [9/1500], Train Loss: 2.4616, Val Loss: 2.4240\n",
      "Epoch [10/1500], Train Loss: 2.4369, Val Loss: 2.4031\n",
      "Epoch [11/1500], Train Loss: 2.4084, Val Loss: 2.3863\n",
      "Epoch [12/1500], Train Loss: 2.3890, Val Loss: 2.3541\n",
      "Epoch [13/1500], Train Loss: 2.3693, Val Loss: 2.3329\n",
      "Epoch [14/1500], Train Loss: 2.3428, Val Loss: 2.3173\n",
      "Epoch [15/1500], Train Loss: 2.3321, Val Loss: 2.3035\n",
      "Epoch [16/1500], Train Loss: 2.3216, Val Loss: 2.2921\n",
      "Epoch [17/1500], Train Loss: 2.2996, Val Loss: 2.2652\n",
      "Epoch [18/1500], Train Loss: 2.2901, Val Loss: 2.2410\n",
      "Epoch [19/1500], Train Loss: 2.2839, Val Loss: 2.2397\n",
      "Epoch [20/1500], Train Loss: 2.2627, Val Loss: 2.2268\n",
      "Epoch [21/1500], Train Loss: 2.2541, Val Loss: 2.2260\n",
      "Epoch [22/1500], Train Loss: 2.2451, Val Loss: 2.2091\n",
      "Epoch [23/1500], Train Loss: 2.2345, Val Loss: 2.1972\n",
      "Epoch [24/1500], Train Loss: 2.2214, Val Loss: 2.1739\n",
      "Epoch [25/1500], Train Loss: 2.2168, Val Loss: 2.1682\n",
      "Epoch [26/1500], Train Loss: 2.2025, Val Loss: 2.1637\n",
      "Epoch [27/1500], Train Loss: 2.1992, Val Loss: 2.1535\n",
      "Epoch [28/1500], Train Loss: 2.1876, Val Loss: 2.1339\n",
      "Epoch [29/1500], Train Loss: 2.1805, Val Loss: 2.1378\n",
      "Epoch [30/1500], Train Loss: 2.1698, Val Loss: 2.1457\n",
      "Epoch [31/1500], Train Loss: 2.1692, Val Loss: 2.1357\n",
      "Epoch [32/1500], Train Loss: 2.1527, Val Loss: 2.1150\n",
      "Epoch [33/1500], Train Loss: 2.1511, Val Loss: 2.1109\n",
      "Epoch [34/1500], Train Loss: 2.1464, Val Loss: 2.1016\n",
      "Epoch [35/1500], Train Loss: 2.1363, Val Loss: 2.0969\n",
      "Epoch [36/1500], Train Loss: 2.1298, Val Loss: 2.0830\n",
      "Epoch [37/1500], Train Loss: 2.1215, Val Loss: 2.0772\n",
      "Epoch [38/1500], Train Loss: 2.1167, Val Loss: 2.0854\n",
      "Epoch [39/1500], Train Loss: 2.1123, Val Loss: 2.0637\n",
      "Epoch [40/1500], Train Loss: 2.1107, Val Loss: 2.0542\n",
      "Epoch [41/1500], Train Loss: 2.0960, Val Loss: 2.0500\n",
      "Epoch [42/1500], Train Loss: 2.0945, Val Loss: 2.0429\n",
      "Epoch [43/1500], Train Loss: 2.0839, Val Loss: 2.0268\n",
      "Epoch [44/1500], Train Loss: 2.0792, Val Loss: 2.0468\n",
      "Epoch [45/1500], Train Loss: 2.0752, Val Loss: 2.0251\n",
      "Epoch [46/1500], Train Loss: 2.0626, Val Loss: 2.0080\n",
      "Epoch [47/1500], Train Loss: 2.0706, Val Loss: 2.0025\n",
      "Epoch [48/1500], Train Loss: 2.0579, Val Loss: 2.0058\n",
      "Epoch [49/1500], Train Loss: 2.0458, Val Loss: 1.9959\n",
      "Epoch [50/1500], Train Loss: 2.0334, Val Loss: 1.9910\n",
      "Epoch [51/1500], Train Loss: 2.0405, Val Loss: 1.9803\n",
      "Epoch [52/1500], Train Loss: 2.0277, Val Loss: 1.9719\n",
      "Epoch [53/1500], Train Loss: 2.0260, Val Loss: 1.9683\n",
      "Epoch [54/1500], Train Loss: 2.0186, Val Loss: 1.9656\n",
      "Epoch [55/1500], Train Loss: 2.0110, Val Loss: 1.9608\n",
      "Epoch [56/1500], Train Loss: 2.0075, Val Loss: 1.9364\n",
      "Epoch [57/1500], Train Loss: 1.9987, Val Loss: 1.9445\n",
      "Epoch [58/1500], Train Loss: 1.9943, Val Loss: 1.9341\n",
      "Epoch [59/1500], Train Loss: 1.9901, Val Loss: 1.9222\n",
      "Epoch [60/1500], Train Loss: 1.9891, Val Loss: 1.9309\n",
      "Epoch [61/1500], Train Loss: 1.9819, Val Loss: 1.9192\n",
      "Epoch [62/1500], Train Loss: 1.9754, Val Loss: 1.9185\n",
      "Epoch [63/1500], Train Loss: 1.9743, Val Loss: 1.9032\n",
      "Epoch [64/1500], Train Loss: 1.9607, Val Loss: 1.8991\n",
      "Epoch [65/1500], Train Loss: 1.9641, Val Loss: 1.8984\n",
      "Epoch [66/1500], Train Loss: 1.9449, Val Loss: 1.8934\n",
      "Epoch [67/1500], Train Loss: 1.9512, Val Loss: 1.8846\n",
      "Epoch [68/1500], Train Loss: 1.9394, Val Loss: 1.8713\n",
      "Epoch [69/1500], Train Loss: 1.9437, Val Loss: 1.8639\n",
      "Epoch [70/1500], Train Loss: 1.9225, Val Loss: 1.8831\n",
      "Epoch [71/1500], Train Loss: 1.9357, Val Loss: 1.8642\n",
      "Epoch [72/1500], Train Loss: 1.9335, Val Loss: 1.8769\n",
      "Epoch [73/1500], Train Loss: 1.9159, Val Loss: 1.8569\n",
      "Epoch [74/1500], Train Loss: 1.9158, Val Loss: 1.8502\n",
      "Epoch [75/1500], Train Loss: 1.9086, Val Loss: 1.8433\n",
      "Epoch [76/1500], Train Loss: 1.9079, Val Loss: 1.8503\n",
      "Epoch [77/1500], Train Loss: 1.9046, Val Loss: 1.8330\n",
      "Epoch [78/1500], Train Loss: 1.8994, Val Loss: 1.8216\n",
      "Epoch [79/1500], Train Loss: 1.8958, Val Loss: 1.8429\n",
      "Epoch [80/1500], Train Loss: 1.8961, Val Loss: 1.8385\n",
      "Epoch [81/1500], Train Loss: 1.8912, Val Loss: 1.8135\n",
      "Epoch [82/1500], Train Loss: 1.8832, Val Loss: 1.8169\n",
      "Epoch [83/1500], Train Loss: 1.8771, Val Loss: 1.8364\n",
      "Epoch [84/1500], Train Loss: 1.8738, Val Loss: 1.8058\n",
      "Epoch [85/1500], Train Loss: 1.8790, Val Loss: 1.8027\n",
      "Epoch [86/1500], Train Loss: 1.8683, Val Loss: 1.7880\n",
      "Epoch [87/1500], Train Loss: 1.8738, Val Loss: 1.8091\n",
      "Epoch [88/1500], Train Loss: 1.8633, Val Loss: 1.7970\n",
      "Epoch [89/1500], Train Loss: 1.8562, Val Loss: 1.7786\n",
      "Epoch [90/1500], Train Loss: 1.8550, Val Loss: 1.8000\n",
      "Epoch [91/1500], Train Loss: 1.8527, Val Loss: 1.7742\n",
      "Epoch [92/1500], Train Loss: 1.8483, Val Loss: 1.7598\n",
      "Epoch [93/1500], Train Loss: 1.8486, Val Loss: 1.7682\n",
      "Epoch [94/1500], Train Loss: 1.8360, Val Loss: 1.7918\n",
      "Epoch [95/1500], Train Loss: 1.8355, Val Loss: 1.7449\n",
      "Epoch [96/1500], Train Loss: 1.8364, Val Loss: 1.7426\n",
      "Epoch [97/1500], Train Loss: 1.8366, Val Loss: 1.7773\n",
      "Epoch [98/1500], Train Loss: 1.8228, Val Loss: 1.7341\n",
      "Epoch [99/1500], Train Loss: 1.8206, Val Loss: 1.7378\n",
      "Epoch [100/1500], Train Loss: 1.8222, Val Loss: 1.7299\n",
      "Epoch [101/1500], Train Loss: 1.8123, Val Loss: 1.7265\n",
      "Epoch [102/1500], Train Loss: 1.8089, Val Loss: 1.7346\n",
      "Epoch [103/1500], Train Loss: 1.8115, Val Loss: 1.7103\n",
      "Epoch [104/1500], Train Loss: 1.8139, Val Loss: 1.7407\n",
      "Epoch [105/1500], Train Loss: 1.8060, Val Loss: 1.7270\n",
      "Epoch [106/1500], Train Loss: 1.7933, Val Loss: 1.7182\n",
      "Epoch [107/1500], Train Loss: 1.7892, Val Loss: 1.7177\n",
      "Epoch [108/1500], Train Loss: 1.7988, Val Loss: 1.7079\n",
      "Epoch [109/1500], Train Loss: 1.7988, Val Loss: 1.7254\n",
      "Epoch [110/1500], Train Loss: 1.7903, Val Loss: 1.7001\n",
      "Epoch [111/1500], Train Loss: 1.7834, Val Loss: 1.7062\n",
      "Epoch [112/1500], Train Loss: 1.7804, Val Loss: 1.6803\n",
      "Epoch [113/1500], Train Loss: 1.7772, Val Loss: 1.6910\n",
      "Epoch [114/1500], Train Loss: 1.7697, Val Loss: 1.6857\n",
      "Epoch [115/1500], Train Loss: 1.7827, Val Loss: 1.6834\n",
      "Epoch [116/1500], Train Loss: 1.7760, Val Loss: 1.6907\n",
      "Epoch [117/1500], Train Loss: 1.7625, Val Loss: 1.6737\n",
      "Epoch [118/1500], Train Loss: 1.7645, Val Loss: 1.6793\n",
      "Epoch [119/1500], Train Loss: 1.7573, Val Loss: 1.6744\n",
      "Epoch [120/1500], Train Loss: 1.7606, Val Loss: 1.6796\n",
      "Epoch [121/1500], Train Loss: 1.7525, Val Loss: 1.6577\n",
      "Epoch [122/1500], Train Loss: 1.7557, Val Loss: 1.6616\n",
      "Epoch [123/1500], Train Loss: 1.7472, Val Loss: 1.6802\n",
      "Epoch [124/1500], Train Loss: 1.7508, Val Loss: 1.6629\n",
      "Epoch [125/1500], Train Loss: 1.7454, Val Loss: 1.6521\n",
      "Epoch [126/1500], Train Loss: 1.7374, Val Loss: 1.6600\n",
      "Epoch [127/1500], Train Loss: 1.7410, Val Loss: 1.6539\n",
      "Epoch [128/1500], Train Loss: 1.7414, Val Loss: 1.6490\n",
      "Epoch [129/1500], Train Loss: 1.7371, Val Loss: 1.6392\n",
      "Epoch [130/1500], Train Loss: 1.7326, Val Loss: 1.6569\n",
      "Epoch [131/1500], Train Loss: 1.7346, Val Loss: 1.6256\n",
      "Epoch [132/1500], Train Loss: 1.7375, Val Loss: 1.6287\n",
      "Epoch [133/1500], Train Loss: 1.7292, Val Loss: 1.6417\n",
      "Epoch [134/1500], Train Loss: 1.7342, Val Loss: 1.6207\n",
      "Epoch [135/1500], Train Loss: 1.7198, Val Loss: 1.6302\n",
      "Epoch [136/1500], Train Loss: 1.7168, Val Loss: 1.6206\n",
      "Epoch [137/1500], Train Loss: 1.7209, Val Loss: 1.6320\n",
      "Epoch [138/1500], Train Loss: 1.7237, Val Loss: 1.6233\n",
      "Epoch [139/1500], Train Loss: 1.7158, Val Loss: 1.6395\n",
      "Epoch [140/1500], Train Loss: 1.7218, Val Loss: 1.6221\n",
      "Epoch [141/1500], Train Loss: 1.7088, Val Loss: 1.6117\n",
      "Epoch [142/1500], Train Loss: 1.7138, Val Loss: 1.6141\n",
      "Epoch [143/1500], Train Loss: 1.6992, Val Loss: 1.6190\n",
      "Epoch [144/1500], Train Loss: 1.6976, Val Loss: 1.6124\n",
      "Epoch [145/1500], Train Loss: 1.7057, Val Loss: 1.6030\n",
      "Epoch [146/1500], Train Loss: 1.6979, Val Loss: 1.6074\n",
      "Epoch [147/1500], Train Loss: 1.6969, Val Loss: 1.5911\n",
      "Epoch [148/1500], Train Loss: 1.7000, Val Loss: 1.6001\n",
      "Epoch [149/1500], Train Loss: 1.6847, Val Loss: 1.5927\n",
      "Epoch [150/1500], Train Loss: 1.6841, Val Loss: 1.6180\n",
      "Epoch [151/1500], Train Loss: 1.6886, Val Loss: 1.5821\n",
      "Epoch [152/1500], Train Loss: 1.6724, Val Loss: 1.5815\n",
      "Epoch [153/1500], Train Loss: 1.6829, Val Loss: 1.5798\n",
      "Epoch [154/1500], Train Loss: 1.6799, Val Loss: 1.5908\n",
      "Epoch [155/1500], Train Loss: 1.6925, Val Loss: 1.5950\n",
      "Epoch [156/1500], Train Loss: 1.6778, Val Loss: 1.5860\n",
      "Epoch [157/1500], Train Loss: 1.6746, Val Loss: 1.6019\n",
      "Epoch [158/1500], Train Loss: 1.6794, Val Loss: 1.5686\n",
      "Epoch [159/1500], Train Loss: 1.6823, Val Loss: 1.6004\n",
      "Epoch [160/1500], Train Loss: 1.6791, Val Loss: 1.5650\n",
      "Epoch [161/1500], Train Loss: 1.6665, Val Loss: 1.5719\n",
      "Epoch [162/1500], Train Loss: 1.6716, Val Loss: 1.5789\n",
      "Epoch [163/1500], Train Loss: 1.6667, Val Loss: 1.5797\n",
      "Epoch [164/1500], Train Loss: 1.6621, Val Loss: 1.5688\n",
      "Epoch [165/1500], Train Loss: 1.6616, Val Loss: 1.5609\n",
      "Epoch [166/1500], Train Loss: 1.6617, Val Loss: 1.5739\n",
      "Epoch [167/1500], Train Loss: 1.6676, Val Loss: 1.5653\n",
      "Epoch [168/1500], Train Loss: 1.6588, Val Loss: 1.5790\n",
      "Epoch [169/1500], Train Loss: 1.6547, Val Loss: 1.5595\n",
      "Epoch [170/1500], Train Loss: 1.6564, Val Loss: 1.5827\n",
      "Epoch [171/1500], Train Loss: 1.6584, Val Loss: 1.5606\n",
      "Epoch [172/1500], Train Loss: 1.6525, Val Loss: 1.5621\n",
      "Epoch [173/1500], Train Loss: 1.6639, Val Loss: 1.5500\n",
      "Epoch [174/1500], Train Loss: 1.6557, Val Loss: 1.5355\n",
      "Epoch [175/1500], Train Loss: 1.6489, Val Loss: 1.5331\n",
      "Epoch [176/1500], Train Loss: 1.6451, Val Loss: 1.5329\n",
      "Epoch [177/1500], Train Loss: 1.6425, Val Loss: 1.5512\n",
      "Epoch [178/1500], Train Loss: 1.6534, Val Loss: 1.5500\n",
      "Epoch [179/1500], Train Loss: 1.6445, Val Loss: 1.5471\n",
      "Epoch [180/1500], Train Loss: 1.6388, Val Loss: 1.5876\n",
      "Epoch [181/1500], Train Loss: 1.6449, Val Loss: 1.5506\n",
      "Epoch [182/1500], Train Loss: 1.6370, Val Loss: 1.5341\n",
      "Epoch [183/1500], Train Loss: 1.6386, Val Loss: 1.5377\n",
      "Epoch [184/1500], Train Loss: 1.6275, Val Loss: 1.5229\n",
      "Epoch [185/1500], Train Loss: 1.6431, Val Loss: 1.5372\n",
      "Epoch [186/1500], Train Loss: 1.6342, Val Loss: 1.5511\n",
      "Epoch [187/1500], Train Loss: 1.6243, Val Loss: 1.5361\n",
      "Epoch [188/1500], Train Loss: 1.6241, Val Loss: 1.5184\n",
      "Epoch [189/1500], Train Loss: 1.6229, Val Loss: 1.5377\n",
      "Epoch [190/1500], Train Loss: 1.6209, Val Loss: 1.5324\n",
      "Epoch [191/1500], Train Loss: 1.6186, Val Loss: 1.5263\n",
      "Epoch [192/1500], Train Loss: 1.6238, Val Loss: 1.5355\n",
      "Epoch [193/1500], Train Loss: 1.6160, Val Loss: 1.5155\n",
      "Epoch [194/1500], Train Loss: 1.6139, Val Loss: 1.5331\n",
      "Epoch [195/1500], Train Loss: 1.6136, Val Loss: 1.5171\n",
      "Epoch [196/1500], Train Loss: 1.6159, Val Loss: 1.5127\n",
      "Epoch [197/1500], Train Loss: 1.6122, Val Loss: 1.5063\n",
      "Epoch [198/1500], Train Loss: 1.6128, Val Loss: 1.5056\n",
      "Epoch [199/1500], Train Loss: 1.6193, Val Loss: 1.4933\n",
      "Epoch [200/1500], Train Loss: 1.6017, Val Loss: 1.4888\n",
      "Epoch [201/1500], Train Loss: 1.6006, Val Loss: 1.4933\n",
      "Epoch [202/1500], Train Loss: 1.6023, Val Loss: 1.5030\n",
      "Epoch [203/1500], Train Loss: 1.6201, Val Loss: 1.5256\n",
      "Epoch [204/1500], Train Loss: 1.6084, Val Loss: 1.5041\n",
      "Epoch [205/1500], Train Loss: 1.5951, Val Loss: 1.5110\n",
      "Epoch [206/1500], Train Loss: 1.5946, Val Loss: 1.5095\n",
      "Epoch [207/1500], Train Loss: 1.5919, Val Loss: 1.5066\n",
      "Epoch [208/1500], Train Loss: 1.6120, Val Loss: 1.4968\n",
      "Epoch [209/1500], Train Loss: 1.6012, Val Loss: 1.4919\n",
      "Epoch [210/1500], Train Loss: 1.5929, Val Loss: 1.4964\n",
      "Epoch [211/1500], Train Loss: 1.5919, Val Loss: 1.4843\n",
      "Epoch [212/1500], Train Loss: 1.5980, Val Loss: 1.4833\n",
      "Epoch [213/1500], Train Loss: 1.5837, Val Loss: 1.4824\n",
      "Epoch [214/1500], Train Loss: 1.5920, Val Loss: 1.4937\n",
      "Epoch [215/1500], Train Loss: 1.6048, Val Loss: 1.4992\n",
      "Epoch [216/1500], Train Loss: 1.5924, Val Loss: 1.5259\n",
      "Epoch [217/1500], Train Loss: 1.5880, Val Loss: 1.5007\n",
      "Epoch [218/1500], Train Loss: 1.5923, Val Loss: 1.4926\n",
      "Epoch [219/1500], Train Loss: 1.5853, Val Loss: 1.4670\n",
      "Epoch [220/1500], Train Loss: 1.5841, Val Loss: 1.4860\n",
      "Epoch [221/1500], Train Loss: 1.5864, Val Loss: 1.4751\n",
      "Epoch [222/1500], Train Loss: 1.5824, Val Loss: 1.4659\n",
      "Epoch [223/1500], Train Loss: 1.5818, Val Loss: 1.4838\n",
      "Epoch [224/1500], Train Loss: 1.5800, Val Loss: 1.5028\n",
      "Epoch [225/1500], Train Loss: 1.5813, Val Loss: 1.4756\n",
      "Epoch [226/1500], Train Loss: 1.5837, Val Loss: 1.5072\n",
      "Epoch [227/1500], Train Loss: 1.5805, Val Loss: 1.4698\n",
      "Epoch [228/1500], Train Loss: 1.5738, Val Loss: 1.4574\n",
      "Epoch [229/1500], Train Loss: 1.5723, Val Loss: 1.4843\n",
      "Epoch [230/1500], Train Loss: 1.5728, Val Loss: 1.4701\n",
      "Epoch [231/1500], Train Loss: 1.5711, Val Loss: 1.4699\n",
      "Epoch [232/1500], Train Loss: 1.5684, Val Loss: 1.4780\n",
      "Epoch [233/1500], Train Loss: 1.5771, Val Loss: 1.4468\n",
      "Epoch [234/1500], Train Loss: 1.5708, Val Loss: 1.4661\n",
      "Epoch [235/1500], Train Loss: 1.5638, Val Loss: 1.4907\n",
      "Epoch [236/1500], Train Loss: 1.5682, Val Loss: 1.4501\n",
      "Epoch [237/1500], Train Loss: 1.5611, Val Loss: 1.4567\n",
      "Epoch [238/1500], Train Loss: 1.5601, Val Loss: 1.4623\n",
      "Epoch [239/1500], Train Loss: 1.5616, Val Loss: 1.4918\n",
      "Epoch [240/1500], Train Loss: 1.5673, Val Loss: 1.4580\n",
      "Epoch [241/1500], Train Loss: 1.5529, Val Loss: 1.4644\n",
      "Epoch [242/1500], Train Loss: 1.5564, Val Loss: 1.4527\n",
      "Epoch [243/1500], Train Loss: 1.5591, Val Loss: 1.4556\n",
      "Epoch [244/1500], Train Loss: 1.5513, Val Loss: 1.4528\n",
      "Epoch [245/1500], Train Loss: 1.5626, Val Loss: 1.4509\n",
      "Epoch [246/1500], Train Loss: 1.5478, Val Loss: 1.4445\n",
      "Epoch [247/1500], Train Loss: 1.5487, Val Loss: 1.4448\n",
      "Epoch [248/1500], Train Loss: 1.5552, Val Loss: 1.4341\n",
      "Epoch [249/1500], Train Loss: 1.5503, Val Loss: 1.4497\n",
      "Epoch [250/1500], Train Loss: 1.5457, Val Loss: 1.4365\n",
      "Epoch [251/1500], Train Loss: 1.5406, Val Loss: 1.4432\n",
      "Epoch [252/1500], Train Loss: 1.5539, Val Loss: 1.4652\n",
      "Epoch [253/1500], Train Loss: 1.5673, Val Loss: 1.4311\n",
      "Epoch [254/1500], Train Loss: 1.5438, Val Loss: 1.4506\n",
      "Epoch [255/1500], Train Loss: 1.5546, Val Loss: 1.4551\n",
      "Epoch [256/1500], Train Loss: 1.5381, Val Loss: 1.4542\n",
      "Epoch [257/1500], Train Loss: 1.5475, Val Loss: 1.4567\n",
      "Epoch [258/1500], Train Loss: 1.5426, Val Loss: 1.4557\n",
      "Epoch [259/1500], Train Loss: 1.5409, Val Loss: 1.4335\n",
      "Epoch [260/1500], Train Loss: 1.5490, Val Loss: 1.4446\n",
      "Epoch [261/1500], Train Loss: 1.5382, Val Loss: 1.4244\n",
      "Epoch [262/1500], Train Loss: 1.5415, Val Loss: 1.4330\n",
      "Epoch [263/1500], Train Loss: 1.5449, Val Loss: 1.4290\n",
      "Epoch [264/1500], Train Loss: 1.5346, Val Loss: 1.4284\n",
      "Epoch [265/1500], Train Loss: 1.5333, Val Loss: 1.4567\n",
      "Epoch [266/1500], Train Loss: 1.5297, Val Loss: 1.4322\n",
      "Epoch [267/1500], Train Loss: 1.5439, Val Loss: 1.4502\n",
      "Epoch [268/1500], Train Loss: 1.5485, Val Loss: 1.4260\n",
      "Epoch [269/1500], Train Loss: 1.5324, Val Loss: 1.4487\n",
      "Epoch [270/1500], Train Loss: 1.5342, Val Loss: 1.4808\n",
      "Epoch [271/1500], Train Loss: 1.5402, Val Loss: 1.4340\n",
      "Epoch [272/1500], Train Loss: 1.5335, Val Loss: 1.4170\n",
      "Epoch [273/1500], Train Loss: 1.5211, Val Loss: 1.4254\n",
      "Epoch [274/1500], Train Loss: 1.5198, Val Loss: 1.4310\n",
      "Epoch [275/1500], Train Loss: 1.5274, Val Loss: 1.4586\n",
      "Epoch [276/1500], Train Loss: 1.5239, Val Loss: 1.4276\n",
      "Epoch [277/1500], Train Loss: 1.5319, Val Loss: 1.4189\n",
      "Epoch [278/1500], Train Loss: 1.5218, Val Loss: 1.4121\n",
      "Epoch [279/1500], Train Loss: 1.5316, Val Loss: 1.4400\n",
      "Epoch [280/1500], Train Loss: 1.5250, Val Loss: 1.4346\n",
      "Epoch [281/1500], Train Loss: 1.5325, Val Loss: 1.4202\n",
      "Epoch [282/1500], Train Loss: 1.5319, Val Loss: 1.4673\n",
      "Epoch [283/1500], Train Loss: 1.5150, Val Loss: 1.4213\n",
      "Epoch [284/1500], Train Loss: 1.5156, Val Loss: 1.4302\n",
      "Epoch [285/1500], Train Loss: 1.5161, Val Loss: 1.4040\n",
      "Epoch [286/1500], Train Loss: 1.5216, Val Loss: 1.4303\n",
      "Epoch [287/1500], Train Loss: 1.5265, Val Loss: 1.4177\n",
      "Epoch [288/1500], Train Loss: 1.5034, Val Loss: 1.4080\n",
      "Epoch [289/1500], Train Loss: 1.5152, Val Loss: 1.4037\n",
      "Epoch [290/1500], Train Loss: 1.5191, Val Loss: 1.4191\n",
      "Epoch [291/1500], Train Loss: 1.5228, Val Loss: 1.4151\n",
      "Epoch [292/1500], Train Loss: 1.5099, Val Loss: 1.4152\n",
      "Epoch [293/1500], Train Loss: 1.5113, Val Loss: 1.4352\n",
      "Epoch [294/1500], Train Loss: 1.5068, Val Loss: 1.4013\n",
      "Epoch [295/1500], Train Loss: 1.5201, Val Loss: 1.4083\n",
      "Epoch [296/1500], Train Loss: 1.5124, Val Loss: 1.4048\n",
      "Epoch [297/1500], Train Loss: 1.5137, Val Loss: 1.3998\n",
      "Epoch [298/1500], Train Loss: 1.5134, Val Loss: 1.4324\n",
      "Epoch [299/1500], Train Loss: 1.5111, Val Loss: 1.4110\n",
      "Epoch [300/1500], Train Loss: 1.5136, Val Loss: 1.4124\n",
      "Epoch [301/1500], Train Loss: 1.5052, Val Loss: 1.3901\n",
      "Epoch [302/1500], Train Loss: 1.4982, Val Loss: 1.4129\n",
      "Epoch [303/1500], Train Loss: 1.5111, Val Loss: 1.4024\n",
      "Epoch [304/1500], Train Loss: 1.5069, Val Loss: 1.4274\n",
      "Epoch [305/1500], Train Loss: 1.5033, Val Loss: 1.4069\n",
      "Epoch [306/1500], Train Loss: 1.5068, Val Loss: 1.4174\n",
      "Epoch [307/1500], Train Loss: 1.5029, Val Loss: 1.4040\n",
      "Epoch [308/1500], Train Loss: 1.4956, Val Loss: 1.4062\n",
      "Epoch [309/1500], Train Loss: 1.5095, Val Loss: 1.4108\n",
      "Epoch [310/1500], Train Loss: 1.5105, Val Loss: 1.4349\n",
      "Epoch [311/1500], Train Loss: 1.5002, Val Loss: 1.3999\n",
      "Epoch [312/1500], Train Loss: 1.5008, Val Loss: 1.4117\n",
      "Epoch [313/1500], Train Loss: 1.4943, Val Loss: 1.4145\n",
      "Epoch [314/1500], Train Loss: 1.5036, Val Loss: 1.4155\n",
      "Epoch [315/1500], Train Loss: 1.5037, Val Loss: 1.3716\n",
      "Epoch [316/1500], Train Loss: 1.4975, Val Loss: 1.4034\n",
      "Epoch [317/1500], Train Loss: 1.5081, Val Loss: 1.4197\n",
      "Epoch [318/1500], Train Loss: 1.4866, Val Loss: 1.4016\n",
      "Epoch [319/1500], Train Loss: 1.4988, Val Loss: 1.3856\n",
      "Epoch [320/1500], Train Loss: 1.4874, Val Loss: 1.3964\n",
      "Epoch [321/1500], Train Loss: 1.5023, Val Loss: 1.3953\n",
      "Epoch [322/1500], Train Loss: 1.4794, Val Loss: 1.3794\n",
      "Epoch [323/1500], Train Loss: 1.4924, Val Loss: 1.3833\n",
      "Epoch [324/1500], Train Loss: 1.4903, Val Loss: 1.4381\n",
      "Epoch [325/1500], Train Loss: 1.4818, Val Loss: 1.3709\n",
      "Epoch [326/1500], Train Loss: 1.4932, Val Loss: 1.3884\n",
      "Epoch [327/1500], Train Loss: 1.4928, Val Loss: 1.3887\n",
      "Epoch [328/1500], Train Loss: 1.4808, Val Loss: 1.3693\n",
      "Epoch [329/1500], Train Loss: 1.4914, Val Loss: 1.3726\n",
      "Epoch [330/1500], Train Loss: 1.4961, Val Loss: 1.3717\n",
      "Epoch [331/1500], Train Loss: 1.4892, Val Loss: 1.3746\n",
      "Epoch [332/1500], Train Loss: 1.4749, Val Loss: 1.3934\n",
      "Epoch [333/1500], Train Loss: 1.4658, Val Loss: 1.3745\n",
      "Epoch [334/1500], Train Loss: 1.4799, Val Loss: 1.3762\n",
      "Epoch [335/1500], Train Loss: 1.4764, Val Loss: 1.3709\n",
      "Epoch [336/1500], Train Loss: 1.4823, Val Loss: 1.3917\n",
      "Epoch [337/1500], Train Loss: 1.4705, Val Loss: 1.3842\n",
      "Epoch [338/1500], Train Loss: 1.4836, Val Loss: 1.3775\n",
      "Epoch [339/1500], Train Loss: 1.4761, Val Loss: 1.3888\n",
      "Epoch [340/1500], Train Loss: 1.4889, Val Loss: 1.3857\n",
      "Epoch [341/1500], Train Loss: 1.4842, Val Loss: 1.3743\n",
      "Epoch [342/1500], Train Loss: 1.4885, Val Loss: 1.3974\n",
      "Epoch [343/1500], Train Loss: 1.4853, Val Loss: 1.3912\n",
      "Epoch [344/1500], Train Loss: 1.4730, Val Loss: 1.3743\n",
      "Epoch [345/1500], Train Loss: 1.4812, Val Loss: 1.3550\n",
      "Epoch [346/1500], Train Loss: 1.4727, Val Loss: 1.3711\n",
      "Epoch [347/1500], Train Loss: 1.4722, Val Loss: 1.3678\n",
      "Epoch [348/1500], Train Loss: 1.4668, Val Loss: 1.3633\n",
      "Epoch [349/1500], Train Loss: 1.4793, Val Loss: 1.3661\n",
      "Epoch [350/1500], Train Loss: 1.4753, Val Loss: 1.3747\n",
      "Epoch [351/1500], Train Loss: 1.4724, Val Loss: 1.3865\n",
      "Epoch [352/1500], Train Loss: 1.4715, Val Loss: 1.3648\n",
      "Epoch [353/1500], Train Loss: 1.4621, Val Loss: 1.3618\n",
      "Epoch [354/1500], Train Loss: 1.4728, Val Loss: 1.3846\n",
      "Epoch [355/1500], Train Loss: 1.4702, Val Loss: 1.3703\n",
      "Epoch [356/1500], Train Loss: 1.4659, Val Loss: 1.4192\n",
      "Epoch [357/1500], Train Loss: 1.4735, Val Loss: 1.3992\n",
      "Epoch [358/1500], Train Loss: 1.4700, Val Loss: 1.3603\n",
      "Epoch [359/1500], Train Loss: 1.4707, Val Loss: 1.3722\n",
      "Epoch [360/1500], Train Loss: 1.4653, Val Loss: 1.3528\n",
      "Epoch [361/1500], Train Loss: 1.4662, Val Loss: 1.3653\n",
      "Epoch [362/1500], Train Loss: 1.4541, Val Loss: 1.3633\n",
      "Epoch [363/1500], Train Loss: 1.4567, Val Loss: 1.3619\n",
      "Epoch [364/1500], Train Loss: 1.4657, Val Loss: 1.3530\n",
      "Epoch [365/1500], Train Loss: 1.4594, Val Loss: 1.3614\n",
      "Epoch [366/1500], Train Loss: 1.4612, Val Loss: 1.3551\n",
      "Epoch [367/1500], Train Loss: 1.4517, Val Loss: 1.3703\n",
      "Epoch [368/1500], Train Loss: 1.4559, Val Loss: 1.3582\n",
      "Epoch [369/1500], Train Loss: 1.4496, Val Loss: 1.3589\n",
      "Epoch [370/1500], Train Loss: 1.4583, Val Loss: 1.3578\n",
      "Epoch [371/1500], Train Loss: 1.4490, Val Loss: 1.3584\n",
      "Epoch [372/1500], Train Loss: 1.4502, Val Loss: 1.3472\n",
      "Epoch [373/1500], Train Loss: 1.4567, Val Loss: 1.3494\n",
      "Epoch [374/1500], Train Loss: 1.4508, Val Loss: 1.3577\n",
      "Epoch [375/1500], Train Loss: 1.4457, Val Loss: 1.3535\n",
      "Epoch [376/1500], Train Loss: 1.4533, Val Loss: 1.3550\n",
      "Epoch [377/1500], Train Loss: 1.4544, Val Loss: 1.3606\n",
      "Epoch [378/1500], Train Loss: 1.4583, Val Loss: 1.3675\n",
      "Epoch [379/1500], Train Loss: 1.4457, Val Loss: 1.3400\n",
      "Epoch [380/1500], Train Loss: 1.4455, Val Loss: 1.3754\n",
      "Epoch [381/1500], Train Loss: 1.4515, Val Loss: 1.3748\n",
      "Epoch [382/1500], Train Loss: 1.4478, Val Loss: 1.3780\n",
      "Epoch [383/1500], Train Loss: 1.4396, Val Loss: 1.3574\n",
      "Epoch [384/1500], Train Loss: 1.4463, Val Loss: 1.3423\n",
      "Epoch [385/1500], Train Loss: 1.4475, Val Loss: 1.3597\n",
      "Epoch [386/1500], Train Loss: 1.4473, Val Loss: 1.3427\n",
      "Epoch [387/1500], Train Loss: 1.4492, Val Loss: 1.3386\n",
      "Epoch [388/1500], Train Loss: 1.4363, Val Loss: 1.3418\n",
      "Epoch [389/1500], Train Loss: 1.4465, Val Loss: 1.3451\n",
      "Epoch [390/1500], Train Loss: 1.4370, Val Loss: 1.3381\n",
      "Epoch [391/1500], Train Loss: 1.4430, Val Loss: 1.3308\n",
      "Epoch [392/1500], Train Loss: 1.4304, Val Loss: 1.3346\n",
      "Epoch [393/1500], Train Loss: 1.4270, Val Loss: 1.3578\n",
      "Epoch [394/1500], Train Loss: 1.4456, Val Loss: 1.3396\n",
      "Epoch [395/1500], Train Loss: 1.4373, Val Loss: 1.3777\n",
      "Epoch [396/1500], Train Loss: 1.4457, Val Loss: 1.3605\n",
      "Epoch [397/1500], Train Loss: 1.4537, Val Loss: 1.3440\n",
      "Epoch [398/1500], Train Loss: 1.4372, Val Loss: 1.3514\n",
      "Epoch [399/1500], Train Loss: 1.4387, Val Loss: 1.3426\n",
      "Epoch [400/1500], Train Loss: 1.4353, Val Loss: 1.3495\n",
      "Epoch [401/1500], Train Loss: 1.4404, Val Loss: 1.3591\n",
      "Epoch [402/1500], Train Loss: 1.4339, Val Loss: 1.3429\n",
      "Epoch [403/1500], Train Loss: 1.4339, Val Loss: 1.3493\n",
      "Epoch [404/1500], Train Loss: 1.4301, Val Loss: 1.3472\n",
      "Epoch [405/1500], Train Loss: 1.4428, Val Loss: 1.3239\n",
      "Epoch [406/1500], Train Loss: 1.4250, Val Loss: 1.3378\n",
      "Epoch [407/1500], Train Loss: 1.4449, Val Loss: 1.3549\n",
      "Epoch [408/1500], Train Loss: 1.4354, Val Loss: 1.3407\n",
      "Epoch [409/1500], Train Loss: 1.4370, Val Loss: 1.3192\n",
      "Epoch [410/1500], Train Loss: 1.4348, Val Loss: 1.3216\n",
      "Epoch [411/1500], Train Loss: 1.4309, Val Loss: 1.3435\n",
      "Epoch [412/1500], Train Loss: 1.4298, Val Loss: 1.3235\n",
      "Epoch [413/1500], Train Loss: 1.4318, Val Loss: 1.3282\n",
      "Epoch [414/1500], Train Loss: 1.4387, Val Loss: 1.3304\n",
      "Epoch [415/1500], Train Loss: 1.4325, Val Loss: 1.3172\n",
      "Epoch [416/1500], Train Loss: 1.4263, Val Loss: 1.3319\n",
      "Epoch [417/1500], Train Loss: 1.4363, Val Loss: 1.3514\n",
      "Epoch [418/1500], Train Loss: 1.4370, Val Loss: 1.3221\n",
      "Epoch [419/1500], Train Loss: 1.4218, Val Loss: 1.3128\n",
      "Epoch [420/1500], Train Loss: 1.4279, Val Loss: 1.3323\n",
      "Epoch [421/1500], Train Loss: 1.4323, Val Loss: 1.3277\n",
      "Epoch [422/1500], Train Loss: 1.4244, Val Loss: 1.3340\n",
      "Epoch [423/1500], Train Loss: 1.4294, Val Loss: 1.3075\n",
      "Epoch [424/1500], Train Loss: 1.4127, Val Loss: 1.3265\n",
      "Epoch [425/1500], Train Loss: 1.4183, Val Loss: 1.3293\n",
      "Epoch [426/1500], Train Loss: 1.4280, Val Loss: 1.3210\n",
      "Epoch [427/1500], Train Loss: 1.4311, Val Loss: 1.3284\n",
      "Epoch [428/1500], Train Loss: 1.4312, Val Loss: 1.3156\n",
      "Epoch [429/1500], Train Loss: 1.4245, Val Loss: 1.3210\n",
      "Epoch [430/1500], Train Loss: 1.4319, Val Loss: 1.3491\n",
      "Epoch [431/1500], Train Loss: 1.4251, Val Loss: 1.3246\n",
      "Epoch [432/1500], Train Loss: 1.4212, Val Loss: 1.3428\n",
      "Epoch [433/1500], Train Loss: 1.4220, Val Loss: 1.3505\n",
      "Epoch [434/1500], Train Loss: 1.4319, Val Loss: 1.3173\n",
      "Epoch [435/1500], Train Loss: 1.4210, Val Loss: 1.3164\n",
      "Epoch [436/1500], Train Loss: 1.4318, Val Loss: 1.3049\n",
      "Epoch [437/1500], Train Loss: 1.4153, Val Loss: 1.3030\n",
      "Epoch [438/1500], Train Loss: 1.4237, Val Loss: 1.3119\n",
      "Epoch [439/1500], Train Loss: 1.4007, Val Loss: 1.3261\n",
      "Epoch [440/1500], Train Loss: 1.4174, Val Loss: 1.3806\n",
      "Epoch [441/1500], Train Loss: 1.4126, Val Loss: 1.3199\n",
      "Epoch [442/1500], Train Loss: 1.4169, Val Loss: 1.3114\n",
      "Epoch [443/1500], Train Loss: 1.4135, Val Loss: 1.3413\n",
      "Epoch [444/1500], Train Loss: 1.4122, Val Loss: 1.3147\n",
      "Epoch [445/1500], Train Loss: 1.4091, Val Loss: 1.3210\n",
      "Epoch [446/1500], Train Loss: 1.4084, Val Loss: 1.3173\n",
      "Epoch [447/1500], Train Loss: 1.4139, Val Loss: 1.3321\n",
      "Epoch [448/1500], Train Loss: 1.4110, Val Loss: 1.3023\n",
      "Epoch [449/1500], Train Loss: 1.4119, Val Loss: 1.3108\n",
      "Epoch [450/1500], Train Loss: 1.4186, Val Loss: 1.3384\n",
      "Epoch [451/1500], Train Loss: 1.4157, Val Loss: 1.3372\n",
      "Epoch [452/1500], Train Loss: 1.4221, Val Loss: 1.3352\n",
      "Epoch [453/1500], Train Loss: 1.4075, Val Loss: 1.3144\n",
      "Epoch [454/1500], Train Loss: 1.4105, Val Loss: 1.3177\n",
      "Epoch [455/1500], Train Loss: 1.4181, Val Loss: 1.2989\n",
      "Epoch [456/1500], Train Loss: 1.4124, Val Loss: 1.2998\n",
      "Epoch [457/1500], Train Loss: 1.4079, Val Loss: 1.3097\n",
      "Epoch [458/1500], Train Loss: 1.4024, Val Loss: 1.3088\n",
      "Epoch [459/1500], Train Loss: 1.4072, Val Loss: 1.3151\n",
      "Epoch [460/1500], Train Loss: 1.4136, Val Loss: 1.3185\n",
      "Epoch [461/1500], Train Loss: 1.4019, Val Loss: 1.2976\n",
      "Epoch [462/1500], Train Loss: 1.3957, Val Loss: 1.2971\n",
      "Epoch [463/1500], Train Loss: 1.3937, Val Loss: 1.3115\n",
      "Epoch [464/1500], Train Loss: 1.4047, Val Loss: 1.3117\n",
      "Epoch [465/1500], Train Loss: 1.3905, Val Loss: 1.2939\n",
      "Epoch [466/1500], Train Loss: 1.4150, Val Loss: 1.3030\n",
      "Epoch [467/1500], Train Loss: 1.3960, Val Loss: 1.3133\n",
      "Epoch [468/1500], Train Loss: 1.4080, Val Loss: 1.3087\n",
      "Epoch [469/1500], Train Loss: 1.4106, Val Loss: 1.3107\n",
      "Epoch [470/1500], Train Loss: 1.4033, Val Loss: 1.3048\n",
      "Epoch [471/1500], Train Loss: 1.4066, Val Loss: 1.3248\n",
      "Epoch [472/1500], Train Loss: 1.4011, Val Loss: 1.3012\n",
      "Epoch [473/1500], Train Loss: 1.3951, Val Loss: 1.2893\n",
      "Epoch [474/1500], Train Loss: 1.3863, Val Loss: 1.2990\n",
      "Epoch [475/1500], Train Loss: 1.3977, Val Loss: 1.2869\n",
      "Epoch [476/1500], Train Loss: 1.4009, Val Loss: 1.3011\n",
      "Epoch [477/1500], Train Loss: 1.3962, Val Loss: 1.3166\n",
      "Epoch [478/1500], Train Loss: 1.4084, Val Loss: 1.3414\n",
      "Epoch [479/1500], Train Loss: 1.4135, Val Loss: 1.3213\n",
      "Epoch [480/1500], Train Loss: 1.4019, Val Loss: 1.2896\n",
      "Epoch [481/1500], Train Loss: 1.3857, Val Loss: 1.3157\n",
      "Epoch [482/1500], Train Loss: 1.3886, Val Loss: 1.3044\n",
      "Epoch [483/1500], Train Loss: 1.3877, Val Loss: 1.2973\n",
      "Epoch [484/1500], Train Loss: 1.3875, Val Loss: 1.3137\n",
      "Epoch [485/1500], Train Loss: 1.4005, Val Loss: 1.3036\n",
      "Epoch [486/1500], Train Loss: 1.3987, Val Loss: 1.2989\n",
      "Epoch [487/1500], Train Loss: 1.3980, Val Loss: 1.3003\n",
      "Epoch [488/1500], Train Loss: 1.3974, Val Loss: 1.3010\n",
      "Epoch [489/1500], Train Loss: 1.3975, Val Loss: 1.3013\n",
      "Epoch [490/1500], Train Loss: 1.4006, Val Loss: 1.2960\n",
      "Epoch [491/1500], Train Loss: 1.3896, Val Loss: 1.2889\n",
      "Epoch [492/1500], Train Loss: 1.3904, Val Loss: 1.3048\n",
      "Epoch [493/1500], Train Loss: 1.3904, Val Loss: 1.2979\n",
      "Epoch [494/1500], Train Loss: 1.3964, Val Loss: 1.2953\n",
      "Epoch [495/1500], Train Loss: 1.3884, Val Loss: 1.3024\n",
      "Epoch [496/1500], Train Loss: 1.4065, Val Loss: 1.2999\n",
      "Epoch [497/1500], Train Loss: 1.4018, Val Loss: 1.2864\n",
      "Epoch [498/1500], Train Loss: 1.3849, Val Loss: 1.2994\n",
      "Epoch [499/1500], Train Loss: 1.3901, Val Loss: 1.2909\n",
      "Epoch [500/1500], Train Loss: 1.3830, Val Loss: 1.2844\n",
      "Epoch [501/1500], Train Loss: 1.3896, Val Loss: 1.2973\n",
      "Epoch [502/1500], Train Loss: 1.3929, Val Loss: 1.2926\n",
      "Epoch [503/1500], Train Loss: 1.3967, Val Loss: 1.2902\n",
      "Epoch [504/1500], Train Loss: 1.4041, Val Loss: 1.2923\n",
      "Epoch [505/1500], Train Loss: 1.3901, Val Loss: 1.2910\n",
      "Epoch [506/1500], Train Loss: 1.3817, Val Loss: 1.2885\n",
      "Epoch [507/1500], Train Loss: 1.3882, Val Loss: 1.3014\n",
      "Epoch [508/1500], Train Loss: 1.3895, Val Loss: 1.3096\n",
      "Epoch [509/1500], Train Loss: 1.3773, Val Loss: 1.2811\n",
      "Epoch [510/1500], Train Loss: 1.3849, Val Loss: 1.2958\n",
      "Epoch [511/1500], Train Loss: 1.3927, Val Loss: 1.2933\n",
      "Epoch [512/1500], Train Loss: 1.3901, Val Loss: 1.3117\n",
      "Epoch [513/1500], Train Loss: 1.3846, Val Loss: 1.2765\n",
      "Epoch [514/1500], Train Loss: 1.3828, Val Loss: 1.2791\n",
      "Epoch [515/1500], Train Loss: 1.3841, Val Loss: 1.3024\n",
      "Epoch [516/1500], Train Loss: 1.3852, Val Loss: 1.2891\n",
      "Epoch [517/1500], Train Loss: 1.3866, Val Loss: 1.2897\n",
      "Epoch [518/1500], Train Loss: 1.3822, Val Loss: 1.2866\n",
      "Epoch [519/1500], Train Loss: 1.3778, Val Loss: 1.2807\n",
      "Epoch [520/1500], Train Loss: 1.3878, Val Loss: 1.2998\n",
      "Epoch [521/1500], Train Loss: 1.3851, Val Loss: 1.2944\n",
      "Epoch [522/1500], Train Loss: 1.3856, Val Loss: 1.2814\n",
      "Epoch [523/1500], Train Loss: 1.3856, Val Loss: 1.2720\n",
      "Epoch [524/1500], Train Loss: 1.3817, Val Loss: 1.2772\n",
      "Epoch [525/1500], Train Loss: 1.3796, Val Loss: 1.2779\n",
      "Epoch [526/1500], Train Loss: 1.3909, Val Loss: 1.2811\n",
      "Epoch [527/1500], Train Loss: 1.3884, Val Loss: 1.2889\n",
      "Epoch [528/1500], Train Loss: 1.3729, Val Loss: 1.3049\n",
      "Epoch [529/1500], Train Loss: 1.3761, Val Loss: 1.2854\n",
      "Epoch [530/1500], Train Loss: 1.3839, Val Loss: 1.2759\n",
      "Epoch [531/1500], Train Loss: 1.3719, Val Loss: 1.2809\n",
      "Epoch [532/1500], Train Loss: 1.3647, Val Loss: 1.2677\n",
      "Epoch [533/1500], Train Loss: 1.3672, Val Loss: 1.2708\n",
      "Epoch [534/1500], Train Loss: 1.3644, Val Loss: 1.2806\n",
      "Epoch [535/1500], Train Loss: 1.3685, Val Loss: 1.2838\n",
      "Epoch [536/1500], Train Loss: 1.3706, Val Loss: 1.2771\n",
      "Epoch [537/1500], Train Loss: 1.3797, Val Loss: 1.2884\n",
      "Epoch [538/1500], Train Loss: 1.3787, Val Loss: 1.2898\n",
      "Epoch [539/1500], Train Loss: 1.3889, Val Loss: 1.2718\n",
      "Epoch [540/1500], Train Loss: 1.3789, Val Loss: 1.3081\n",
      "Epoch [541/1500], Train Loss: 1.3681, Val Loss: 1.2561\n",
      "Epoch [542/1500], Train Loss: 1.3742, Val Loss: 1.2629\n",
      "Epoch [543/1500], Train Loss: 1.3607, Val Loss: 1.2812\n",
      "Epoch [544/1500], Train Loss: 1.3597, Val Loss: 1.2734\n",
      "Epoch [545/1500], Train Loss: 1.3642, Val Loss: 1.2716\n",
      "Epoch [546/1500], Train Loss: 1.3633, Val Loss: 1.2862\n",
      "Epoch [547/1500], Train Loss: 1.3686, Val Loss: 1.2756\n",
      "Epoch [548/1500], Train Loss: 1.3748, Val Loss: 1.2707\n",
      "Epoch [549/1500], Train Loss: 1.3642, Val Loss: 1.2747\n",
      "Epoch [550/1500], Train Loss: 1.3619, Val Loss: 1.2565\n",
      "Epoch [551/1500], Train Loss: 1.3648, Val Loss: 1.2817\n",
      "Epoch [552/1500], Train Loss: 1.3720, Val Loss: 1.2660\n",
      "Epoch [553/1500], Train Loss: 1.3515, Val Loss: 1.2546\n",
      "Epoch [554/1500], Train Loss: 1.3544, Val Loss: 1.2742\n",
      "Epoch [555/1500], Train Loss: 1.3679, Val Loss: 1.2665\n",
      "Epoch [556/1500], Train Loss: 1.3675, Val Loss: 1.2735\n",
      "Epoch [557/1500], Train Loss: 1.3644, Val Loss: 1.2582\n",
      "Epoch [558/1500], Train Loss: 1.3703, Val Loss: 1.2562\n",
      "Epoch [559/1500], Train Loss: 1.3564, Val Loss: 1.2559\n",
      "Epoch [560/1500], Train Loss: 1.3613, Val Loss: 1.2788\n",
      "Epoch [561/1500], Train Loss: 1.3620, Val Loss: 1.2742\n",
      "Epoch [562/1500], Train Loss: 1.3658, Val Loss: 1.2726\n",
      "Epoch [563/1500], Train Loss: 1.3580, Val Loss: 1.2730\n",
      "Epoch [564/1500], Train Loss: 1.3567, Val Loss: 1.2888\n",
      "Epoch [565/1500], Train Loss: 1.3549, Val Loss: 1.2606\n",
      "Epoch [566/1500], Train Loss: 1.3482, Val Loss: 1.2659\n",
      "Epoch [567/1500], Train Loss: 1.3654, Val Loss: 1.2888\n",
      "Epoch [568/1500], Train Loss: 1.3631, Val Loss: 1.2795\n",
      "Epoch [569/1500], Train Loss: 1.3608, Val Loss: 1.2746\n",
      "Epoch [570/1500], Train Loss: 1.3644, Val Loss: 1.2666\n",
      "Epoch [571/1500], Train Loss: 1.3701, Val Loss: 1.2746\n",
      "Epoch [572/1500], Train Loss: 1.3554, Val Loss: 1.2767\n",
      "Epoch [573/1500], Train Loss: 1.3646, Val Loss: 1.2583\n",
      "Epoch [574/1500], Train Loss: 1.3525, Val Loss: 1.2660\n",
      "Epoch [575/1500], Train Loss: 1.3621, Val Loss: 1.2669\n",
      "Epoch [576/1500], Train Loss: 1.3566, Val Loss: 1.2544\n",
      "Epoch [577/1500], Train Loss: 1.3570, Val Loss: 1.2738\n",
      "Epoch [578/1500], Train Loss: 1.3528, Val Loss: 1.2644\n",
      "Epoch [579/1500], Train Loss: 1.3526, Val Loss: 1.2780\n",
      "Epoch [580/1500], Train Loss: 1.3554, Val Loss: 1.2724\n",
      "Epoch [581/1500], Train Loss: 1.3442, Val Loss: 1.2607\n",
      "Epoch [582/1500], Train Loss: 1.3412, Val Loss: 1.2696\n",
      "Epoch [583/1500], Train Loss: 1.3425, Val Loss: 1.2640\n",
      "Epoch [584/1500], Train Loss: 1.3427, Val Loss: 1.2623\n",
      "Epoch [585/1500], Train Loss: 1.3422, Val Loss: 1.2577\n",
      "Epoch [586/1500], Train Loss: 1.3579, Val Loss: 1.2595\n",
      "Epoch [587/1500], Train Loss: 1.3554, Val Loss: 1.2618\n",
      "Epoch [588/1500], Train Loss: 1.3650, Val Loss: 1.2678\n",
      "Epoch [589/1500], Train Loss: 1.3537, Val Loss: 1.2578\n",
      "Epoch [590/1500], Train Loss: 1.3466, Val Loss: 1.2895\n",
      "Epoch [591/1500], Train Loss: 1.3471, Val Loss: 1.2655\n",
      "Epoch [592/1500], Train Loss: 1.3407, Val Loss: 1.2506\n",
      "Epoch [593/1500], Train Loss: 1.3471, Val Loss: 1.2582\n",
      "Epoch [594/1500], Train Loss: 1.3660, Val Loss: 1.2632\n",
      "Epoch [595/1500], Train Loss: 1.3358, Val Loss: 1.2472\n",
      "Epoch [596/1500], Train Loss: 1.3520, Val Loss: 1.2510\n",
      "Epoch [597/1500], Train Loss: 1.3556, Val Loss: 1.2911\n",
      "Epoch [598/1500], Train Loss: 1.3590, Val Loss: 1.2871\n",
      "Epoch [599/1500], Train Loss: 1.3506, Val Loss: 1.2798\n",
      "Epoch [600/1500], Train Loss: 1.3524, Val Loss: 1.2655\n",
      "Epoch [601/1500], Train Loss: 1.3475, Val Loss: 1.2480\n",
      "Epoch [602/1500], Train Loss: 1.3511, Val Loss: 1.2588\n",
      "Epoch [603/1500], Train Loss: 1.3426, Val Loss: 1.2584\n",
      "Epoch [604/1500], Train Loss: 1.3473, Val Loss: 1.2660\n",
      "Epoch [605/1500], Train Loss: 1.3467, Val Loss: 1.2552\n",
      "Epoch [606/1500], Train Loss: 1.3346, Val Loss: 1.2482\n",
      "Epoch [607/1500], Train Loss: 1.3429, Val Loss: 1.2351\n",
      "Epoch [608/1500], Train Loss: 1.3388, Val Loss: 1.2725\n",
      "Epoch [609/1500], Train Loss: 1.3353, Val Loss: 1.2387\n",
      "Epoch [610/1500], Train Loss: 1.3364, Val Loss: 1.2551\n",
      "Epoch [611/1500], Train Loss: 1.3376, Val Loss: 1.2649\n",
      "Epoch [612/1500], Train Loss: 1.3400, Val Loss: 1.2614\n",
      "Epoch [613/1500], Train Loss: 1.3567, Val Loss: 1.2574\n",
      "Epoch [614/1500], Train Loss: 1.3423, Val Loss: 1.2624\n",
      "Epoch [615/1500], Train Loss: 1.3436, Val Loss: 1.2657\n",
      "Epoch [616/1500], Train Loss: 1.3499, Val Loss: 1.2516\n",
      "Epoch [617/1500], Train Loss: 1.3417, Val Loss: 1.2603\n",
      "Epoch [618/1500], Train Loss: 1.3462, Val Loss: 1.2501\n",
      "Epoch [619/1500], Train Loss: 1.3437, Val Loss: 1.2518\n",
      "Epoch [620/1500], Train Loss: 1.3419, Val Loss: 1.2510\n",
      "Epoch [621/1500], Train Loss: 1.3432, Val Loss: 1.2422\n",
      "Epoch [622/1500], Train Loss: 1.3409, Val Loss: 1.2489\n",
      "Epoch [623/1500], Train Loss: 1.3367, Val Loss: 1.2713\n",
      "Epoch [624/1500], Train Loss: 1.3495, Val Loss: 1.2684\n",
      "Epoch [625/1500], Train Loss: 1.3319, Val Loss: 1.2459\n",
      "Epoch [626/1500], Train Loss: 1.3421, Val Loss: 1.2410\n",
      "Epoch [627/1500], Train Loss: 1.3410, Val Loss: 1.2463\n",
      "Epoch [628/1500], Train Loss: 1.3411, Val Loss: 1.2347\n",
      "Epoch [629/1500], Train Loss: 1.3496, Val Loss: 1.2697\n",
      "Epoch [630/1500], Train Loss: 1.3323, Val Loss: 1.2310\n",
      "Epoch [631/1500], Train Loss: 1.3368, Val Loss: 1.2480\n",
      "Epoch [632/1500], Train Loss: 1.3433, Val Loss: 1.2431\n",
      "Epoch [633/1500], Train Loss: 1.3420, Val Loss: 1.2291\n",
      "Epoch [634/1500], Train Loss: 1.3320, Val Loss: 1.2426\n",
      "Epoch [635/1500], Train Loss: 1.3255, Val Loss: 1.2458\n",
      "Epoch [636/1500], Train Loss: 1.3306, Val Loss: 1.2350\n",
      "Epoch [637/1500], Train Loss: 1.3465, Val Loss: 1.2559\n",
      "Epoch [638/1500], Train Loss: 1.3520, Val Loss: 1.2534\n",
      "Epoch [639/1500], Train Loss: 1.3339, Val Loss: 1.2334\n",
      "Epoch [640/1500], Train Loss: 1.3330, Val Loss: 1.2421\n",
      "Epoch [641/1500], Train Loss: 1.3342, Val Loss: 1.2417\n",
      "Epoch [642/1500], Train Loss: 1.3361, Val Loss: 1.2396\n",
      "Epoch [643/1500], Train Loss: 1.3253, Val Loss: 1.2463\n",
      "Epoch [644/1500], Train Loss: 1.3379, Val Loss: 1.2342\n",
      "Epoch [645/1500], Train Loss: 1.3307, Val Loss: 1.2519\n",
      "Epoch [646/1500], Train Loss: 1.3315, Val Loss: 1.2432\n",
      "Epoch [647/1500], Train Loss: 1.3324, Val Loss: 1.2491\n",
      "Epoch [648/1500], Train Loss: 1.3305, Val Loss: 1.2470\n",
      "Epoch [649/1500], Train Loss: 1.3392, Val Loss: 1.2594\n",
      "Epoch [650/1500], Train Loss: 1.3290, Val Loss: 1.2460\n",
      "Epoch [651/1500], Train Loss: 1.3319, Val Loss: 1.2455\n",
      "Epoch [652/1500], Train Loss: 1.3374, Val Loss: 1.2340\n",
      "Epoch [653/1500], Train Loss: 1.3217, Val Loss: 1.2437\n",
      "Epoch [654/1500], Train Loss: 1.3278, Val Loss: 1.2402\n",
      "Epoch [655/1500], Train Loss: 1.3273, Val Loss: 1.2227\n",
      "Epoch [656/1500], Train Loss: 1.3329, Val Loss: 1.2449\n",
      "Epoch [657/1500], Train Loss: 1.3243, Val Loss: 1.2392\n",
      "Epoch [658/1500], Train Loss: 1.3232, Val Loss: 1.2465\n",
      "Epoch [659/1500], Train Loss: 1.3303, Val Loss: 1.2440\n",
      "Epoch [660/1500], Train Loss: 1.3282, Val Loss: 1.2416\n",
      "Epoch [661/1500], Train Loss: 1.3339, Val Loss: 1.2362\n",
      "Epoch [662/1500], Train Loss: 1.3425, Val Loss: 1.2524\n",
      "Epoch [663/1500], Train Loss: 1.3280, Val Loss: 1.2465\n",
      "Epoch [664/1500], Train Loss: 1.3366, Val Loss: 1.2491\n",
      "Epoch [665/1500], Train Loss: 1.3475, Val Loss: 1.2555\n",
      "Epoch [666/1500], Train Loss: 1.3183, Val Loss: 1.2394\n",
      "Epoch [667/1500], Train Loss: 1.3227, Val Loss: 1.2259\n",
      "Epoch [668/1500], Train Loss: 1.3298, Val Loss: 1.2414\n",
      "Epoch [669/1500], Train Loss: 1.3288, Val Loss: 1.2443\n",
      "Epoch [670/1500], Train Loss: 1.3221, Val Loss: 1.2400\n",
      "Epoch [671/1500], Train Loss: 1.3302, Val Loss: 1.2440\n",
      "Epoch [672/1500], Train Loss: 1.3146, Val Loss: 1.2249\n",
      "Epoch [673/1500], Train Loss: 1.3301, Val Loss: 1.2408\n",
      "Epoch [674/1500], Train Loss: 1.3215, Val Loss: 1.2381\n",
      "Epoch [675/1500], Train Loss: 1.3252, Val Loss: 1.2222\n",
      "Epoch [676/1500], Train Loss: 1.3255, Val Loss: 1.2463\n",
      "Epoch [677/1500], Train Loss: 1.3289, Val Loss: 1.2336\n",
      "Epoch [678/1500], Train Loss: 1.3318, Val Loss: 1.2548\n",
      "Epoch [679/1500], Train Loss: 1.3230, Val Loss: 1.2295\n",
      "Epoch [680/1500], Train Loss: 1.3201, Val Loss: 1.2490\n",
      "Epoch [681/1500], Train Loss: 1.3069, Val Loss: 1.2192\n",
      "Epoch [682/1500], Train Loss: 1.3182, Val Loss: 1.2558\n",
      "Epoch [683/1500], Train Loss: 1.3300, Val Loss: 1.2372\n",
      "Epoch [684/1500], Train Loss: 1.3174, Val Loss: 1.2227\n",
      "Epoch [685/1500], Train Loss: 1.3186, Val Loss: 1.2687\n",
      "Epoch [686/1500], Train Loss: 1.3205, Val Loss: 1.2300\n",
      "Epoch [687/1500], Train Loss: 1.3165, Val Loss: 1.2525\n",
      "Epoch [688/1500], Train Loss: 1.3196, Val Loss: 1.2168\n",
      "Epoch [689/1500], Train Loss: 1.3206, Val Loss: 1.2555\n",
      "Epoch [690/1500], Train Loss: 1.3130, Val Loss: 1.2298\n",
      "Epoch [691/1500], Train Loss: 1.3206, Val Loss: 1.2364\n",
      "Epoch [692/1500], Train Loss: 1.3193, Val Loss: 1.2356\n",
      "Epoch [693/1500], Train Loss: 1.3148, Val Loss: 1.2260\n",
      "Epoch [694/1500], Train Loss: 1.3128, Val Loss: 1.2200\n",
      "Epoch [695/1500], Train Loss: 1.3227, Val Loss: 1.2395\n",
      "Epoch [696/1500], Train Loss: 1.3130, Val Loss: 1.2225\n",
      "Epoch [697/1500], Train Loss: 1.3213, Val Loss: 1.2276\n",
      "Epoch [698/1500], Train Loss: 1.3109, Val Loss: 1.2226\n",
      "Epoch [699/1500], Train Loss: 1.3272, Val Loss: 1.2124\n",
      "Epoch [700/1500], Train Loss: 1.3156, Val Loss: 1.2253\n",
      "Epoch [701/1500], Train Loss: 1.3126, Val Loss: 1.2304\n",
      "Epoch [702/1500], Train Loss: 1.3128, Val Loss: 1.2268\n",
      "Epoch [703/1500], Train Loss: 1.3161, Val Loss: 1.2472\n",
      "Epoch [704/1500], Train Loss: 1.3179, Val Loss: 1.2127\n",
      "Epoch [705/1500], Train Loss: 1.3047, Val Loss: 1.2103\n",
      "Epoch [706/1500], Train Loss: 1.3095, Val Loss: 1.2361\n",
      "Epoch [707/1500], Train Loss: 1.3215, Val Loss: 1.2212\n",
      "Epoch [708/1500], Train Loss: 1.3099, Val Loss: 1.2250\n",
      "Epoch [709/1500], Train Loss: 1.3181, Val Loss: 1.2174\n",
      "Epoch [710/1500], Train Loss: 1.3265, Val Loss: 1.2106\n",
      "Epoch [711/1500], Train Loss: 1.3078, Val Loss: 1.2386\n",
      "Epoch [712/1500], Train Loss: 1.3098, Val Loss: 1.2215\n",
      "Epoch [713/1500], Train Loss: 1.3164, Val Loss: 1.2040\n",
      "Epoch [714/1500], Train Loss: 1.3107, Val Loss: 1.2265\n",
      "Epoch [715/1500], Train Loss: 1.3188, Val Loss: 1.2210\n",
      "Epoch [716/1500], Train Loss: 1.3142, Val Loss: 1.2308\n",
      "Epoch [717/1500], Train Loss: 1.3141, Val Loss: 1.2301\n",
      "Epoch [718/1500], Train Loss: 1.3067, Val Loss: 1.2436\n",
      "Epoch [719/1500], Train Loss: 1.3055, Val Loss: 1.2203\n",
      "Epoch [720/1500], Train Loss: 1.3178, Val Loss: 1.2062\n",
      "Epoch [721/1500], Train Loss: 1.3186, Val Loss: 1.2194\n",
      "Epoch [722/1500], Train Loss: 1.3109, Val Loss: 1.2073\n",
      "Epoch [723/1500], Train Loss: 1.3117, Val Loss: 1.2182\n",
      "Epoch [724/1500], Train Loss: 1.3058, Val Loss: 1.2222\n",
      "Epoch [725/1500], Train Loss: 1.3118, Val Loss: 1.2124\n",
      "Epoch [726/1500], Train Loss: 1.3137, Val Loss: 1.2047\n",
      "Epoch [727/1500], Train Loss: 1.3107, Val Loss: 1.2386\n",
      "Epoch [728/1500], Train Loss: 1.3126, Val Loss: 1.2154\n",
      "Epoch [729/1500], Train Loss: 1.3137, Val Loss: 1.2199\n",
      "Epoch [730/1500], Train Loss: 1.3051, Val Loss: 1.2170\n",
      "Epoch [731/1500], Train Loss: 1.3092, Val Loss: 1.2092\n",
      "Epoch [732/1500], Train Loss: 1.2982, Val Loss: 1.2209\n",
      "Epoch [733/1500], Train Loss: 1.3111, Val Loss: 1.2234\n",
      "Epoch [734/1500], Train Loss: 1.3043, Val Loss: 1.2227\n",
      "Epoch [735/1500], Train Loss: 1.3099, Val Loss: 1.2099\n",
      "Epoch [736/1500], Train Loss: 1.3078, Val Loss: 1.2318\n",
      "Epoch [737/1500], Train Loss: 1.3125, Val Loss: 1.2469\n",
      "Epoch [738/1500], Train Loss: 1.3068, Val Loss: 1.2235\n",
      "Epoch [739/1500], Train Loss: 1.2956, Val Loss: 1.2242\n",
      "Epoch [740/1500], Train Loss: 1.3094, Val Loss: 1.2272\n",
      "Epoch [741/1500], Train Loss: 1.2982, Val Loss: 1.2103\n",
      "Epoch [742/1500], Train Loss: 1.2997, Val Loss: 1.2284\n",
      "Epoch [743/1500], Train Loss: 1.3135, Val Loss: 1.2120\n",
      "Epoch [744/1500], Train Loss: 1.3033, Val Loss: 1.2079\n",
      "Epoch [745/1500], Train Loss: 1.3046, Val Loss: 1.2363\n",
      "Epoch [746/1500], Train Loss: 1.2998, Val Loss: 1.2118\n",
      "Epoch [747/1500], Train Loss: 1.3027, Val Loss: 1.2045\n",
      "Epoch [748/1500], Train Loss: 1.3025, Val Loss: 1.2142\n",
      "Epoch [749/1500], Train Loss: 1.2985, Val Loss: 1.2147\n",
      "Epoch [750/1500], Train Loss: 1.3068, Val Loss: 1.2428\n",
      "Epoch [751/1500], Train Loss: 1.3092, Val Loss: 1.2287\n",
      "Epoch [752/1500], Train Loss: 1.2993, Val Loss: 1.2176\n",
      "Epoch [753/1500], Train Loss: 1.2945, Val Loss: 1.2369\n",
      "Epoch [754/1500], Train Loss: 1.2967, Val Loss: 1.2376\n",
      "Epoch [755/1500], Train Loss: 1.2988, Val Loss: 1.2201\n",
      "Epoch [756/1500], Train Loss: 1.2945, Val Loss: 1.2184\n",
      "Epoch [757/1500], Train Loss: 1.3106, Val Loss: 1.2301\n",
      "Epoch [758/1500], Train Loss: 1.2867, Val Loss: 1.2196\n",
      "Epoch [759/1500], Train Loss: 1.2938, Val Loss: 1.2129\n",
      "Epoch [760/1500], Train Loss: 1.2882, Val Loss: 1.2061\n",
      "Epoch [761/1500], Train Loss: 1.2856, Val Loss: 1.2090\n",
      "Epoch [762/1500], Train Loss: 1.2897, Val Loss: 1.2072\n",
      "Epoch [763/1500], Train Loss: 1.2934, Val Loss: 1.2137\n",
      "Epoch [764/1500], Train Loss: 1.2815, Val Loss: 1.2008\n",
      "Epoch [765/1500], Train Loss: 1.2906, Val Loss: 1.2204\n",
      "Epoch [766/1500], Train Loss: 1.2927, Val Loss: 1.2030\n",
      "Epoch [767/1500], Train Loss: 1.2991, Val Loss: 1.2013\n",
      "Epoch [768/1500], Train Loss: 1.3008, Val Loss: 1.2062\n",
      "Epoch [769/1500], Train Loss: 1.2918, Val Loss: 1.2083\n",
      "Epoch [770/1500], Train Loss: 1.3013, Val Loss: 1.2011\n",
      "Epoch [771/1500], Train Loss: 1.2886, Val Loss: 1.2266\n",
      "Epoch [772/1500], Train Loss: 1.2907, Val Loss: 1.2115\n",
      "Epoch [773/1500], Train Loss: 1.2881, Val Loss: 1.2292\n",
      "Epoch [774/1500], Train Loss: 1.3069, Val Loss: 1.1994\n",
      "Epoch [775/1500], Train Loss: 1.2973, Val Loss: 1.2084\n",
      "Epoch [776/1500], Train Loss: 1.2862, Val Loss: 1.2142\n",
      "Epoch [777/1500], Train Loss: 1.2983, Val Loss: 1.2087\n",
      "Epoch [778/1500], Train Loss: 1.2987, Val Loss: 1.2139\n",
      "Epoch [779/1500], Train Loss: 1.2968, Val Loss: 1.2004\n",
      "Epoch [780/1500], Train Loss: 1.2828, Val Loss: 1.2048\n",
      "Epoch [781/1500], Train Loss: 1.2933, Val Loss: 1.2354\n",
      "Epoch [782/1500], Train Loss: 1.2978, Val Loss: 1.2234\n",
      "Epoch [783/1500], Train Loss: 1.2879, Val Loss: 1.2390\n",
      "Epoch [784/1500], Train Loss: 1.2945, Val Loss: 1.2015\n",
      "Epoch [785/1500], Train Loss: 1.2845, Val Loss: 1.2208\n",
      "Epoch [786/1500], Train Loss: 1.2889, Val Loss: 1.2015\n",
      "Epoch [787/1500], Train Loss: 1.2806, Val Loss: 1.1995\n",
      "Epoch [788/1500], Train Loss: 1.2861, Val Loss: 1.2309\n",
      "Epoch [789/1500], Train Loss: 1.2996, Val Loss: 1.2247\n",
      "Epoch [790/1500], Train Loss: 1.2952, Val Loss: 1.2186\n",
      "Epoch [791/1500], Train Loss: 1.2904, Val Loss: 1.2071\n",
      "Epoch [792/1500], Train Loss: 1.2892, Val Loss: 1.1868\n",
      "Epoch [793/1500], Train Loss: 1.2849, Val Loss: 1.2032\n",
      "Epoch [794/1500], Train Loss: 1.2872, Val Loss: 1.1946\n",
      "Epoch [795/1500], Train Loss: 1.2868, Val Loss: 1.1943\n",
      "Epoch [796/1500], Train Loss: 1.2849, Val Loss: 1.2110\n",
      "Epoch [797/1500], Train Loss: 1.2891, Val Loss: 1.1957\n",
      "Epoch [798/1500], Train Loss: 1.2787, Val Loss: 1.2027\n",
      "Epoch [799/1500], Train Loss: 1.3023, Val Loss: 1.2121\n",
      "Epoch [800/1500], Train Loss: 1.2844, Val Loss: 1.2291\n",
      "Epoch [801/1500], Train Loss: 1.2901, Val Loss: 1.1921\n",
      "Epoch [802/1500], Train Loss: 1.2813, Val Loss: 1.2089\n",
      "Epoch [803/1500], Train Loss: 1.2906, Val Loss: 1.2087\n",
      "Epoch [804/1500], Train Loss: 1.2883, Val Loss: 1.2047\n",
      "Epoch [805/1500], Train Loss: 1.2914, Val Loss: 1.2122\n",
      "Epoch [806/1500], Train Loss: 1.2774, Val Loss: 1.1901\n",
      "Epoch [807/1500], Train Loss: 1.2942, Val Loss: 1.2295\n",
      "Epoch [808/1500], Train Loss: 1.2854, Val Loss: 1.2019\n",
      "Epoch [809/1500], Train Loss: 1.2932, Val Loss: 1.2150\n",
      "Epoch [810/1500], Train Loss: 1.2830, Val Loss: 1.1975\n",
      "Epoch [811/1500], Train Loss: 1.2783, Val Loss: 1.1992\n",
      "Epoch [812/1500], Train Loss: 1.2794, Val Loss: 1.1936\n",
      "Epoch [813/1500], Train Loss: 1.2796, Val Loss: 1.2153\n",
      "Epoch [814/1500], Train Loss: 1.2798, Val Loss: 1.1910\n",
      "Epoch [815/1500], Train Loss: 1.2844, Val Loss: 1.2078\n",
      "Epoch [816/1500], Train Loss: 1.2875, Val Loss: 1.1949\n",
      "Epoch [817/1500], Train Loss: 1.2868, Val Loss: 1.1938\n",
      "Epoch [818/1500], Train Loss: 1.2913, Val Loss: 1.2097\n",
      "Epoch [819/1500], Train Loss: 1.2942, Val Loss: 1.2007\n",
      "Epoch [820/1500], Train Loss: 1.2872, Val Loss: 1.2102\n",
      "Epoch [821/1500], Train Loss: 1.2837, Val Loss: 1.1793\n",
      "Epoch [822/1500], Train Loss: 1.2854, Val Loss: 1.1977\n",
      "Epoch [823/1500], Train Loss: 1.2806, Val Loss: 1.1931\n",
      "Epoch [824/1500], Train Loss: 1.2806, Val Loss: 1.1984\n",
      "Epoch [825/1500], Train Loss: 1.2867, Val Loss: 1.1998\n",
      "Epoch [826/1500], Train Loss: 1.2873, Val Loss: 1.1963\n",
      "Epoch [827/1500], Train Loss: 1.2653, Val Loss: 1.1943\n",
      "Epoch [828/1500], Train Loss: 1.2740, Val Loss: 1.2075\n",
      "Epoch [829/1500], Train Loss: 1.2868, Val Loss: 1.1981\n",
      "Epoch [830/1500], Train Loss: 1.2817, Val Loss: 1.2016\n",
      "Epoch [831/1500], Train Loss: 1.2788, Val Loss: 1.1916\n",
      "Epoch [832/1500], Train Loss: 1.2827, Val Loss: 1.1942\n",
      "Epoch [833/1500], Train Loss: 1.2809, Val Loss: 1.2066\n",
      "Epoch [834/1500], Train Loss: 1.2863, Val Loss: 1.1918\n",
      "Epoch [835/1500], Train Loss: 1.2742, Val Loss: 1.1985\n",
      "Epoch [836/1500], Train Loss: 1.2881, Val Loss: 1.1922\n",
      "Epoch [837/1500], Train Loss: 1.2816, Val Loss: 1.2274\n",
      "Epoch [838/1500], Train Loss: 1.2780, Val Loss: 1.1897\n",
      "Epoch [839/1500], Train Loss: 1.2828, Val Loss: 1.1924\n",
      "Epoch [840/1500], Train Loss: 1.2842, Val Loss: 1.1762\n",
      "Epoch [841/1500], Train Loss: 1.2786, Val Loss: 1.1973\n",
      "Epoch [842/1500], Train Loss: 1.2769, Val Loss: 1.1923\n",
      "Epoch [843/1500], Train Loss: 1.2784, Val Loss: 1.2026\n",
      "Epoch [844/1500], Train Loss: 1.2923, Val Loss: 1.2089\n",
      "Epoch [845/1500], Train Loss: 1.2844, Val Loss: 1.1957\n",
      "Epoch [846/1500], Train Loss: 1.2881, Val Loss: 1.1898\n",
      "Epoch [847/1500], Train Loss: 1.2880, Val Loss: 1.1972\n",
      "Epoch [848/1500], Train Loss: 1.2741, Val Loss: 1.2006\n",
      "Epoch [849/1500], Train Loss: 1.2796, Val Loss: 1.2006\n",
      "Epoch [850/1500], Train Loss: 1.2743, Val Loss: 1.1889\n",
      "Epoch [851/1500], Train Loss: 1.2776, Val Loss: 1.2214\n",
      "Epoch [852/1500], Train Loss: 1.2830, Val Loss: 1.2041\n",
      "Epoch [853/1500], Train Loss: 1.2837, Val Loss: 1.2001\n",
      "Epoch [854/1500], Train Loss: 1.2693, Val Loss: 1.2109\n",
      "Epoch [855/1500], Train Loss: 1.2642, Val Loss: 1.2070\n",
      "Epoch [856/1500], Train Loss: 1.2834, Val Loss: 1.1812\n",
      "Epoch [857/1500], Train Loss: 1.2628, Val Loss: 1.1900\n",
      "Epoch [858/1500], Train Loss: 1.2866, Val Loss: 1.2064\n",
      "Epoch [859/1500], Train Loss: 1.2681, Val Loss: 1.1822\n",
      "Epoch [860/1500], Train Loss: 1.2692, Val Loss: 1.1958\n",
      "Epoch [861/1500], Train Loss: 1.2633, Val Loss: 1.1855\n",
      "Epoch [862/1500], Train Loss: 1.2814, Val Loss: 1.1902\n",
      "Epoch [863/1500], Train Loss: 1.2794, Val Loss: 1.1869\n",
      "Epoch [864/1500], Train Loss: 1.2720, Val Loss: 1.2085\n",
      "Epoch [865/1500], Train Loss: 1.2730, Val Loss: 1.2044\n",
      "Epoch [866/1500], Train Loss: 1.2820, Val Loss: 1.1789\n",
      "Epoch [867/1500], Train Loss: 1.2691, Val Loss: 1.1825\n",
      "Epoch [868/1500], Train Loss: 1.2703, Val Loss: 1.1900\n",
      "Epoch [869/1500], Train Loss: 1.2728, Val Loss: 1.1844\n",
      "Epoch [870/1500], Train Loss: 1.2799, Val Loss: 1.1974\n",
      "Epoch [871/1500], Train Loss: 1.2679, Val Loss: 1.1810\n",
      "Epoch [872/1500], Train Loss: 1.2665, Val Loss: 1.1775\n",
      "Epoch [873/1500], Train Loss: 1.2731, Val Loss: 1.1770\n",
      "Epoch [874/1500], Train Loss: 1.2872, Val Loss: 1.1844\n",
      "Epoch [875/1500], Train Loss: 1.2729, Val Loss: 1.1865\n",
      "Epoch [876/1500], Train Loss: 1.2792, Val Loss: 1.1933\n",
      "Epoch [877/1500], Train Loss: 1.2721, Val Loss: 1.1657\n",
      "Epoch [878/1500], Train Loss: 1.2689, Val Loss: 1.1732\n",
      "Epoch [879/1500], Train Loss: 1.2671, Val Loss: 1.1874\n",
      "Epoch [880/1500], Train Loss: 1.2706, Val Loss: 1.1897\n",
      "Epoch [881/1500], Train Loss: 1.2718, Val Loss: 1.1866\n",
      "Epoch [882/1500], Train Loss: 1.2633, Val Loss: 1.1904\n",
      "Epoch [883/1500], Train Loss: 1.2749, Val Loss: 1.1796\n",
      "Epoch [884/1500], Train Loss: 1.2593, Val Loss: 1.1779\n",
      "Epoch [885/1500], Train Loss: 1.2651, Val Loss: 1.1841\n",
      "Epoch [886/1500], Train Loss: 1.2630, Val Loss: 1.1941\n",
      "Epoch [887/1500], Train Loss: 1.2664, Val Loss: 1.1751\n",
      "Epoch [888/1500], Train Loss: 1.2819, Val Loss: 1.2018\n",
      "Epoch [889/1500], Train Loss: 1.2748, Val Loss: 1.1876\n",
      "Epoch [890/1500], Train Loss: 1.2726, Val Loss: 1.2034\n",
      "Epoch [891/1500], Train Loss: 1.2651, Val Loss: 1.1870\n",
      "Epoch [892/1500], Train Loss: 1.2593, Val Loss: 1.1835\n",
      "Epoch [893/1500], Train Loss: 1.2652, Val Loss: 1.1803\n",
      "Epoch [894/1500], Train Loss: 1.2702, Val Loss: 1.2381\n",
      "Epoch [895/1500], Train Loss: 1.2769, Val Loss: 1.2162\n",
      "Epoch [896/1500], Train Loss: 1.2691, Val Loss: 1.1904\n",
      "Epoch [897/1500], Train Loss: 1.2734, Val Loss: 1.2122\n",
      "Epoch [898/1500], Train Loss: 1.2681, Val Loss: 1.1862\n",
      "Epoch [899/1500], Train Loss: 1.2674, Val Loss: 1.1841\n",
      "Epoch [900/1500], Train Loss: 1.2606, Val Loss: 1.1739\n",
      "Epoch [901/1500], Train Loss: 1.2692, Val Loss: 1.2063\n",
      "Epoch [902/1500], Train Loss: 1.2654, Val Loss: 1.1929\n",
      "Epoch [903/1500], Train Loss: 1.2691, Val Loss: 1.1861\n",
      "Epoch [904/1500], Train Loss: 1.2673, Val Loss: 1.1956\n",
      "Epoch [905/1500], Train Loss: 1.2582, Val Loss: 1.1931\n",
      "Epoch [906/1500], Train Loss: 1.2706, Val Loss: 1.1900\n",
      "Epoch [907/1500], Train Loss: 1.2656, Val Loss: 1.1966\n",
      "Epoch [908/1500], Train Loss: 1.2679, Val Loss: 1.2150\n",
      "Epoch [909/1500], Train Loss: 1.2687, Val Loss: 1.1886\n",
      "Epoch [910/1500], Train Loss: 1.2686, Val Loss: 1.2200\n",
      "Epoch [911/1500], Train Loss: 1.2681, Val Loss: 1.2197\n",
      "Epoch [912/1500], Train Loss: 1.2567, Val Loss: 1.1843\n",
      "Epoch [913/1500], Train Loss: 1.2653, Val Loss: 1.1821\n",
      "Epoch [914/1500], Train Loss: 1.2584, Val Loss: 1.1958\n",
      "Epoch [915/1500], Train Loss: 1.2696, Val Loss: 1.1938\n",
      "Epoch [916/1500], Train Loss: 1.2637, Val Loss: 1.1898\n",
      "Epoch [917/1500], Train Loss: 1.2709, Val Loss: 1.1827\n",
      "Epoch [918/1500], Train Loss: 1.2623, Val Loss: 1.2001\n",
      "Epoch [919/1500], Train Loss: 1.2610, Val Loss: 1.1821\n",
      "Epoch [920/1500], Train Loss: 1.2522, Val Loss: 1.1777\n",
      "Epoch [921/1500], Train Loss: 1.2659, Val Loss: 1.2005\n",
      "Epoch [922/1500], Train Loss: 1.2625, Val Loss: 1.1819\n",
      "Epoch [923/1500], Train Loss: 1.2661, Val Loss: 1.2088\n",
      "Epoch [924/1500], Train Loss: 1.2676, Val Loss: 1.2057\n",
      "Epoch [925/1500], Train Loss: 1.2635, Val Loss: 1.2001\n",
      "Epoch [926/1500], Train Loss: 1.2604, Val Loss: 1.1825\n",
      "Epoch [927/1500], Train Loss: 1.2628, Val Loss: 1.1923\n",
      "Epoch [928/1500], Train Loss: 1.2679, Val Loss: 1.1875\n",
      "Epoch [929/1500], Train Loss: 1.2685, Val Loss: 1.1821\n",
      "Epoch [930/1500], Train Loss: 1.2611, Val Loss: 1.1875\n",
      "Epoch [931/1500], Train Loss: 1.2779, Val Loss: 1.1802\n",
      "Epoch [932/1500], Train Loss: 1.2710, Val Loss: 1.1848\n",
      "Epoch [933/1500], Train Loss: 1.2771, Val Loss: 1.1924\n",
      "Epoch [934/1500], Train Loss: 1.2560, Val Loss: 1.2139\n",
      "Epoch [935/1500], Train Loss: 1.2657, Val Loss: 1.2448\n",
      "Epoch [936/1500], Train Loss: 1.2629, Val Loss: 1.1761\n",
      "Epoch [937/1500], Train Loss: 1.2619, Val Loss: 1.2031\n",
      "Epoch [938/1500], Train Loss: 1.2517, Val Loss: 1.1916\n",
      "Epoch [939/1500], Train Loss: 1.2611, Val Loss: 1.1856\n",
      "Epoch [940/1500], Train Loss: 1.2558, Val Loss: 1.2085\n",
      "Epoch [941/1500], Train Loss: 1.2659, Val Loss: 1.1764\n",
      "Epoch [942/1500], Train Loss: 1.2643, Val Loss: 1.1946\n",
      "Epoch [943/1500], Train Loss: 1.2562, Val Loss: 1.1802\n",
      "Epoch [944/1500], Train Loss: 1.2619, Val Loss: 1.1897\n",
      "Epoch [945/1500], Train Loss: 1.2617, Val Loss: 1.1855\n",
      "Epoch [946/1500], Train Loss: 1.2625, Val Loss: 1.1901\n",
      "Epoch [947/1500], Train Loss: 1.2639, Val Loss: 1.2065\n",
      "Epoch [948/1500], Train Loss: 1.2508, Val Loss: 1.1807\n",
      "Epoch [949/1500], Train Loss: 1.2639, Val Loss: 1.1682\n",
      "Epoch [950/1500], Train Loss: 1.2557, Val Loss: 1.2101\n",
      "Epoch [951/1500], Train Loss: 1.2540, Val Loss: 1.1854\n",
      "Epoch [952/1500], Train Loss: 1.2647, Val Loss: 1.1917\n",
      "Epoch [953/1500], Train Loss: 1.2574, Val Loss: 1.1685\n",
      "Epoch [954/1500], Train Loss: 1.2499, Val Loss: 1.1785\n",
      "Epoch [955/1500], Train Loss: 1.2595, Val Loss: 1.1821\n",
      "Epoch [956/1500], Train Loss: 1.2622, Val Loss: 1.1834\n",
      "Epoch [957/1500], Train Loss: 1.2587, Val Loss: 1.1882\n",
      "Epoch [958/1500], Train Loss: 1.2667, Val Loss: 1.2041\n",
      "Epoch [959/1500], Train Loss: 1.2579, Val Loss: 1.1713\n",
      "Epoch [960/1500], Train Loss: 1.2481, Val Loss: 1.1936\n",
      "Epoch [961/1500], Train Loss: 1.2525, Val Loss: 1.1719\n",
      "Epoch [962/1500], Train Loss: 1.2556, Val Loss: 1.1841\n",
      "Epoch [963/1500], Train Loss: 1.2627, Val Loss: 1.1872\n",
      "Epoch [964/1500], Train Loss: 1.2530, Val Loss: 1.1705\n",
      "Epoch [965/1500], Train Loss: 1.2612, Val Loss: 1.1782\n",
      "Epoch [966/1500], Train Loss: 1.2568, Val Loss: 1.1690\n",
      "Epoch [967/1500], Train Loss: 1.2556, Val Loss: 1.1766\n",
      "Epoch [968/1500], Train Loss: 1.2561, Val Loss: 1.1719\n",
      "Epoch [969/1500], Train Loss: 1.2663, Val Loss: 1.1836\n",
      "Epoch [970/1500], Train Loss: 1.2563, Val Loss: 1.1761\n",
      "Epoch [971/1500], Train Loss: 1.2410, Val Loss: 1.1650\n",
      "Epoch [972/1500], Train Loss: 1.2473, Val Loss: 1.1904\n",
      "Epoch [973/1500], Train Loss: 1.2605, Val Loss: 1.1696\n",
      "Epoch [974/1500], Train Loss: 1.2376, Val Loss: 1.1688\n",
      "Epoch [975/1500], Train Loss: 1.2511, Val Loss: 1.1663\n",
      "Epoch [976/1500], Train Loss: 1.2479, Val Loss: 1.1983\n",
      "Epoch [977/1500], Train Loss: 1.2556, Val Loss: 1.1707\n",
      "Epoch [978/1500], Train Loss: 1.2606, Val Loss: 1.1955\n",
      "Epoch [979/1500], Train Loss: 1.2468, Val Loss: 1.1792\n",
      "Epoch [980/1500], Train Loss: 1.2477, Val Loss: 1.1857\n",
      "Epoch [981/1500], Train Loss: 1.2519, Val Loss: 1.2007\n",
      "Epoch [982/1500], Train Loss: 1.2589, Val Loss: 1.1735\n",
      "Epoch [983/1500], Train Loss: 1.2513, Val Loss: 1.1648\n",
      "Epoch [984/1500], Train Loss: 1.2377, Val Loss: 1.1703\n",
      "Epoch [985/1500], Train Loss: 1.2514, Val Loss: 1.1748\n",
      "Epoch [986/1500], Train Loss: 1.2593, Val Loss: 1.1827\n",
      "Epoch [987/1500], Train Loss: 1.2472, Val Loss: 1.1834\n",
      "Epoch [988/1500], Train Loss: 1.2564, Val Loss: 1.1606\n",
      "Epoch [989/1500], Train Loss: 1.2498, Val Loss: 1.1777\n",
      "Epoch [990/1500], Train Loss: 1.2563, Val Loss: 1.1565\n",
      "Epoch [991/1500], Train Loss: 1.2443, Val Loss: 1.1761\n",
      "Epoch [992/1500], Train Loss: 1.2582, Val Loss: 1.1869\n",
      "Epoch [993/1500], Train Loss: 1.2458, Val Loss: 1.1614\n",
      "Epoch [994/1500], Train Loss: 1.2545, Val Loss: 1.1658\n",
      "Epoch [995/1500], Train Loss: 1.2506, Val Loss: 1.1644\n",
      "Epoch [996/1500], Train Loss: 1.2325, Val Loss: 1.1647\n",
      "Epoch [997/1500], Train Loss: 1.2501, Val Loss: 1.1691\n",
      "Epoch [998/1500], Train Loss: 1.2463, Val Loss: 1.1585\n",
      "Epoch [999/1500], Train Loss: 1.2629, Val Loss: 1.1816\n",
      "Epoch [1000/1500], Train Loss: 1.2438, Val Loss: 1.1712\n",
      "Epoch [1001/1500], Train Loss: 1.2348, Val Loss: 1.1738\n",
      "Epoch [1002/1500], Train Loss: 1.2480, Val Loss: 1.1776\n",
      "Epoch [1003/1500], Train Loss: 1.2478, Val Loss: 1.1741\n",
      "Epoch [1004/1500], Train Loss: 1.2499, Val Loss: 1.1602\n",
      "Epoch [1005/1500], Train Loss: 1.2429, Val Loss: 1.1917\n",
      "Epoch [1006/1500], Train Loss: 1.2537, Val Loss: 1.1661\n",
      "Epoch [1007/1500], Train Loss: 1.2368, Val Loss: 1.1587\n",
      "Epoch [1008/1500], Train Loss: 1.2467, Val Loss: 1.1557\n",
      "Epoch [1009/1500], Train Loss: 1.2376, Val Loss: 1.1544\n",
      "Epoch [1010/1500], Train Loss: 1.2458, Val Loss: 1.1520\n",
      "Epoch [1011/1500], Train Loss: 1.2489, Val Loss: 1.1549\n",
      "Epoch [1012/1500], Train Loss: 1.2454, Val Loss: 1.1751\n",
      "Epoch [1013/1500], Train Loss: 1.2397, Val Loss: 1.1861\n",
      "Epoch [1014/1500], Train Loss: 1.2522, Val Loss: 1.1572\n",
      "Epoch [1015/1500], Train Loss: 1.2440, Val Loss: 1.1755\n",
      "Epoch [1016/1500], Train Loss: 1.2441, Val Loss: 1.1656\n",
      "Epoch [1017/1500], Train Loss: 1.2425, Val Loss: 1.1607\n",
      "Epoch [1018/1500], Train Loss: 1.2438, Val Loss: 1.1719\n",
      "Epoch [1019/1500], Train Loss: 1.2471, Val Loss: 1.1624\n",
      "Epoch [1020/1500], Train Loss: 1.2524, Val Loss: 1.1785\n",
      "Epoch [1021/1500], Train Loss: 1.2435, Val Loss: 1.1723\n",
      "Epoch [1022/1500], Train Loss: 1.2469, Val Loss: 1.1631\n",
      "Epoch [1023/1500], Train Loss: 1.2305, Val Loss: 1.1718\n",
      "Epoch [1024/1500], Train Loss: 1.2526, Val Loss: 1.1633\n",
      "Epoch [1025/1500], Train Loss: 1.2376, Val Loss: 1.1705\n",
      "Epoch [1026/1500], Train Loss: 1.2377, Val Loss: 1.1653\n",
      "Epoch [1027/1500], Train Loss: 1.2351, Val Loss: 1.1529\n",
      "Epoch [1028/1500], Train Loss: 1.2447, Val Loss: 1.1733\n",
      "Epoch [1029/1500], Train Loss: 1.2421, Val Loss: 1.1631\n",
      "Epoch [1030/1500], Train Loss: 1.2486, Val Loss: 1.1883\n",
      "Epoch [1031/1500], Train Loss: 1.2371, Val Loss: 1.1708\n",
      "Epoch [1032/1500], Train Loss: 1.2458, Val Loss: 1.1512\n",
      "Epoch [1033/1500], Train Loss: 1.2432, Val Loss: 1.1858\n",
      "Epoch [1034/1500], Train Loss: 1.2467, Val Loss: 1.1820\n",
      "Epoch [1035/1500], Train Loss: 1.2482, Val Loss: 1.1724\n",
      "Epoch [1036/1500], Train Loss: 1.2383, Val Loss: 1.1782\n",
      "Epoch [1037/1500], Train Loss: 1.2405, Val Loss: 1.1497\n",
      "Epoch [1038/1500], Train Loss: 1.2331, Val Loss: 1.1615\n",
      "Epoch [1039/1500], Train Loss: 1.2415, Val Loss: 1.1454\n",
      "Epoch [1040/1500], Train Loss: 1.2425, Val Loss: 1.1817\n",
      "Epoch [1041/1500], Train Loss: 1.2387, Val Loss: 1.1512\n",
      "Epoch [1042/1500], Train Loss: 1.2326, Val Loss: 1.1504\n",
      "Epoch [1043/1500], Train Loss: 1.2244, Val Loss: 1.1616\n",
      "Epoch [1044/1500], Train Loss: 1.2472, Val Loss: 1.1732\n",
      "Epoch [1045/1500], Train Loss: 1.2370, Val Loss: 1.1736\n",
      "Epoch [1046/1500], Train Loss: 1.2427, Val Loss: 1.1687\n",
      "Epoch [1047/1500], Train Loss: 1.2368, Val Loss: 1.1540\n",
      "Epoch [1048/1500], Train Loss: 1.2373, Val Loss: 1.1629\n",
      "Epoch [1049/1500], Train Loss: 1.2386, Val Loss: 1.1826\n",
      "Epoch [1050/1500], Train Loss: 1.2452, Val Loss: 1.1661\n",
      "Epoch [1051/1500], Train Loss: 1.2446, Val Loss: 1.1418\n",
      "Epoch [1052/1500], Train Loss: 1.2298, Val Loss: 1.1624\n",
      "Epoch [1053/1500], Train Loss: 1.2305, Val Loss: 1.1693\n",
      "Epoch [1054/1500], Train Loss: 1.2239, Val Loss: 1.1583\n",
      "Epoch [1055/1500], Train Loss: 1.2231, Val Loss: 1.1419\n",
      "Epoch [1056/1500], Train Loss: 1.2512, Val Loss: 1.1688\n",
      "Epoch [1057/1500], Train Loss: 1.2362, Val Loss: 1.1690\n",
      "Epoch [1058/1500], Train Loss: 1.2414, Val Loss: 1.1648\n",
      "Epoch [1059/1500], Train Loss: 1.2409, Val Loss: 1.1655\n",
      "Epoch [1060/1500], Train Loss: 1.2382, Val Loss: 1.1545\n",
      "Epoch [1061/1500], Train Loss: 1.2424, Val Loss: 1.1533\n",
      "Epoch [1062/1500], Train Loss: 1.2325, Val Loss: 1.1511\n",
      "Epoch [1063/1500], Train Loss: 1.2444, Val Loss: 1.1580\n",
      "Epoch [1064/1500], Train Loss: 1.2404, Val Loss: 1.1844\n",
      "Epoch [1065/1500], Train Loss: 1.2459, Val Loss: 1.1736\n",
      "Epoch [1066/1500], Train Loss: 1.2310, Val Loss: 1.1808\n",
      "Epoch [1067/1500], Train Loss: 1.2265, Val Loss: 1.1749\n",
      "Epoch [1068/1500], Train Loss: 1.2551, Val Loss: 1.1685\n",
      "Epoch [1069/1500], Train Loss: 1.2398, Val Loss: 1.1833\n",
      "Epoch [1070/1500], Train Loss: 1.2399, Val Loss: 1.1698\n",
      "Epoch [1071/1500], Train Loss: 1.2266, Val Loss: 1.1548\n"
     ]
    }
   ],
   "source": [
    "model = model_pipeline(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rn-KrUz59yds"
   },
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 419,
     "status": "error",
     "timestamp": 1730838563074,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "AwFKsiy3LVKk",
    "outputId": "98ebf328-32d7-41f1-f466-f8cf6f031925"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e20ffa2db97a>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Assuming you have already initialized wandb with wandb.init()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Wandb model name: {run_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "Save the model\n",
    "torch.save(model.state_dict(), '/content/drive/MyDrive/PhD/Colab Notebooks/trained_models/lstm_model_xx_nothickness.pth')\n",
    "\n",
    "# # Save the model as onnx\n",
    "# torch.onnx.export(model, X_train, 'lstm_model.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ggQaw0lISGG"
   },
   "outputs": [],
   "source": [
    "def preprocess_test_data(data, data_percentage):\n",
    "    # Windowing the data\n",
    "    data = calculate_averages_and_dispersion(data, data_percentage)\n",
    "    print(data.shape)\n",
    "\n",
    "    # Assuming the last column is the target\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "\n",
    "    # Encode labeling of target data using presaved pkl file\n",
    "    # Load label encoder\n",
    "    label_encoder_path = '/content/drive/MyDrive/PhD/Colab Notebooks/label_encoder_2.pkl'\n",
    "    le = joblib.load(label_encoder_path)\n",
    "    y = le.transform(y)\n",
    "    print('y: ', y)\n",
    "\n",
    "    # # Standardize the features\n",
    "    # scaler = StandardScaler()\n",
    "    # X = scaler.fit_transform(X)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXfBY8sm1Z43"
   },
   "source": [
    "## Load New Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1507,
     "status": "ok",
     "timestamp": 1730838675175,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "ArtddUGY1Z43",
    "outputId": "ff7bcc70-117d-46c8-8c87-f8984fc3afce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['REF_1.csv', '.ipynb_checkpoints']\n",
      "      Sample  Frequency (GHz)    LG (mV)    HG (mV)  Thickness (mm)\n",
      "0        REF              100  32.718837  -0.854611               0\n",
      "1        REF              100  19.289465   0.122118               0\n",
      "2        REF              100  29.178366   0.244164               0\n",
      "3        REF              100  42.729825   0.854540               0\n",
      "4        REF              100  54.083751   0.366259               0\n",
      "...      ...              ...        ...        ...             ...\n",
      "63784    REF              600  -0.244170  33.207178               0\n",
      "63785    REF              600  -0.854596  41.753143               0\n",
      "63786    REF              600   0.854596  21.242827               0\n",
      "63787    REF              600   0.000000  36.747649               0\n",
      "63788    REF              600   0.244170  27.225003               0\n",
      "\n",
      "[63789 rows x 5 columns]\n",
      "(63789, 5)\n",
      "     Frequency (GHz)  LG (mV) mean  HG (mV) mean  LG (mV) std deviation  \\\n",
      "0                100     51.378981     -0.063946              20.119522   \n",
      "1                100     43.726854      0.056686              18.066434   \n",
      "2                100     44.811088      0.074118              18.031144   \n",
      "3                100     46.787705      0.005805              20.747336   \n",
      "4                100     46.867642     -0.040696              21.492982   \n",
      "..               ...           ...           ...                    ...   \n",
      "658              600      0.112186     33.592131               0.810680   \n",
      "659              600      0.103387     32.597852               0.896565   \n",
      "660              600      0.276067     33.182981               0.890577   \n",
      "661              600     -0.067092     33.749412               1.053275   \n",
      "662              600     -0.313933     35.073338               0.821568   \n",
      "\n",
      "     HG (mV) std deviation Sample  \n",
      "0                 0.738717    REF  \n",
      "1                 0.700541    REF  \n",
      "2                 0.717143    REF  \n",
      "3                 0.783414    REF  \n",
      "4                 0.716474    REF  \n",
      "..                     ...    ...  \n",
      "658               8.833808    REF  \n",
      "659               9.363379    REF  \n",
      "660               9.369229    REF  \n",
      "661               9.338285    REF  \n",
      "662               8.520847    REF  \n",
      "\n",
      "[663 rows x 6 columns]\n",
      "(663, 6)\n",
      "y:  [14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14]\n"
     ]
    }
   ],
   "source": [
    "# Load new data\n",
    "input_data_test = '/content/drive/MyDrive/PhD/Colab Notebooks/test_data/'\n",
    "\n",
    "print(os.listdir(input_data_test))\n",
    "\n",
    "data_test = load_data_from_directory(input_data_test)\n",
    "\n",
    "# Load and preprocess data\n",
    "X_test, y_test = preprocess_test_data(data_test, data_percentage=8.33) # 1s window size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faC6EfmDMsRL"
   },
   "source": [
    "## Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2180,
     "status": "ok",
     "timestamp": 1730838686594,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "nxNfBFvFowYp",
    "outputId": "fe857d74-7ad1-43c2-fda9-b80186635d8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "input_dim: 5, hidden_dim: 256, output_dim: 15\n",
      "Predicted labels: ['I1' 'J1' 'J1' 'H1' 'H1' 'REF' 'J1' 'H1' 'H1' 'H1' 'H1' 'J1' 'C1' 'J1'\n",
      " 'J1' 'K1' 'K1' 'K1' 'M1' 'M1' 'J1' 'K1' 'K1' 'K1' 'M1' 'K1' 'D1' 'D1'\n",
      " 'D1' 'D1' 'D1' 'I1' 'D1' 'M1' 'D1' 'D1' 'M1' 'D1' 'B1' 'L1' 'D1' 'B1'\n",
      " 'I1' 'D1' 'L1' 'D1' 'B1' 'D1' 'I1' 'L1' 'B1' 'L1' 'J1' 'J1' 'J1' 'J1'\n",
      " 'J1' 'J1' 'M1' 'J1' 'J1' 'F1' 'J1' 'J1' 'J1' 'L1' 'M1' 'L1' 'L1' 'L1'\n",
      " 'L1' 'M1' 'L1' 'L1' 'L1' 'L1' 'L1' 'M1' 'B1' 'B1' 'B1' 'B1' 'B1' 'B1'\n",
      " 'B1' 'B1' 'B1' 'B1' 'B1' 'B1' 'J1' 'H1' 'H1' 'H1' 'H1' 'L1' 'L1' 'H1'\n",
      " 'L1' 'H1' 'H1' 'H1' 'H1' 'H1' 'B1' 'M1' 'H1' 'N1' 'N1' 'H1' 'N1' 'N1'\n",
      " 'H1' 'H1' 'N1' 'N1' 'N1' 'REF' 'C1' 'C1' 'K1' 'C1' 'C1' 'C1' 'C1' 'C1'\n",
      " 'C1' 'C1' 'C1' 'K1' 'J1' 'J1' 'J1' 'J1' 'J1' 'J1' 'J1' 'J1' 'J1' 'J1'\n",
      " 'J1' 'J1' 'A1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'I1' 'M1'\n",
      " 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1'\n",
      " 'M1' 'D1' 'I1' 'I1' 'D1' 'A1' 'A1' 'K1' 'K1' 'L1' 'K1' 'K1' 'L1' 'I1'\n",
      " 'I1' 'I1' 'I1' 'I1' 'I1' 'I1' 'I1' 'I1' 'I1' 'I1' 'I1' 'I1' 'F1' 'D1'\n",
      " 'D1' 'D1' 'D1' 'D1' 'D1' 'E1' 'D1' 'D1' 'D1' 'L1' 'D1' 'D1' 'H1' 'H1'\n",
      " 'H1' 'REF' 'H1' 'REF' 'H1' 'H1' 'REF' 'H1' 'H1' 'H1' 'J1' 'J1' 'REF'\n",
      " 'REF' 'J1' 'REF' 'REF' 'REF' 'J1' 'REF' 'REF' 'J1' 'REF' 'REF' 'H1' 'REF'\n",
      " 'REF' 'H1' 'H1' 'N1' 'REF' 'REF' 'REF' 'N1' 'N1' 'REF' 'H1' 'D1' 'D1'\n",
      " 'D1' 'L1' 'L1' 'L1' 'M1' 'L1' 'M1' 'M1' 'M1' 'M1' 'M1' 'REF' 'H1' 'J1'\n",
      " 'J1' 'J1' 'H1' 'H1' 'H1' 'J1' 'J1' 'H1' 'H1' 'REF' 'REF' 'E1' 'E1' 'E1'\n",
      " 'REF' 'E1' 'REF' 'REF' 'REF' 'N1' 'E1' 'E1' 'E1' 'N1' 'REF' 'REF' 'REF'\n",
      " 'REF' 'REF' 'E1' 'REF' 'REF' 'REF' 'REF' 'N1' 'M1' 'REF' 'REF' 'REF'\n",
      " 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'H1' 'M1' 'M1' 'A1'\n",
      " 'L1' 'L1' 'L1' 'L1' 'A1' 'A1' 'L1' 'A1' 'A1' 'M1' 'I1' 'I1' 'C1' 'I1'\n",
      " 'I1' 'I1' 'I1' 'I1' 'B1' 'I1' 'I1' 'I1' 'G1' 'M1' 'M1' 'F1' 'F1' 'N1'\n",
      " 'F1' 'L1' 'F1' 'F1' 'H1' 'H1' 'H1' 'H1' 'N1' 'N1' 'N1' 'E1' 'N1' 'N1'\n",
      " 'E1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1'\n",
      " 'M1' 'M1' 'M1' 'N1' 'N1' 'M1' 'F1' 'F1' 'F1' 'F1' 'F1' 'F1' 'F1' 'F1'\n",
      " 'N1' 'N1' 'N1' 'N1' 'N1' 'F1' 'F1' 'F1' 'F1' 'F1' 'F1' 'F1' 'F1' 'F1'\n",
      " 'F1' 'F1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
      " 'N1' 'N1' 'N1' 'A1' 'F1' 'A1' 'A1' 'A1' 'F1' 'A1' 'F1' 'A1' 'A1' 'A1'\n",
      " 'A1' 'F1' 'A1' 'A1' 'B1' 'A1' 'A1' 'A1' 'A1' 'A1' 'A1' 'A1' 'A1' 'A1'\n",
      " 'B1' 'N1' 'A1' 'N1' 'A1' 'A1' 'N1' 'N1' 'N1' 'A1' 'A1' 'N1' 'N1' 'N1'\n",
      " 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF' 'REF'\n",
      " 'REF' 'N1' 'G1' 'G1' 'G1' 'N1' 'N1' 'N1' 'G1' 'N1' 'N1' 'N1' 'G1' 'N1'\n",
      " 'F1' 'F1' 'G1' 'F1' 'G1' 'M1' 'G1' 'F1' 'G1' 'I1' 'I1' 'F1' 'M1' 'N1'\n",
      " 'N1' 'G1' 'M1' 'N1' 'M1' 'N1' 'N1' 'N1' 'N1' 'N1' 'A1' 'N1' 'M1' 'M1'\n",
      " 'M1' 'M1' 'M1' 'REF' 'M1' 'M1' 'REF' 'REF' 'REF' 'M1' 'I1' 'M1' 'M1' 'I1'\n",
      " 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'I1' 'M1' 'M1' 'REF' 'M1'\n",
      " 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'REF' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
      " 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1'\n",
      " 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1'\n",
      " 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'M1' 'N1' 'N1' 'N1' 'M1' 'M1' 'M1' 'M1'\n",
      " 'M1' 'M1' 'N1' 'M1' 'M1' 'N1' 'M1' 'N1' 'N1' 'M1' 'M1' 'N1' 'N1' 'N1'\n",
      " 'N1' 'N1' 'M1' 'N1' 'N1' 'N1' 'M1' 'N1' 'M1' 'M1' 'M1' 'M1' 'M1' 'N1'\n",
      " 'M1' 'M1' 'M1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1' 'N1'\n",
      " 'N1' 'A1' 'G1' 'M1' 'N1' 'N1' 'M1' 'N1' 'N1' 'M1' 'M1' 'N1' 'M1' 'M1'\n",
      " 'A1' 'M1' 'M1' 'N1' 'N1' 'N1' 'N1' 'N1' 'M1' 'N1' 'N1' 'N1' 'M1' 'G1']\n",
      "Classes in label encoder: ['A1' 'B1' 'C1' 'D1' 'E1' 'F1' 'G1' 'H1' 'I1' 'J1' 'K1' 'L1' 'M1' 'N1'\n",
      " 'REF']\n",
      "Number of classes: 15\n",
      "Number of correct predictions: 67/663\n",
      "Accuracy: 10.11%\n",
      "Most repeated prediction: M1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A1       0.00      0.00      0.00         0\n",
      "          B1       0.00      0.00      0.00         0\n",
      "          C1       0.00      0.00      0.00         0\n",
      "          D1       0.00      0.00      0.00         0\n",
      "          E1       0.00      0.00      0.00         0\n",
      "          F1       0.00      0.00      0.00         0\n",
      "          G1       0.00      0.00      0.00         0\n",
      "          H1       0.00      0.00      0.00         0\n",
      "          I1       0.00      0.00      0.00         0\n",
      "          J1       0.00      0.00      0.00         0\n",
      "          K1       0.00      0.00      0.00         0\n",
      "          L1       0.00      0.00      0.00         0\n",
      "          M1       0.00      0.00      0.00         0\n",
      "          N1       0.00      0.00      0.00         0\n",
      "         REF       1.00      0.10      0.18       663\n",
      "\n",
      "    accuracy                           0.10       663\n",
      "   macro avg       0.07      0.01      0.01       663\n",
      "weighted avg       1.00      0.10      0.18       663\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-1c94cb395ec6>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
      "<ipython-input-31-1c94cb395ec6>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test).long().to(device)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "<ipython-input-31-1c94cb395ec6>:66: RuntimeWarning: invalid value encountered in divide\n",
      "  conf_matrix_normalized = np.nan_to_num(conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABFAAAAPfCAYAAADgxj2/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdeXxU9f3v8feEYJCEmQRkCUqRRZHNJID1Z9UKiiwWl7qiWAGrbVUQiiIGRUSFCLhTl2qRpQVRf60URWkVVES9ymICyqIICi5BfpDMJESykHP/oJkmEp0EMpmc83k97+M8HpmTMzPfl+H23vk8zpzjcxzHEQAAAAAAAH5UXKwXAAAAAAAA0NAxQAEAAAAAAIiAAQoAAAAAAEAEDFAAAAAAAAAiYIACAAAAAAAQAQMUAAAAAACACBigAAAAAAAARMAABQAAAAAAIAIGKAAAAAAAABEwQAEAIMo+++wzDRgwQIFAQD6fT4sXL67T1//iiy/k8/k0d+7cOn1dN+vbt6/69u0b62UAAAAPYYACADDh888/1+9//3t17NhRTZo0kd/v1+mnn65HH31U33//fVTfe/jw4dqwYYOmTp2qv/71r+rTp09U368+jRgxQj6fT36/v9r/jp999pl8Pp98Pp8eeOCBWr/+N998o7vvvlvZ2dl1sFoAAIDDFx/rBQAAEG1Lly7VZZddpoSEBF1zzTXq0aOHSkpKtGrVKo0fP16ffPKJnn766ai89/fff6/3339fd9xxh0aNGhWV92jfvr2+//57NW7cOCqvH0l8fLyKior08ssv6/LLL6/yuwULFqhJkybav3//Yb32N998oylTpuj4449Xenp6jZ/373//+7DeDwAA4McwQAEAeNr27ds1dOhQtW/fXitWrFBqamr4dzfddJO2bt2qpUuXRu39d+/eLUlKTk6O2nv4fD41adIkaq8fSUJCgk4//XQ999xzhwxQFi5cqF/96lf6+9//Xi9rKSoqUtOmTXXUUUfVy/sBAAA7+AoPAMDTZsyYocLCQs2ePbvK8KRC586dNWbMmPDjsrIy3XvvverUqZMSEhJ0/PHHa+LEiSouLq7yvOOPP15DhgzRqlWr9POf/1xNmjRRx44dNX/+/PAxd999t9q3by9JGj9+vHw+n44//nhJB7/6UvFzZXfffbd8Pl+Vfa+//rrOOOMMJScnKykpSV26dNHEiRPDv/+xa6CsWLFCZ555phITE5WcnKwLL7xQmzZtqvb9tm7dqhEjRig5OVmBQEAjR45UUVHRj/+H/YGrrrpKr732mvLz88P7Vq9erc8++0xXXXXVIcfv3btXt956q3r27KmkpCT5/X4NHjxYOTk54WPeeustnXLKKZKkkSNHhr8KVNHZt29f9ejRQ2vXrtUvf/lLNW3aNPzf5YfXQBk+fLiaNGlySP/AgQOVkpKib775psatAADAJgYoAABPe/nll9WxY0f94he/qNHx1113ne666y716tVLDz/8sM466yxlZWVp6NChhxy7detWXXrppTr33HP14IMPKiUlRSNGjNAnn3wiSbr44ov18MMPS5KuvPJK/fWvf9UjjzxSq/V/8sknGjJkiIqLi3XPPffowQcf1AUXXKB33333J5/3xhtvaODAgfruu+909913a9y4cXrvvfd0+umn64svvjjk+Msvv1wFBQXKysrS5Zdfrrlz52rKlCk1XufFF18sn8+nf/zjH+F9Cxcu1EknnaRevXodcvy2bdu0ePFiDRkyRA899JDGjx+vDRs26KyzzgoPM7p27ap77rlHkvS73/1Of/3rX/XXv/5Vv/zlL8Ovs2fPHg0ePFjp6el65JFH1K9fv2rX9+ijj6ply5YaPny4Dhw4IEn685//rH//+9+aNWuW2rZtW+NWAABglAMAgEcFg0FHknPhhRfW6Pjs7GxHknPddddV2X/rrbc6kpwVK1aE97Vv396R5KxcuTK877vvvnMSEhKcW265Jbxv+/btjiRn5syZVV5z+PDhTvv27Q9Zw+TJk53K/8/zww8/7Ehydu/e/aPrrniPOXPmhPelp6c7rVq1cvbs2RPel5OT48TFxTnXXHPNIe937bXXVnnNX//6106LFi1+9D0rdyQmJjqO4ziXXnqpc8455ziO4zgHDhxw2rRp40yZMqXa/wb79+93Dhw4cEhHQkKCc88994T3rV69+pC2CmeddZYjyXnqqaeq/d1ZZ51VZd+//vUvR5Jz3333Odu2bXOSkpKciy66KGIjAACA4zgOZ6AAADwrFApJkpo1a1aj41999VVJ0rhx46rsv+WWWyTpkGuldOvWTWeeeWb4ccuWLdWlSxdt27btsNf8QxXXTvnnP/+p8vLyGj3n22+/VXZ2tkaMGKHmzZuH95988sk699xzw52V/eEPf6jy+Mwzz9SePXvC/w1r4qqrrtJbb72l3NxcrVixQrm5udV+fUc6eN2UuLiD/9+QAwcOaM+ePeGvJ61bt67G75mQkKCRI0fW6NgBAwbo97//ve655x5dfPHFatKkif785z/X+L0AAIBtDFAAAJ7l9/slSQUFBTU6/ssvv1RcXJw6d+5cZX+bNm2UnJysL7/8ssr+n/3sZ4e8RkpKivLy8g5zxYe64oordPrpp+u6665T69atNXToUL3wwgs/OUypWGeXLl0O+V3Xrl31f//3f9q3b1+V/T9sSUlJkaRatZx33nlq1qyZnn/+eS1YsECnnHLKIf8tK5SXl+vhhx/WCSecoISEBB1zzDFq2bKl1q9fr2AwWOP3PPbYY2t1wdgHHnhAzZs3V3Z2th577DG1atWqxs8FAAC2cRceAIBn+f1+tW3bVh9//HGtnvfDi7j+mEaNGlW733Gcw36PiutzVDj66KO1cuVKvfnmm1q6dKmWLVum559/Xmeffbb+/e9//+gaautIWiokJCTo4osv1rx587Rt2zbdfffdP3rstGnTNGnSJF177bW699571bx5c8XFxWns2LE1PtNGOvjfpzY++ugjfffdd5KkDRs26Morr6zV8wEAaGj279+vkpKSWC+j1o466qiY3kXwcDBAAQB42pAhQ/T000/r/fff12mnnfaTx7Zv317l5eX67LPP1LVr1/D+Xbt2KT8/P3xHnbqQkpJS5Y41FX54loskxcXF6ZxzztE555yjhx56SNOmTdMdd9yhN998U/3796+2Q5K2bNlyyO82b96sY445RomJiUceUY2rrrpKzz77rOLi4qq98G6F//3f/1W/fv00e/bsKvvz8/N1zDHHhB/XdJhVE/v27dPIkSPVrVs3/eIXv9CMGTP061//OnynHwAA3Gb//v06ulkLqazmd85rKNq0aaPt27e7aojCAAUA4Gm33XabFixYoOuuu04rVqxQ69atq/z+888/1yuvvKIxY8bovPPO08SJE/XII49UuTbGQw89JEn61a9+VWfr6tSpk4LBoNavX6+TTz5Z0sFrl7z00ktVjtu7d2+V65hIUnp6uiQdcmvlCqmpqUpPT9e8efOUmZkZvo7Kxx9/rH//+9+6+uqr66zjh/r166d7771XLVq0UJs2bX70uEaNGh1ydsuLL76or7/+usrXfioGPdUNm2prwoQJ2rFjh/7f//t/6tKli5YvX67hw4fro48+UkJCwhG/PgAA9a2kpEQqK1JCt+FSo5p/pTXmDpQod+M8lZSUMEABAKCh6NSpkxYuXKgrrrhCXbt21TXXXKMePXqopKRE7733nl588UWNGDFCkpSWlqbhw4fr6aefVn5+vs466yx9+OGHmjdvni666KIfvUXu4Rg6dKgmTJigX//617r55ptVVFSkJ598UieeeGKVi6jec889WrlypX71q1+pffv2+u677/TEE0/ouOOO0xlnnPGjrz9z5kwNHjxYp512mn7729/q+++/16xZsxQIBH7yqzVHKi4uTnfeeWfE44YMGaJ77rlHI0eO1C9+8Qtt2LBBCxYsUMeOHasc16lTJyUnJ+upp55Ss2bNlJiYqFNPPVUdOnSo1bpWrFihJ554QpMnTw7fVnnOnDnq27evJk2apBkzZtTq9QAAaFAaHSWfiwYoNf+CcMPCAAUA4HkXXHCB1q9fr5kzZ+qf//ynnnzySSUkJOjkk0/Wgw8+qOuvvz587F/+8hd17NhRc+fO1UsvvaQ2bdooMzNTkydPrtM1tWjRQi+99JLGjRun2267TR06dFBWVpY+++yzKgOUCy64QF988YWeffZZ/d///Z+OOeYYnXXWWZoyZYoCgcCPvn7//v21bNkyTZ48WXfddZcaN26ss846S9OnT6/18CEaJk6cqH379mnhwoV6/vnn1atXLy1dulS33357leMaN24cPpPmD3/4g8rKyjRnzpxaNRQUFOjaa69VRkaG7rjjjvD+M888U2PGjNGDDz6oiy++WP/zP/9TZ30AANQrX9zBzS3ctNZKfE5trg4HAAAAAAAahFAopEAgoISTf++uM1AOlKh4/Z8VDAbDd010A3eOfQAAAAAAAOoRAxQAAAAAAIAIuAYKAAAAAABu5pPk88V6FTXnoqVWxhkoAAAAAAAAETBAAQAAAAAAiICv8AAAAAAA4GbcxrheMECRVF5erm+++UbNmjWTz03fGwMAAAAA/CjHcVRQUKC2bdsqLs6dH9rRcDBAkfTNN9+oXbt2sV4GAAAAACAKdu7cqeOOOy7Wy4DLMUCR1KxZM0nS1u071czvj/FqAAAAAAB1oSAUUucO7cKf+YAjwQBFCn9tp5nfLz8DFAAAAADwFM9fqsHnc9ltjF201kr4EhgAAAAAAEAEDFAAAAAAAAAi4Cs8AAAAAAC4GbcxrhfuXDUAAAAAAEA9YoACAAAAAAAQAQMUAAAAAACACLgGCgAAAAAAbsZtjOsFZ6AAAAAAAABEwAAFAAAAAAAgAgYoAAAAAAAAEXANFAAAAAAAXC1O8rnp/Ag3rfW/3LlqAAAAAACAesQABQAAAAAAIAK+wgMAAAAAgJtxG+N6wRkoMVBaWqqxN49SassUtW3VXH8cM1plZWWxXlbUWOq11CrZ6qXVm62SrV5LrZKtXkutkq1eWr3ZKtnqtdQKb2OAEgP3T7tP77+7SuvWb9TanE/03qp3NOP+abFeVtRY6rXUKtnqpdWbrZKtXkutkq1eS62SrV5avdkq2eq11ApvY4ASA/PmPqsJE+9UamqqUlNTdVvmHZo7Z3aslxU1lnottUq2emn1Zqtkq9dSq2Sr11KrZKuXVm+2SrZ6LbXC2xig1LO8vDx9/dVXSktLD+9LS0vXzh07FAwGY7ewKLHUa6lVstVLqzdbJVu9llolW72WWiVbvbR6s1Wy1WupNaZ8ce7bXMidq3axwsJCSVIgOTm8r+LngoKCGKwouiz1WmqVbPXSevBnr7VKtnottUq2ei21SrZ6aT34s9daJVu9llrhfQxQ6llSUpIkKVRp2lrxc7NmzWKypmiy1GupVbLVS6s3WyVbvZZaJVu9llolW720erNVstVrqRXexwClnqWkpOjY445TTk52eF9OTraOa9dOgUAgdguLEku9llolW720erNVstVrqVWy1WupVbLVS6s3WyVbvZZaY6riNsZu2lyIAUoMXDN8pGZkTVVubq5yc3M18/5pGnntdbFeVtRY6rXUKtnqpdWbrZKtXkutkq1eS62SrV5avdkq2eq11Apvi4/1AizKvGOS9u7Zo4yeXSVJQ6+6WrfdPjHGq4oeS72WWiVbvbR6s1Wy1WupVbLVa6lVstVLqzdbJVu9llrhbT7HcZxYLyKS999/X2eccYYGDRqkpUuXVvndzTffrHfffVcff/yxunbtquzs7Fq/figUUiAQ0K49Qfn9/jpaNQAAAAAglkKhkFq3CCgY9OZnvYrPsgk/v0W++IRYL6fGnLJiFX/4oOv+Lq74Cs/s2bM1evRorVy5Ut98880hv7/22mt1xRVXxGBlAAAAAADEWKxvSWzkNsYN/is8hYWFev7557VmzRrl5uZq7ty5mjjxv6d7PfbYY5Kk3bt3a/369TV6zeLiYhUXF4cfh0Khul00AAAAAADwlAY/9nnhhRd00kknqUuXLrr66qv17LPP6ki/dZSVlaVAIBDe2rVrV0erBQAAAAAAXtTgByizZ8/W1VdfLUkaNGiQgsGg3n777SN6zczMTAWDwfC2c+fOulgqAAAAAAD1L9a3JOY2xrG3ZcsWffjhh7ryyislSfHx8briiis0e/bsI3rdhIQE+f3+KlttvfD8Ig278vIjWkddOXDggPqk99TmTZui9h6Wei21SrZ6LbVKtnottUq2emmNDf4d1y1LrZKtXkutkr1e4Ica9ABl9uzZKisrU9u2bRUfH6/4+Hg9+eST+vvf/65gMBizdZWXl2vypInKnDgpvC8UCmn4b65Sq+Z+tT+2tbKm3vuTrxHp+MwJ49W2VXP9vFeaNm3cGN6/fds2ndo7Xfv37w/va9SokcaOu1V3TYrOrcAs9VpqlWz1WmqVbPVaapVs9dLqzVbJVq+lVslWr6VWyV4vUC2ngSotLXVat27tPPjgg86GDRuqbJ06dXKefPLJKsdPnjzZSUtLO6z3CgaDjiRn156g832pE3H7++KXnV69elfZN+zqa5xzBwx0vt2d56z/ZItzXLt2zl+enfejr/FTx7/z3odOp86dnV17gs4DDz3qnPerIeHnnTtgoPPav5cf8nr/l1/oNGvWzNny+Zc1aqjNZqnXUqu1Xkut1nottVrrpdWbrdZ6LbVa67XU6ubeXXsOftYLBoN18Cm14an4LJvwP7c5Tc6Y5Jot4X9uc+XfpcGegfLKK68oLy9Pv/3tb9WjR48q2yWXXBL+Gs/WrVuVnZ2t3Nxcff/998rOzlZ2drZKSkqitralLy/RWf3ODj8uKirSiy8s0uQp9yk5OVknnHiibrhptObOqf6rRpGO3759m3r17iO/36/+5w7Qtm2fS5IWPbdQrVu3Ud9K710hMTFRvfucomWvLqWXVnqNt1rrtdRqrZdWb7Za67XUaq3XUqvFXteJ9S2JjdzGuMGuevbs2erfv78CgcAhv7vkkku0Zs0arV+/Xtddd50yMjL05z//WZ9++qkyMjKUkZGhb775Jmpry8nJVpcuJ4Uff7pli0pKSpSWnh7el5aWro83VH9b5UjHd+/eQ+vWrlF+fr5WLH9D3Xv0VF5enmZOn6b7Zz74o+s6qWs3rc/JPqK26ljqtdQq2eq11CrZ6rXUKtnqpdWbrZKtXkutkq1eS62SvV6gOg12gPLyyy9r6dLqJ4k///nP5TiOTj75ZL311ltyHOeQ7fjjj4/a2vLz89Ss0oVnC/cVKjExUfHx8eF9gUCyCgoKqn1+pOO7de+um0aN0cBz+uqN1/+lrOkPaOKE8brl1gnatGmjBp17tgYPOEfvrlpV5XX9fr/y8vPqMlWSrV5LrZKtXkutkq1eS62SrV5avdkq2eq11CrZ6rXUKtnrBaoTH/kQ/FBycooKQqHw46TEJBUVFamsrCz8PwihUFDNmjWr9vk1Of6Gm0bphptGSZJWvbNSO3fu0NCrhunETu31+vK35TiOBg04W1u2fiHff24BFQqFlJKcQi+t9BpvtdZrqdVaL63ebLXWa6nVWq+lVou9QHUa7BkoDVlaWrq2bNkcfnxily5q3Lix1ufkhPfl5GSre4+e1T6/NseXlJRo/LixenTWE9q9e7cOlJWpQ8eO6tipk0pLSrR79+7wsZs3bdTJael1UFiVpV5LrZKtXkutkq1eS62SrV5avdkq2eq11CrZ6rXUKtnrdR2fL/bXNKnV5ov1f7HDwgDlMJw35Hy9/dab4cdNmzbVpZddoXvunqRgMKitn32mJx+fpZHXXlft82tz/MzpWbr40svUqXNnHXPMMSouLtb6nBxtWL9eJSUlatGihaSDF2Vau2a1Bg4+j15a6TXeaq3XUqu1Xlq92Wqt11KrtV5LrRZ7gWrV6z1/Gqja3sa4cH+Z0/744501H22ocnusy64Y6iQlJTktW7Z0Jk2eUuU5AwYOcqbcO7XGx39f6jg5H292evXq7YSKSsL75i9Y5LRJTXVS27Z1Fix6Mbx/9pz5zpDzL6jz25VZ67XUaq3XUqu1Xkut1npp9WartV5LrdZ6LbW6udfMbYx/kek0+eUU12wJv8h05d/F5ziOE9sRTuyFQiEFAgHt2hOUv9KFkX7K84ue08tLFutvC5+P8uoiKy8v16m90zV/wSJ17dYtKu9hqddSq2Sr11KrZKvXUqtkq5fW2ODfcd2y1CrZ6rXUKrmzNxQKqXWLgILBmn/Wc5OKz7IJZ0yUL75JrJdTY07ZfhWvmua6vwsDFB3eAAUAAAAA0LAxQGmY3DpA4RooAAAAAAAAETBAAQAAAAAAiCA+1gsAAAAAAABHoOL2wG7hprVW4s5VAwAAAAAA1CMGKAAAAAAAABHwFR4AAAAAANzM5zu4uYWb1loJZ6AAAAAAAABEwAAFAAAAAAA0WCtXrtT555+vtm3byufzafHixT967B/+8Af5fD498sgjVfbv3btXw4YNk9/vV3Jysn7729+qsLCwVutggAIAAAAAABqsffv2KS0tTY8//vhPHvfSSy/p//2//6e2bdse8rthw4bpk08+0euvv65XXnlFK1eu1O9+97tarYNroAAAAAAA4GYev43x4MGDNXjw4J885uuvv9bo0aP1r3/9S7/61a+q/G7Tpk1atmyZVq9erT59+kiSZs2apfPOO08PPPBAtQOX6rjovzAAAAAAAPCKUChUZSsuLj6s1ykvL9dvfvMbjR8/Xt27dz/k9++//76Sk5PDwxNJ6t+/v+Li4vTBBx/U+H0YoAAAAAAAgHrXrl07BQKB8JaVlXVYrzN9+nTFx8fr5ptvrvb3ubm5atWqVZV98fHxat68uXJzc2v8PnyFBwAAAAAAN3PpbYx37twpv98f3p2QkFDrl1q7dq0effRRrVu3Tr4o/zfgDBQAAAAAAFDv/H5/le1wBijvvPOOvvvuO/3sZz9TfHy84uPj9eWXX+qWW27R8ccfL0lq06aNvvvuuyrPKysr0969e9WmTZsavxdnoAAAAAAAAFf6zW9+o/79+1fZN3DgQP3mN7/RyJEjJUmnnXaa8vPztXbtWvXu3VuStGLFCpWXl+vUU0+t8XsxQAEAAAAAAA1WYWGhtm7dGn68fft2ZWdnq3nz5vrZz36mFi1aVDm+cePGatOmjbp06SJJ6tq1qwYNGqTrr79eTz31lEpLSzVq1CgNHTq0xnfgkRigAAAAAADgbh6/jfGaNWvUr1+/8ONx48ZJkoYPH665c+fW6DUWLFigUaNG6ZxzzlFcXJwuueQSPfbYY7VaBwMUAAAAAADQYPXt21eO49T4+C+++OKQfc2bN9fChQuPaB0uGlEBAAAAAADEBgMUAAAAAACACPgKDwAAAAAAbubzHdzcwk1rrYQzUAAAAAAAACJggAIAAAAAABABX+EBAAAAAMDNPH4b44bCnasGAAAAAACoRwxQAAAAAAAAImCAAgAAAAAAEAHXQAEAAAAAwM24jXG94AwUAAAAAACACBigAAAAAAAARMBXeAAAAAAAcDWX3cbYpedyuHPVAAAAAAAA9YgBCgAAAAAAQAQMUGKgtLRUY28epdSWKWrbqrn+OGa0ysrKYr2sqLHUa6lVstVLqzdbJVu9llolW72WWiVbvbR6s1Wy1WupFd7GACUG7p92n95/d5XWrd+otTmf6L1V72jG/dNivayosdRrqVWy1UurN1slW72WWiVbvZZaJVu9tHqzVbLVa6k1ZipuY+ymzYUYoMTAvLnPasLEO5WamqrU1FTdlnmH5s6ZHetlRY2lXkutkq1eWr3ZKtnqtdQq2eq11CrZ6qXVm62SrV5LrfA2Bij1LC8vT19/9ZXS0tLD+9LS0rVzxw4Fg8HYLSxKLPVaapVs9dLqzVbJVq+lVslWr6VWyVYvrd5slWz1WmqF9zFAqWeFhYWSpEBycnhfxc8FBQUxWFF0Weq11CrZ6qX14M9ea5Vs9VpqlWz1WmqVbPXSevBnr7VKtnottcL7GKDUs6SkJElSqNK0teLnZs2axWRN0WSp11KrZKuXVm+2SrZ6LbVKtnottUq2emn1Zqtkq9dSa0z5fJIvzkUb10BBDaSkpOjY445TTk52eF9OTraOa9dOgUAgdguLEku9llolW720erNVstVrqVWy1WupVbLVS6s3WyVbvZZa4X0MUGLgmuEjNSNrqnJzc5Wbm6uZ90/TyGuvi/WyosZSr6VWyVYvrd5slWz1WmqVbPVaapVs9dLqzVbJVq+lVnhbfKwXYFHmHZO0d88eZfTsKkkaetXVuu32iTFeVfRY6rXUKtnqpdWbrZKtXkutkq1eS62SrV5avdkq2eq11BozFV+NcQs3rbUSn+M4TqwX8WNGjBihefPmhR83b95cp5xyimbMmKGTTz5ZkjR16lQtXbpU2dnZOuqoo5Sfn1/r9wmFQgoEAtq1Jyi/319XywcAAAAAxFAoFFLrFgEFg978rFfxWTZh4APyNT461supMaf0exX/61bX/V0a/Nhn0KBB+vbbb/Xtt99q+fLlio+P15AhQ8K/Lykp0WWXXaYbbrghhqsEAAAAAABe1uC/wpOQkKA2bdpIktq0aaPbb79dZ555pnbv3q2WLVtqypQpkqS5c+fGcJUAAAAAAMDLGvwApbLCwkL97W9/U+fOndWiRYvDfp3i4mIVFxeHH4dCobpYHgAAAAAA9c/nc9etgd201koa/ADllVdeCd87fN++fUpNTdUrr7yiuLjD//ZRVlZW+MwVAAAAAACASBr8NVD69eun7OxsZWdn68MPP9TAgQM1ePBgffnll4f9mpmZmQoGg+Ft586dtX6NF55fpGFXXn7Ya6hLBw4cUJ/0ntq8aVPU3sNSr6VWyVavpVbJVq+lVslWL62xwb/jumWpVbLVa6lVstcL/FCDH6AkJiaqc+fO6ty5s0455RT95S9/0b59+/TMM88c9msmJCTI7/dX2WqjvLxckydNVObESeF9oVBIw39zlVo196v9sa2VNfXen3yNSMdnThivtq2a6+e90rRp48bw/u3btunU3unav39/eF+jRo00dtytumtSdG4FZqnXUqtkq9dSq2Sr11KrZKuXVm+2SrZ6LbVKtnottUr2el2n4jbGbtpcyHWr9vl8iouL0/fffx+zNSx77VU1T2muHj17hveNGzNaeXv36tNtO/TGm+/o2dnPaMFf5//oa/zU8WtWr9bLSxZr89YvNHzkb3XnxAnh540ZfaOmP/CQmjRpUuX1fn3JpXprxXLt2LGjjmtt9VpqlWz1WmqVbPVaapVs9dLqzVbJVq+lVslWr6VWyV4vUJ0GP0ApLi5Wbm6ucnNztWnTJo0ePVqFhYU6//zzJUk7duxQdna2duzYoQMHDoS/7lNYWBi1NS19eYnO6nd2+HFRUZFefGGRJk+5T8nJyTrhxBN1w02jNXfO7GqfH+n47du3qVfvPvL7/ep/7gBt2/a5JGnRcwvVunUb9a303hUSExPVu88pWvbqUnpppdd4q7VeS63Wemn1Zqu1Xkut1nottVrsBarT4Acoy5YtU2pqqlJTU3Xqqadq9erVevHFF9W3b19J0l133aWMjAxNnjxZhYWFysjIUEZGhtasWRO1NeXkZKtLl5PCjz/dskUlJSVKS08P70tLS9fHG9ZX+/xIx3fv3kPr1q5Rfn6+Vix/Q9179FReXp5mTp+m+2c++KPrOqlrN63PyT6itupY6rXUKtnqtdQq2eq11CrZ6qXVm62SrV5LrZKtXkutkr1eoDoNeoAyd+5cOY4T3kKhkD788ENdcsklP3pMxVYxYImG/Pw8Nat03ZTCfYVKTExUfPx/b2oUCCSroKCg2udHOr5b9+66adQYDTynr954/V/Kmv6AJk4Yr1tunaBNmzZq0Llna/CAc/TuqlVVXtfv9ysvP68uUyXZ6rXUKtnqtdQq2eq11CrZ6qXVm62SrV5LrZKtXkutkr1e16m4jbGbNhdq8LcxboiSk1NUEAqFHyclJqmoqEhlZWXh/0EIhYJq1qxZtc+vyfE33DRKN9w0SpK06p2V2rlzh4ZeNUwndmqv15e/LcdxNGjA2dqy9Qv5/vOPLxQKKSU5hV5a6TXeaq3XUqu1Xlq92Wqt11KrtV5LrRZ7geo06DNQGqq0tHRt2bI5/PjELl3UuHFjrc/JCe/LyclW9x49q3t6rY4vKSnR+HFj9eisJ7R7924dKCtTh44d1bFTJ5WWlGj37t3hYzdv2qiT09LroLAqS72WWiVbvZZaJVu9llolW720erNVstVrqVWy1WupVbLXC1SHAcphOG/I+Xr7rTfDj5s2bapLL7tC99w9ScFgUFs/+0xPPj5LI6+9rtrn1+b4mdOzdPGll6lT58465phjVFxcrPU5Odqwfr1KSkrUokULSQcvyrR2zWoNHHwevbTSa7zVWq+lVmu9tHqz1VqvpVZrvZZaLfa6TqxvSWzkNsZy4ASDQUeSs2tP0Pm+1Im4Fe4vc9off7yz5qMN4X279gSdy64Y6iQlJTktW7Z0Jk2eUuU5AwYOcqbcO7XGx39f6jg5H292evXq7YSKSsL75i9Y5LRJTXVS27Z1Fix6Mbx/9pz5zpDzL6jR+mu7Weq11Gqt11KrtV5LrdZ6afVmq7VeS63Wei21url3156Dn/WCwWCsP3ZGRcVn2YRfPeY0uegZ12wJv3rMlX8Xn+M4TmxHOLEXCoUUCAS0a09Q/koXRvopzy96Ti8vWay/LXw+yquLrLy8XKf2Ttf8BYvUtVu3qLyHpV5LrZKtXkutkq1eS62SrV5aY4N/x3XLUqtkq9dSq+TO3lAopNYtAgoGa/5Zz00qPssm/Oox+RofHevl1JhT+r2Kl97sur8LAxQd3gAFAAAAANCwMUBpmNw6QOEuPAAAAAAAuJnbbg3sprVW4tIrtwAAAAAAANQfBigAAAAAAAARMEABAAAAAACIgGugAAAAAADgYj6fTz43XVfETWuthDNQAAAAAAAAImCAAgAAAAAAEAFf4QEAAAAAwMX4Ck/94AwUAAAAAACACBigAAAAAAAARMAABQAAAAAAIAKugQIAAAAAgJv5/rO5hZvWWglnoAAAAAAAAETAAAUAAAAAACACvsIDAAAAAICLcRvj+sEZKAAAAAAAABEwQAEAAAAAAIiAAQoAAAAAAEAEXAMFAAAAAAAX4xoo9YMzUAAAAAAAACJggAIAAAAAABABX+EBAAAAAMDF+ApP/eAMFAAAAAAAgAgYoAAAAAAAAETAAAUAAAAAACACroECAAAAAICLcQ2U+sEZKAAAAAAAABEwQAEAAAAAAIiAAQoAAAAAAEAEXAMFAAAAAAA38/1ncws3rbUSzkABAAAAAACIgAEKAAAAAABABHyFBwAAAAAAF+M2xvWDM1AAAAAAAAAiYIASA6WlpRp78yiltkxR21bN9ccxo1VWVhbrZUWNpV5LrZKtXlq92SrZ6rXUKtnqtdQq2eql1Zutkq1eS63wNgYoMXD/tPv0/rurtG79Rq3N+UTvrXpHM+6fFutlRY2lXkutkq1eWr3ZKtnqtdQq2eq11CrZ6qXVm62SrV5LrfA2BigxMG/us5ow8U6lpqYqNTVVt2XeoblzZsd6WVFjqddSq2Srl1Zvtkq2ei21SrZ6LbVKtnpp9WarZKvXUmus+Hz/vQ6KO7ZY/xc7PAxQ6lleXp6+/uorpaWlh/elpaVr544dCgaDsVtYlFjqtdQq2eql1Zutkq1eS62SrV5LrZKtXlq92SrZ6rXUCu9jgFLPCgsLJUmB5OTwvoqfCwoKYrCi6LLUa6lVstVL68GfvdYq2eq11CrZ6rXUKtnqpfXgz15rlWz1WmqF9zFAqWdJSUmSpFClaWvFz82aNYvJmqLJUq+lVslWL63ebJVs9VpqlWz1WmqVbPXS6s1WyVavpdZY8inWX8mp5SZ3foeHAUo9S0lJ0bHHHaecnOzwvpycbB3Xrp0CgUDsFhYllnottUq2emn1Zqtkq9dSq2Sr11KrZKuXVm+2SrZ6LbXC+xigxMA1w0dqRtZU5ebmKjc3VzPvn6aR114X62VFjaVeS62SrV5avdkq2eq11CrZ6rXUKtnqpdWbrZKtXkut8Lb4WC8gktzcXE2dOlVLly7V119/rVatWik9PV1jx47VOeeco6effloLFy7UunXrVFBQoLy8PCVX+n5dQ5R5xyTt3bNHGT27SpKGXnW1brt9YoxXFT2Wei21SrZ6afVmq2Sr11KrZKvXUqtkq5dWb7ZKtnottcLbfI7jOLFexI/54osvdPrppys5OVn33HOPevbsqdLSUv3rX//S008/rc2bN+uRRx7R/v37JUmZmZmHNUAJhUIKBALatScov98fhRIAAAAAQH0LhUJq3SKgYNCbn/UqPsumXPEX+Y5qGuvl1JhTUqS8569z3d+lQZ+BcuONN8rn8+nDDz9UYmJieH/37t117bXXSpLGjh0rSXrrrbdisEIAAAAAAGBBgx2g7N27V8uWLdPUqVOrDE8qHMnXdIqLi1VcXBx+HAqFDvu1AAAAAACA9zXYi8hu3bpVjuPopJNOqvPXzsrKUiAQCG/t2rWr8/cAAAAAAADe0WAHKNG8NEtmZqaCwWB427lzZ9TeCwAAAACAqPK5cHOhBjtAOeGEE+Tz+bR58+Y6f+2EhAT5/f4qW2298PwiDbvy8jpf2+E4cOCA+qT31OZNm6L2HpZ6LbVKtnottUq2ei21SrZ6aY0N/h3XLUutkq1eS62SvV7ghxrsAKV58+YaOHCgHn/8ce3bt++Q3+fn59f/ov6jvLxckydNVObESeF9oVBIw39zlVo196v9sa2VNfXen3yNSMdnThivtq2a6+e90rRp48bw/u3btunU3unhOw9JUqNGjTR23K26a1J0bgVmqddSq2Sr11KrZKvXUqtkq5dWb7ZKtnottUq2ei21SvZ6gWo5Ddjnn3/utGnTxunWrZvzv//7v86nn37qbNy40Xn00Uedk046yXEcx/n222+djz76yHnmmWccSc7KlSudjz76yNmzZ0+N3ycYDDqSnF17gs73pU7E7e+LX3Z69epdZd+wq69xzh0w0Pl2d56z/pMtznHt2jl/eXbej77GTx3/znsfOp06d3Z27Qk6Dzz0qHPer4aEn3fugIHOa/9efsjr/V9+odOsWTNny+df1qihNpulXkut1nottVrrtdRqrZdWb7Za67XUaq3XUqube3ftOfhZLxgMRutja0xVfJZNuXK202L4c67ZUq6c7cq/S4M9A0WSOnbsqHXr1qlfv3665ZZb1KNHD5177rlavny5nnzySUnSU089pYyMDF1//fWSpF/+8pfKyMjQkiVLoraupS8v0Vn9zg4/Lioq0osvLNLkKfcpOTlZJ5x4om64abTmzpld7fMjHb99+zb16t1Hfr9f/c8doG3bPpckLXpuoVq3bqO+ld67QmJionr3OUXLXl1KL630Gm+11mup1Vovrd5stdZrqdVar6VWi71AdRr0AEWSUlNT9ac//UlffPGFiouL9dVXX+mf//yn+vbtK0m6++675TjOIduIESOitqacnGx16fLfuwN9umWLSkpKlJaeHt6Xlpaujzesr/b5kY7v3r2H1q1do/z8fK1Y/oa69+ipvLw8zZw+TffPfPBH13VS125an5N9RG3VsdRrqVWy1WupVbLVa6lVstVLqzdbJVu9llolW72WWiV7vUB1GvwApSHKz89Ts0oXni3cV6jExETFx8eH9wUCySooKKj2+ZGO79a9u24aNUYDz+mrN17/l7KmP6CJE8brllsnaNOmjRp07tkaPOAcvbtqVZXX9fv9ysvPq8tUSbZ6LbVKtnottUq2ei21SrZ6afVmq2Sr11KrZKvXUqtkrxeoTnzkQ/BDyckpKgiFwo+TEpNUVFSksrKy8P8ghEJBNWvWrNrn1+T4G24apRtuGiVJWvXOSu3cuUNDrxqmEzu11+vL35bjOBo04Gxt2fqFfD7ff14jpJTkFHpppdd4q7VeS63Wemn1Zqu1Xkut1nottVrsdRufzxf+b+IGblprZZyBchjS0tK1Zct/b698Ypcuaty4sdbn5IT35eRkq3uPntU+vzbHl5SUaPy4sXp01hPavXu3DpSVqUPHjurYqZNKS0q0e/fu8LGbN23UyWnpdVBYlaVeS62SrV5LrZKtXkutkq1eWr3ZKtnqtdQq2eq11CrZ6wWqwwDlMJw35Hy9/dab4cdNmzbVpZddoXvunqRgMKitn32mJx+fpZHXXlft82tz/MzpWbr40svUqXNnHXPMMSouLtb6nBxtWL9eJSUlatGihaSDF2Vau2a1Bg4+j15a6TXeaq3XUqu1Xlq92Wqt11KrtV5LrRZ7gWrV701/Gqba3sa4cH+Z0/744501H22ocnusy64Y6iQlJTktW7Z0Jk2eUuU5AwYOcqbcO7XGx39f6jg5H292evXq7YSKSsL75i9Y5LRJTXVS27Z1Fix6Mbx/9pz5zpDzL6jz25VZ67XUaq3XUqu1Xkut1npp9WartV5LrdZ6LbW6udfKbYxbDJvjtBz5vGu2FsPmuPLv4nMcx4ntCCf2QqGQAoGAdu0Jyl/pwkg/5flFz+nlJYv1t4XPR3l1kZWXl+vU3umav2CRunbrFpX3sNRrqVWy1WupVbLVa6lVstVLa2zw77huWWqVbPVaapXc2RsKhdS6RUDBYM0/67lJxWfZFsPmKO6oprFeTo2VlxRpz4KRrvu7MEDR4Q1QAAAAAAANGwOUhsmtAxSugQIAAAAAABABtzEGAAAAAMDNfP/Z3MJNa62EM1AAAAAAAAAiYIACAAAAAAAQAV/hAQAAAADAxXw+n3w+93wvxk1rrYwzUAAAAAAAACJggAIAAAAAABABAxQAAAAAAIAIuAYKAAAAAAAuxjVQ6gdnoAAAAAAAAETAAAUAAAAAADRYK1eu1Pnnn6+2bdvK5/Np8eLF4d+VlpZqwoQJ6tmzpxITE9W2bVtdc801+uabb6q8xt69ezVs2DD5/X4lJyfrt7/9rQoLC2u1DgYoAAAAAACgwdq3b5/S0tL0+OOPH/K7oqIirVu3TpMmTdK6dev0j3/8Q1u2bNEFF1xQ5bhhw4bpk08+0euvv65XXnlFK1eu1O9+97tarYNroAAAAAAA4GJevwbK4MGDNXjw4Gp/FwgE9Prrr1fZ96c//Uk///nPtWPHDv3sZz/Tpk2btGzZMq1evVp9+vSRJM2aNUvnnXeeHnjgAbVt27ZG6+AMFAAAAAAAUO9CoVCVrbi4uE5eNxgMyufzKTk5WZL0/vvvKzk5OTw8kaT+/fsrLi5OH3zwQY1flwEKAAAAAACod+3atVMgEAhvWVlZR/ya+/fv14QJE3TllVfK7/dLknJzc9WqVasqx8XHx6t58+bKzc2t8WvzFR4AAAAAAFzMrV/h2blzZ3jIIUkJCQlH9LqlpaW6/PLL5TiOnnzyySN6reowQAEAAAAAAPXO7/dXGaAciYrhyZdffqkVK1ZUed02bdrou+++q3J8WVmZ9u7dqzZt2tT4PfgKDwAAAAAAcK2K4clnn32mN954Qy1atKjy+9NOO035+flau3ZteN+KFStUXl6uU089tcbvwxkoAAAAAACgwSosLNTWrVvDj7dv367s7Gw1b95cqampuvTSS7Vu3Tq98sorOnDgQPi6Js2bN9dRRx2lrl27atCgQbr++uv11FNPqbS0VKNGjdLQoUNrfAceiQEKAAAAAADu5vvP5ha1XOuaNWvUr1+/8ONx48ZJkoYPH667775bS5YskSSlp6dXed6bb76pvn37SpIWLFigUaNG6ZxzzlFcXJwuueQSPfbYY7VaBwMUAAAAAADQYPXt21eO4/zo73/qdxWaN2+uhQsXHtE6uAYKAAAAAABABJyBAgAAAACAi7n1NsZuwxkoAAAAAAAAETBAAQAAAAAAiIABCgAAAAAAQARcAwUAAAAAABfjGij1gzNQAAAAAAAAImCAAgAAAAAAEAFf4QEAAAAAwMX4Ck/94AwUAAAAAACACBigAAAAAAAARMAABQAAAAAAIAIGKDFQWlqqsTePUmrLFLVt1Vx/HDNaZWVlsV5W1FjqtdQq2eql1Zutkq1eS62SrV5LrZKtXlq92SrZ6rXUGjM+F24uxAAlBu6fdp/ef3eV1q3fqLU5n+i9Ve9oxv3TYr2sqLHUa6lVstVLqzdbJVu9llolW72WWiVbvbR6s1Wy1WupFd7GACUG5s19VhMm3qnU1FSlpqbqtsw7NHfO7FgvK2os9VpqlWz10urNVslWr6VWyVavpVbJVi+t3myVbPVaaoW3MUCpZ3l5efr6q6+UlpYe3peWlq6dO3YoGAzGbmFRYqnXUqtkq5dWb7ZKtnottUq2ei21SrZ6afVmq2Sr11IrvI8BSj0rLCyUJAWSk8P7Kn4uKCiIwYqiy1KvpVbJVi+tB3/2Wqtkq9dSq2Sr11KrZKuX1oM/e61VstVrqTWWfD6f6zY3YoBSz5KSkiRJoUrT1oqfmzVrFpM1RZOlXkutkq1eWr3ZKtnqtdQq2eq11CrZ6qXVm62SrV5LrfA+Bij1LCUlRcced5xycrLD+3JysnVcu3YKBAKxW1iUWOq11CrZ6qXVm62SrV5LrZKtXkutkq1eWr3ZKtnqtdQK72OAEgPXDB+pGVlTlZubq9zcXM28f5pGXntdrJcVNZZ6LbVKtnpp9WarZKvXUqtkq9dSq2Srl1Zvtkq2ei21xkqsv45j5Ss88bFegEWZd0zS3j17lNGzqyRp6FVX67bbJ8Z4VdFjqddSq2Srl1Zvtkq2ei21SrZ6LbVKtnpp9WarZKvXUiu8zec4jhPrRVRnxIgRmjdvniQpPj5ezZs318knn6wrr7xSI0aMUFzcwZNnnn76aS1cuFDr1q1TQUGB8vLylFzpAkU1EQqFFAgEtGtPUH6/v65TAAAAAAAxEAqF1LpFQMGgNz/rVXyWPe73ixR3VNNYL6fGykuK9NWfh7ru79Kgv8IzaNAgffvtt/riiy/02muvqV+/fhozZoyGDBmisrIySVJRUZEGDRqkiROZYAIAAAAAgOho0F/hSUhIUJs2bSRJxx57rHr16qX/+Z//0TnnnKO5c+fquuuu09ixYyVJb731VuwWCgAAAABAjPjkruuK+OSetVbWoM9Aqc7ZZ5+ttLQ0/eMf/zjs1yguLlYoFKqyAQAAAAAA/BjXDVAk6aSTTtIXX3xx2M/PyspSIBAIb+3atau7xQEAAAAAAM9x5QDFcZwjOj0pMzNTwWAwvO3cubPWr/HC84s07MrLD3sNdenAgQPqk95Tmzdtitp7WOq11CrZ6rXUKtnqtdQq2eqlNTb4d1y3LLVKtnottUr2et0k1rcktnIbY1cOUDZt2qQOHToc9vMTEhLk9/urbLVRXl6uyZMmKnPipPC+UCik4b+5Sq2a+9X+2NbKmnrvT75GpOMzJ4xX21bN9fNeadq0cWN4//Zt23Rq73Tt378/vK9Ro0YaO+5W3TUpOhfStdRrqVWy1WupVbLVa6lVstVLqzdbJVu9llolW72WWiV7vUB1XDdAWbFihTZs2KBLLrkkZmtY9tqrap7SXD169gzvGzdmtPL27tWn23bojTff0bOzn9GCv87/0df4qePXrF6tl5cs1uatX2j4yN/qzokTws8bM/pGTX/gITVp0qTK6/36kkv11orl2rFjRx3X2uq11CrZ6rXUKtnqtdQq2eql1Zutkq1eS62SrV5LrZK9XqA6DXqAUlxcrNzcXH399ddat26dpk2bpgsvvFBDhgzRNddcI0nKzc1Vdna2tm7dKknasGGDsrOztXfv3qita+nLS3RWv7PDj4uKivTiC4s0ecp9Sk5O1gknnqgbbhqtuXNmV/v8SMdv375NvXr3kd/vV/9zB2jbts8lSYueW6jWrduob6X3rpCYmKjefU7RsleX0ksrvcZbrfVaarXWS6s3W631Wmq11mup1WIvUJ0GPUBZtmyZUlNTdfzxx2vQoEF688039dhjj+mf//ynGjVqJEl66qmnlJGRoeuvv16S9Mtf/lIZGRlasmRJ1NaVk5OtLl1OCj/+dMsWlZSUKC09PbwvLS1dH29YX+3zIx3fvXsPrVu7Rvn5+Vqx/A1179FTeXl5mjl9mu6f+eCPruukrt20Pif7iNqqY6nXUqtkq9dSq2Sr11KrZKuXVm+2SrZ6LbVKtnottUr2el3H58LNhRrsAGXu3LlyHEeO46i0tFTfffedXn/9dY0cOVJxcf9d9t133x0+rvI2YsSIqK0tPz9PzSpdN6VwX6ESExMVHx8f3hcIJKugoKDa50c6vlv37rpp1BgNPKev3nj9X8qa/oAmThivW26doE2bNmrQuWdr8IBz9O6qVVVe1+/3Ky8/ry5TJdnqtdQq2eq11CrZ6rXUKtnqpdWbrZKtXkutkq1eS62SvV6gOvGRD8EPJSenqCAUCj9OSkxSUVGRysrKwv+DEAoF1axZs2qfX5Pjb7hplG64aZQkadU7K7Vz5w4NvWqYTuzUXq8vf1uO42jQgLO1ZesX4SsYh0IhpSSn0EsrvcZbrfVaarXWS6s3W631Wmq11mup1WIvUJ0GewZKQ5aWlq4tWzaHH5/YpYsaN26s9Tk54X05Odnq3qNndU+v1fElJSUaP26sHp31hHbv3q0DZWXq0LGjOnbqpNKSEu3evTt87OZNG3VyWnodFFZlqddSq2Sr11KrZKvXUqtkq5dWb7ZKtnottUq2ei21SvZ63SbWtyTmNsb4UecNOV9vv/Vm+HHTpk116WVX6J67JykYDGrrZ5/pycdnaeS111X7/NocP3N6li6+9DJ16txZxxxzjIqLi7U+J0cb1q9XSUmJWrRoIengRZnWrlmtgYPPo5dWeo23Wuu11Gqtl1ZvtlrrtdRqrddSq8VeoFoOnGAw6Ehydu0JOt+XOhG3wv1lTvvjj3fWfLQhvG/XnqBz2RVDnaSkJKdly5bOpMlTqjxnwMBBzpR7p9b4+O9LHSfn481Or169nVBRSXjf/AWLnDapqU5q27bOgkUvhvfPnjPfGXL+BTVaf203S72WWq31Wmq11mup1Vovrd5stdZrqdVar6VWN/fu2nPws14wGIz1x86oqPgs2/7GF50Of1zqmq39jS+68u/icxzHie0IJ/ZCoZACgYB27QnKX+nCSD/l+UXP6eUli/W3hc9HeXWRlZeX69Te6Zq/YJG6dusWlfew1GupVbLVa6lVstVrqVWy1UtrbPDvuG5ZapVs9VpqldzZGwqF1LpFQMFgzT/ruUnFZ9n2N76ouISmsV5OjZUXF+nLJy5z3d+FAYoOb4ACAAAAAGjYrAxQjr/pf103QPni8Utd93fhGigAAAAAAAARMEABAAAAAACIgAEKAAAAAABABPGxXgAAAAAAADh8Pt/BzS3ctNbKOAMFAAAAAAAgAgYoAAAAAAAAEfAVHgAAAAAAXOzgV3jc870YFy21Cs5AAQAAAAAAiIABCgAAAAAAQAQMUAAAAAAAACLgGigAAAAAALiZy25jLDettRLOQAEAAAAAAIiAAQoAAAAAAEAEfIUHAAAAAAAX8/l8LruNsXvWWhlnoAAAAAAAAETAAAUAAAAAACACBigAAAAAAAARcA0UAAAAAABczOey2xi7aa2VcQYKAAAAAABABAxQAAAAAAAAImCAAgAAAAAAEAHXQAEAAAAAwMXi4nyKi3PPhUUcF621Ms5AAQAAAAAAiIABCgAAAAAAQAR8hQcAAAAAABfjNsb1gzNQAAAAAAAAImCAAgAAAAAAEAEDFAAAAAAAgAi4BgoAAAAAAC7m8/nkc9GFRdy01so4AwUAAAAAACACBigAAAAAAAAR8BUeAAAAAABcjNsY1w/OQAEAAAAAAIiAAQoAAAAAAEAEDFBioLS0VGNvHqXUlilq26q5/jhmtMrKymK9rKix1GupVbLVS6s3WyVbvZZaJVu9llolW720erNVstVrqRXexgAlBu6fdp/ef3eV1q3fqLU5n+i9Ve9oxv3TYr2sqLHUa6lVstVLqzdbJVu9llolW72WWiVbvbR6s1Wy1WupNVYqbmPsps2NGKDEwLy5z2rCxDuVmpqq1NRU3ZZ5h+bOmR3rZUWNpV5LrZKtXlq92SrZ6rXUKtnqtdQq2eql1Zutkq1eS63wNgYo9SwvL09ff/WV0tLSw/vS0tK1c8cOBYPB2C0sSiz1WmqVbPXS6s1WyVavpVbJVq+lVslWL63ebJVs9VpqhfcxQKlnhYWFkqRAcnJ4X8XPBQUFMVhRdFnqtdQq2eql9eDPXmuVbPVaapVs9VpqlWz10nrwZ6+1SrZ6LbXGUqy/jsNXeBAVSUlJkqRQpWlrxc/NmjWLyZqiyVKvpVbJVi+t3myVbPVaapVs9VpqlWz10urNVslWr6VWeB8DlHqWkpKiY487Tjk52eF9OTnZOq5dOwUCgdgtLEos9VpqlWz10urNVslWr6VWyVavpVbJVi+t3myVbPVaaoX3MUCJgWuGj9SMrKnKzc1Vbm6uZt4/TSOvvS7Wy4oaS72WWiVbvbR6s1Wy1WupVbLVa6lVstVLqzdbJVu9llrhbfGxXkAkI0aM0Lx58w7ZP3DgQC1btkxPP/20Fi5cqHXr1qmgoEB5eXlKrvT9uoYo845J2rtnjzJ6dpUkDb3qat12+8QYryp6LPVaapVs9dLqzVbJVq+lVslWr6VWyVYvrd5slWz1WmqNFZ/v4OYWblprZT7HcZxYL+KnjBgxQrt27dKcOXOq7E9ISFBKSooeeeQR7d+/X5KUmZl5WAOUUCikQCCgXXuC8vv9dbV0AAAAAEAMhUIhtW4RUDDozc96FZ9le9z+TzVKSIz1cmrsQPE+fXz/ha77uzT4M1Ckg8OSNm3aVPu7sWPHSpLeeuut+lsQAAAAAAAwxRUDlLpWXFys4uLi8ONQKBTD1QAAAAAAgIbOFReRfeWVV5SUlFRlmzZt2mG/XlZWlgKBQHhr165dHa4WAAAAAID645NPPp+LNrnzIiiuOAOlX79+evLJJ6vsa968+WG/XmZmpsaNGxd+HAqFGKIAAAAAAIAf5YozUBITE9W5c+cq25EMUBISEuT3+6tstfXC84s07MrLD3sNdenAgQPqk95Tmzdtitp7WOq11CrZ6rXUKtnqtdQq2eqlNTb4d1y3LLVKtnottUr2eoEfcsUApaEpLy/X5EkTlTlxUnhfKBTS8N9cpVbN/Wp/bGtlTb33J18j0vGZE8arbavm+nmvNG3auDG8f/u2bTq1d3r4zkOS1KhRI40dd6vumhSdW4FZ6rXUKtnqtdQq2eq11CrZ6qXVm62SrV5LrZKtXkutkr1et6m4jbGbNjdyxQCluLhYubm5Vbb/+7//kyTl5uYqOztbW7dulSRt2LBB2dnZ2rt3b9TWs+y1V9U8pbl69OwZ3jduzGjl7d2rT7ft0BtvvqNnZz+jBX+d/6Ov8VPHr1m9Wi8vWazNW7/Q8JG/1Z0TJ4SfN2b0jZr+wENq0qRJldf79SWX6q0Vy7Vjx446rrXVa6lVstVrqVWy1WupVbLVS6s3WyVbvZZaJVu9llole71AdVwxQFm2bJlSU1OrbGeccYYk6amnnlJGRoauv/56SdIvf/lLZWRkaMmSJVFbz9KXl+isfmeHHxcVFenFFxZp8pT7lJycrBNOPFE33DRac+fMrvb5kY7fvn2bevXuI7/fr/7nDtC2bZ9LkhY9t1CtW7dR30rvXSExMVG9+5yiZa8upZdWeo23Wuu11Gqtl1ZvtlrrtdRqrddSq8VeoDoNfoAyd+5cOY5zyLZ582ZJ0t13313t70eMGBG1NeXkZKtLl5PCjz/dskUlJSVKS08P70tLS9fHG9ZX+/xIx3fv3kPr1q5Rfn6+Vix/Q9179FReXp5mTp+m+2c++KPrOqlrN63PyT6itupY6rXUKtnqtdQq2eq11CrZ6qXVm62SrV5LrZKtXkutkr1eoDoNfoDSEOXn56lZpQvPFu4rVGJiouLj/3tTo0AgWQUFBdU+P9Lx3bp3102jxmjgOX31xuv/Utb0BzRxwnjdcusEbdq0UYPOPVuDB5yjd1etqvK6fr9fefl5dZkqyVavpVbJVq+lVslWr6VWyVYvrd5slWz1WmqVbPVaapXs9bpNzG9LfBibG7niNsYNTXJyigpCofDjpMQkFRUVqaysLPw/CKFQUM2aNav2+TU5/oabRumGm0ZJkla9s1I7d+7Q0KuG6cRO7fX68rflOI4GDThbW7Z+Ef7HFwqFlJKcQi+t9BpvtdZrqdVaL63ebLXWa6nVWq+lVou9QHU4A+UwpKWla8uWzeHHJ3bposaNG2t9Tk54X05Otrr36Fnd02t1fElJicaPG6tHZz2h3bt360BZmTp07KiOnTqptKREu3fvDh+7edNGnZyWXgeFVVnqtdQq2eq11CrZ6rXUKtnqpdWbrZKtXkutkq1eS62SvV6gOgxQDsN5Q87X22+9GX7ctGlTXXrZFbrn7kkKBoPa+tlnevLxWRp57XXVPr82x8+cnqWLL71MnTp31jHHHKPi4mKtz8nRhvXrVVJSohYtWkg6eFGmtWtWa+Dg8+illV7jrdZ6LbVa66XVm63Wei21Wuu11Gqx121ifUtiK7cxlgMnGAw6kpxde4LO96VOxK1wf5nT/vjjnTUfbQjv27Un6Fx2xVAnKSnJadmypTNp8pQqzxkwcJAz5d6pNT7++1LHyfl4s9OrV28nVFQS3jd/wSKnTWqqk9q2rbNg0Yvh/bPnzHeGnH9BjdZf281Sr6VWa72WWq31Wmq11kurN1ut9VpqtdZrqdXNvbv2HPysFwwGY/2xMyoqPsum3/Gy0/veFa7Z0u942ZV/F5/jOE5sRzixFwqFFAgEtGtPUP5KF0b6Kc8vek4vL1msvy18Psqri6y8vFyn9k7X/AWL1LVbt6i8h6VeS62SrV5LrZKtXkutkq1eWmODf8d1y1KrZKvXUqvkzt5QKKTWLQIKBmv+Wc9NKj7Lpt/xsho1SYz1cmrswP59yp56vuv+LgxQdHgDFAAAAABAw8YApWFy6wCFu/AAAAAAAOBibrs1sJvWWhkXkQUAAAAAAIiAAQoAAAAAAEAEfIUHAAAAAAAXc9utgd201so4AwUAAAAAACACBigAAAAAAAARMEABAAAAAACIgGugAAAAAADgYtzGuH5wBgoAAAAAAEAEDFAAAAAAAAAiYIACAAAAAAAQAddAAQAAAADAzXySqy4r4qa1VsIZKAAAAAAAoMFauXKlzj//fLVt21Y+n0+LFy+u8nvHcXTXXXcpNTVVRx99tPr376/PPvusyjF79+7VsGHD5Pf7lZycrN/+9rcqLCys1ToYoAAAAAAAgAZr3759SktL0+OPP17t72fMmKHHHntMTz31lD744AMlJiZq4MCB2r9/f/iYYcOG6ZNPPtHrr7+uV155RStXrtTvfve7Wq2Dr/AAAAAAAOBiXr+N8eDBgzV48OBqf+c4jh555BHdeeeduvDCCyVJ8+fPV+vWrbV48WINHTpUmzZt0rJly7R69Wr16dNHkjRr1iydd955euCBB9S2bdsarYMzUAAAAAAAQL0LhUJVtuLi4lq/xvbt25Wbm6v+/fuH9wUCAZ166ql6//33JUnvv/++kpOTw8MTSerfv7/i4uL0wQcf1Pi9GKAAAAAAAIB6165dOwUCgfCWlZVV69fIzc2VJLVu3brK/tatW4d/l5ubq1atWlX5fXx8vJo3bx4+pib4Cg8AAAAAAKh3O3fulN/vDz9OSEiI4WoiY4ACAAAAAICL+Vx2G+OKtfr9/ioDlMPRpk0bSdKuXbuUmpoa3r9r1y6lp6eHj/nuu++qPK+srEx79+4NP78m+AoPAAAAAABwpQ4dOqhNmzZavnx5eF8oFNIHH3yg0047TZJ02mmnKT8/X2vXrg0fs2LFCpWXl+vUU0+t8XtxBgoAAAAAAGiwCgsLtXXr1vDj7du3Kzs7W82bN9fPfvYzjR07Vvfdd59OOOEEdejQQZMmTVLbtm110UUXSZK6du2qQYMG6frrr9dTTz2l0tJSjRo1SkOHDq3xHXgkBigAAAAAALia129jvGbNGvXr1y/8eNy4cZKk4cOHa+7cubrtttu0b98+/e53v1N+fr7OOOMMLVu2TE2aNAk/Z8GCBRo1apTOOeccxcXF6ZJLLtFjjz1Wu3U7juPU6hkeFAqFFAgEtGtP8Ii/fwUAAAAAaBhCoZBatwgoGPTmZ72Kz7I/v+c1xTdJjPVyaqxs/z59eNdg1/1duAYKAAAAAABABAxQAAAAAAAAIuAaKAAAAAAAuJhbb2PsNpyBAgAAAAAAEAEDFAAAAAAAgAj4Cg8AAAAAAC7m9dsYNxScgQIAAAAAABABAxQAAAAAAIAIGKAAAAAAAABEwDVQAAAAAABwMa6BUj84AyUGSktLNfbmUUptmaK2rZrrj2NGq6ysLNbLihpLvZZaJVu9tHqzVbLVa6lVstVrqVWy1UurN1slW72WWuFtDFBi4P5p9+n9d1dp3fqNWpvzid5b9Y5m3D8t1suKGku9llolW720erNVstVrqVWy1WupVbLVS6s3WyVbvZZa4W0MUGJg3txnNWHinUpNTVVqaqpuy7xDc+fMjvWyosZSr6VWyVYvrd5slWz1WmqVbPVaapVs9dLqzVbJVq+lVngbA5R6lpeXp6+/+kppaenhfWlp6dq5Y4eCwWDsFhYllnottUq2emn1Zqtkq9dSq2Sr11KrZKuXVm+2SrZ6LbXGks/nvs2NGKDUs8LCQklSIDk5vK/i54KCghisKLos9VpqlWz10nrwZ6+1SrZ6LbVKtnottUq2emk9+LPXWiVbvZZa4X0MUOpZUlKSJClUadpa8XOzZs1isqZostRrqVWy1UurN1slW72WWiVbvZZaJVu9tHqzVbLVa6kV3scApZ6lpKTo2OOOU05OdnhfTk62jmvXToFAIHYLixJLvZZaJVu9tHqzVbLVa6lVstVrqVWy1UurN1slW72WWmOp4jbGbtrciAFKDFwzfKRmZE1Vbm6ucnNzNfP+aRp57XWxXlbUWOq11CrZ6qXVm62SrV5LrZKtXkutkq1eWr3ZKtnqtdQKb4uP9QIsyrxjkvbu2aOMnl0lSUOvulq33T4xxquKHku9llolW720erNVstVrqVWy1WupVbLVS6s3WyVbvZZa4W0+x3GcWC/ip4wYMULz5s07ZP9nn32mb775RjNnztTatWv17bff6qWXXtJFF11U6/cIhUIKBALatScov99fB6sGAAAAAMRaKBRS6xYBBYPe/KxX8Vn29Kx/K75JYqyXU2Nl+/fp3cwBrvu7uOIrPIMGDdK3335bZevQoYP27duntLQ0Pf7447FeIgAAAAAAMRHrWxJbuY2xK77Ck5CQoDZt2hyyf/DgwRo8eHAMVgQAAAAAACxxxQClrhUXF6u4uDj8OBQKxXA1AAAAAACgoXPFV3heeeUVJSUlhbfLLrvsiF4vKytLgUAgvLVr166OVgoAAAAAQP2K9S2JuY1xA9KvXz9lZ2eHt8cee+yIXi8zM1PBYDC87dy5s9av8cLzizTsysuPaB115cCBA+qT3lObN22K2ntY6rXUKtnqtdQq2eq11CrZ6qU1Nvh3XLcstUq2ei21SvZ6gR9yxQAlMTFRnTt3Dm+pqalH9HoJCQny+/1VttooLy/X5EkTlTlxUnhfKBTS8N9cpVbN/Wp/bGtlTb33J18j0vGZE8arbavm+nmvNG3auDG8f/u2bTq1d7r2798f3teoUSONHXer7poUnVuBWeq11CrZ6rXUKtnqtdQq2eql1Zutkq1eS62SrV5LrZK9XqA6rhigNDTLXntVzVOaq0fPnuF948aMVt7evfp02w698eY7enb2M1rw1/k/+ho/dfya1av18pLF2rz1Cw0f+VvdOXFC+HljRt+o6Q88pCZNmlR5vV9fcqneWrFcO3bsqONaW72WWiVbvZZaJVu9llolW720erNVstVrqVWy1WupVbLXC1TH1QOUwsLC8Nd6JGn79u3Kzs6O+v8FWvryEp3V7+zw46KiIr34wiJNnnKfkpOTdcKJJ+qGm0Zr7pzZ1T4/0vHbt29Tr9595Pf71f/cAdq27XNJ0qLnFqp16zbqW+m9KyQmJqp3n1O07NWl9NJKr/FWa72WWq310urNVmu9llqt9VpqtdjrNj7F/rbEtdpi/R/sMLl6gLJmzRplZGQoIyNDkjRu3DhlZGTorrvuiur75uRkq0uXk8KPP92yRSUlJUpLTw/vS0tL18cb1lf7/EjHd+/eQ+vWrlF+fr5WLH9D3Xv0VF5enmZOn6b7Zz74o+s6qWs3rc/JPqK26ljqtdQq2eq11CrZ6rXUKtnqpdWbrZKtXkutkq1eS62SvV6gOg1+gDJ37lwtXry42t/17dtXjuMcss2dOzeqa8rPz1OzStdNKdxXqMTERMXH//eu0IFAsgoKCqp9fqTju3XvrptGjdHAc/rqjdf/pazpD2jihPG65dYJ2rRpowade7YGDzhH765aVeV1/X6/8vLz6jJVkq1eS62SrV5LrZKtXkutkq1eWr3ZKtnqtdQq2eq11CrZ6wWqEx/5EPxQcnKKCkKh8OOkxCQVFRWprKws/D8IoVBQzZo1q/b5NTn+hptG6YabRkmSVr2zUjt37tDQq4bpxE7t9fryt+U4jgYNOFtbtn4RvgVUKBRSSnIKvbTSa7zVWq+lVmu9tHqz1VqvpVZrvZZaLfYC1WnwZ6A0RGlp6dqyZXP48Ylduqhx48Zan5MT3peTk63uPXpW9/RaHV9SUqLx48bq0VlPaPfu3TpQVqYOHTuqY6dOKi0p0e7du8PHbt60USenpddBYVWWei21SrZ6LbVKtnottUq2emn1Zqtkq9dSq2Sr11KrZK/XbeJ8PtdtbsQA5TCcN+R8vf3Wm+HHTZs21aWXXaF77p6kYDCorZ99picfn6WR115X7fNrc/zM6Vm6+NLL1KlzZx1zzDEqLi7W+pwcbVi/XiUlJWrRooWkgxdlWrtmtQYOPo9eWuk13mqt11KrtV5avdlqrddSq7VeS60We4FqOXCCwaAjydm1J+h8X+pE3Ar3lzntjz/eWfPRhvC+XXuCzmVXDHWSkpKcli1bOpMmT6nynAEDBzlT7p1a4+O/L3WcnI83O7169XZCRSXhffMXLHLapKY6qW3bOgsWvRjeP3vOfGfI+RfUaP213Sz1Wmq11mup1VqvpVZrvbR6s9Var6VWa72WWt3cu2vPwc96wWAw1h87o6Lis2zfGW84/We975qt74w3XPl38TmO48R2hBN7oVBIgUBAu/YE5a90YaSf8vyi5/TyksX628Lno7y6yMrLy3Vq73TNX7BIXbt1i8p7WOq11CrZ6rXUKtnqtdQq2eqlNTb4d1y3LLVKtnottUru7A2FQmrdIqBgsOaf9dyk4rNsv5lvKP7oxFgvp8bKvt+nN8f3d93fhQGKDm+AAgAAAABo2BigNExuHaBwDRQAAAAAAIAIGKAAAAAAAABEEB/rBQAAAAAAgMPn8/nkc9Gtgd201so4AwUAAAAAACACBigAAAAAAAAR8BUeAAAAAABcLM53cHMLN621Ms5AAQAAAAAAiIABCgAAAAAAQAQMUAAAAAAAACLgGigAAAAAALiZz2W3BnbRUivjDBQAAAAAAIAIGKAAAAAAAABEwFd4AAAAAABwMZ/v4OYWblprZZyBAgAAAAAAEAEDFAAAAAAAgAgYoAAAAAAAAETANVAAAAAAAHAx33/+j1u4aa2VcQYKAAAAAABABAxQAAAAAAAAImCAAgAAAAAAEAHXQAEAAAAAwMXifAc3t3DTWivjDBQAAAAAAIAIGKAAAAAAAABEwFd4AAAAAABwMZ/PJ5/PPd+LcdNaK+MMFAAAAAAAgAgYoAAAAAAAAETAAAUAAAAAACACroECAAAAAICL+XwHN7dw01or4wwUAAAAAACACBigAAAAAAAARMBXeAAAAAAAcLE4n09xLvpejJvWWhlnoAAAAAAAAETAAAUAAAAAACACBigAAAAAAAARMECJgdLSUo29eZRSW6aobavm+uOY0SorK4v1sqLGUq+lVslWL63ebJVs9VpqlWz1WmqVbPXS6s1WyVavpdZYqbiNsZs2N2KAEgP3T7tP77+7SuvWb9TanE/03qp3NOP+abFeVtRY6rXUKtnqpdWbrZKtXkutkq1eS62SrV5avdkq2eq11ApvY4ASA/PmPqsJE+9UamqqUlNTdVvmHZo7Z3aslxU1lnottUq2emn1Zqtkq9dSq2Sr11KrZKuXVm+2SrZ6LbXC2xig1LO8vDx9/dVXSktLD+9LS0vXzh07FAwGY7ewKLHUa6lVstVLqzdbJVu9llolW72WWiVbvbR6s1Wy1WupNZZ8Pp/rNjdigFLPCgsLJUmB5OTwvoqfCwoKYrCi6LLUa6lVstVL68GfvdYq2eq11CrZ6rXUKtnqpfXgz15rlWz1WmqF9zFAqWdJSUmSpFClaWvFz82aNYvJmqLJUq+lVslWL63ebJVs9VpqlWz1WmqVbPXS6s1WyVavpVZ4HwOUepaSkqJjjztOOTnZ4X05Odk6rl07BQKB2C0sSiz1WmqVbPXS6s1WyVavpVbJVq+lVslWL63ebJVs9VpqhfcxQImBa4aP1IysqcrNzVVubq5m3j9NI6+9LtbLihpLvZZaJVu9tHqzVbLVa6lVstVrqVWy1UurN1slW72WWmMl1rcktnIb4/hYL+Cn5ObmKisrS0uXLtVXX32lQCCgzp076+qrr9bw4cPVtGlTPf3001q4cKHWrVungoIC5eXlKbnS9+saosw7Jmnvnj3K6NlVkjT0qqt12+0TY7yq6LHUa6lVstVLqzdbJVu9llolW72WWiVbvbR6s1Wy1WupFd7mcxzHifUiqrNt2zadfvrpSk5O1pQpU9SzZ08lJCRow4YNevrpp/X73/9eF1xwgR555BHt379fkpSZmXlYA5RQKKRAIKBde4Ly+/1RqAEAAAAA1LdQKKTWLQIKBr35Wa/is+yFT7ytxkcnxXo5NVb6faH+eeNZrvu7NNgzUG688UbFx8drzZo1SkxMDO/v2LGjLrzwQlXMfcaOHStJeuutt2KwSgAAAAAAYEGDHKDs2bNH//73vzVt2rQqw5PKjuS+0cXFxSouLg4/DoVCh/1aAAAAAADEUpzPpzgXXVjETWutrEFeRHbr1q1yHEddunSpsv+YY45RUlKSkpKSNGHChMN+/aysLAUCgfDWrl27I10yAAAAAADwsAY5QPkxH374obKzs9W9e/cqZ5DUVmZmpoLBYHjbuXNnHa4SAAAAAAB4TYMcoHTu3Fk+n09btmypsr9jx47q3Lmzjj766CN6/YSEBPn9/ipbbb3w/CINu/LyI1pHXTlw4ID6pPfU5k2bovYelnottUq2ei21SrZ6LbVKtnppjQ3+HdctS62SrV5LrZK9XjfxuXBzowY5QGnRooXOPfdc/elPf9K+fftivZxDlJeXa/KkicqcOCm8LxQKafhvrlKr5n61P7a1sqbe+5OvEen4zAnj1bZVc/28V5o2bdwY3r992zad2js9fOchSWrUqJHGjrtVd02Kzq3ALPVaapVs9VpqlWz1WmqVbPXS6s1WyVavpVbJVq+lVsleL1CdBjlAkaQnnnhCZWVl6tOnj55//nlt2rRJW7Zs0d/+9jdt3rxZjRo1kiTl5uYqOztbW7dulSRt2LBB2dnZ2rt3b9TWtuy1V9U8pbl69OwZ3jduzGjl7d2rT7ft0BtvvqNnZz+jBX+d/6Ov8VPHr1m9Wi8vWazNW7/Q8JG/1Z0T/3u9lzGjb9T0Bx5SkyZNqrzery+5VG+tWK4dO3bUca2tXkutkq1eS62SrV5LrZKtXlq92SrZ6rXUKtnqtdQq2esFqtNgByidOnXSRx99pP79+yszM1NpaWnq06ePZs2apVtvvVX33ntwWvnUU08pIyND119/vSTpl7/8pTIyMrRkyZKorW3py0t0Vr+zw4+Lior04guLNHnKfUpOTtYJJ56oG24arblzZlf7/EjHb9++Tb1695Hf71f/cwdo27bPJUmLnluo1q3bqG+l966QmJio3n1O0bJXl9JLK73GW631Wmq11kurN1ut9VpqtdZrqdViL1CdBjtAkaTU1FTNmjVL27ZtU0lJiQoKCvTBBx/o1ltvVdOmTSVJd999txzHOWQbMWJE1NaVk5OtLl1OCj/+dMsWlZSUKC09PbwvLS1dH29YX+3zIx3fvXsPrVu7Rvn5+Vqx/A1179FTeXl5mjl9mu6f+eCPruukrt20Pif7iNqqY6nXUqtkq9dSq2Sr11KrZKuXVm+2SrZ6LbVKtnottUr2et3G5/O5bnOjBj1Aaajy8/PUrNKFZwv3FSoxMVHx8fHhfYFAsgoKCqp9fqTju3XvrptGjdHAc/rqjdf/pazpD2jihPG65dYJ2rRpowade7YGDzhH765aVeV1/X6/8vLz6jJVkq1eS62SrV5LrZKtXkutkq1eWr3ZKtnqtdQq2eq11CrZ6wWqEx/5EPxQcnKKCkKh8OOkxCQVFRWprKws/D8IoVBQzZo1q/b5NTn+hptG6YabRkmSVr2zUjt37tDQq4bpxE7t9fryt+U4jgYNOFtbtn4Rnt6FQiGlJKfQSyu9xlut9VpqtdZLqzdbrfVaarXWa6nVYi9QHc5AOQxpaenasmVz+PGJXbqocePGWp+TE96Xk5Ot7j16Vvf0Wh1fUlKi8ePG6tFZT2j37t06UFamDh07qmOnTiotKdHu3bvDx27etFEnp6XXQWFVlnottUq2ei21SrZ6LbVKtnpp9WarZKvXUqtkq9dSq2Sv123ifO7b3IgBymE4b8j5evutN8OPmzZtqksvu0L33D1JwWBQWz/7TE8+Pksjr72u2ufX5viZ07N08aWXqVPnzjrmmGNUXFys9Tk52rB+vUpKStSiRQtJBy/KtHbNag0cfB69tNJrvNVar6VWa720erPVWq+lVmu9llot9gLVcuAEg0FHkrNrT9D5vtSJuBXuL3PaH3+8s+ajDeF9u/YEncuuGOokJSU5LVu2dCZNnlLlOQMGDnKm3Du1xsd/X+o4OR9vdnr16u2EikrC++YvWOS0SU11Utu2dRYsejG8f/ac+c6Q8y+o0fpru1nqtdRqrddSq7VeS63Wemn1Zqu1Xkut1nottbq5d9eeg5/1gsFgrD92RkXFZ9nL/vyOc9X8j1yzXfbnd1z5d/E5juPEdoQTe6FQSIFAQLv2BOWvdGGkn/L8ouf08pLF+tvC56O8usjKy8t1au90zV+wSF27dYvKe1jqtdQq2eq11CrZ6rXUKtnqpTU2+Hdctyy1SrZ6LbVK7uwNhUJq3SKgYLDmn/XcpOKz7GV/fkeNj06K9XJqrPT7Qr34+zNd93dhgKLDG6AAAAAAABo2KwOUy59e5boBygu/O8N1fxeugQIAAAAAABABAxQAAAAAAIAIGKAAAAAAAABEEB/rBQAAAAAAgCPj88V6Bd7HGSgAAAAAAAARMEABAAAAAACIgK/wAAAAAADgYj6fTz4XfYfHTWutjDNQAAAAAAAAImCAAgAAAAAAEAEDFAAAAAAAgAi4BgoAAAAAAC4W5zu4uYWb1loZZ6AAAAAAAABEwAAFAAAAAAAgAr7CAwAAAACAi3Eb4/pRowHKkiVLavyCF1xwwWEvBgAAAAAAoMKBAwd09913629/+5tyc3PVtm1bjRgxQnfeeWd4EOM4jiZPnqxnnnlG+fn5Ov300/Xkk0/qhBNOqNO11GiActFFF9XoxXw+nw4cOHAk6wEAAAAAAJAkTZ8+XU8++aTmzZun7t27a82aNRo5cqQCgYBuvvlmSdKMGTP02GOPad68eerQoYMmTZqkgQMHauPGjWrSpEmdraVGA5Ty8vI6e0MAAAAAAICaeO+993ThhRfqV7/6lSTp+OOP13PPPacPP/xQ0sGzTx555BHdeeeduvDCCyVJ8+fPV+vWrbV48WINHTq0ztZyRBeR3b9/f12tAwAAAAAAHAafCzdJCoVCVbbi4uJD2n7xi19o+fLl+vTTTyVJOTk5WrVqlQYPHixJ2r59u3Jzc9W/f//wcwKBgE499VS9//77h/Xf88fUeoBy4MAB3XvvvTr22GOVlJSkbdu2SZImTZqk2bNn1+niAAAAAACAN7Vr106BQCC8ZWVlHXLM7bffrqFDh+qkk05S48aNlZGRobFjx2rYsGGSpNzcXElS69atqzyvdevW4d/VlVoPUKZOnaq5c+dqxowZOuqoo8L7e/Toob/85S91ujgAAAAAAOBNO3fuVDAYDG+ZmZmHHPPCCy9owYIFWrhwodatW6d58+bpgQce0Lx58+p9vbW+jfH8+fP19NNP65xzztEf/vCH8P60tDRt3ry5ThcHAAAAAAB+WpzPpzgX3Rq4Yq1+v19+v/8njx0/fnz4LBRJ6tmzp7788ktlZWVp+PDhatOmjSRp165dSk1NDT9v165dSk9Pr9t11/YJX3/9tTp37nzI/vLycpWWltbJogAAAAAAAIqKihQXV3V00ahRo/DNbjp06KA2bdpo+fLl4d+HQiF98MEHOu200+p0LbU+A6Vbt25655131L59+yr7//d//1cZGRl1tjAAAAAAAGDb+eefr6lTp+pnP/uZunfvro8++kgPPfSQrr32WkmSz+fT2LFjdd999+mEE04I38a4bdu2uuiii+p0LbUeoNx1110aPny4vv76a5WXl+sf//iHtmzZovnz5+uVV16p08UBAAAAAAC7Zs2apUmTJunGG2/Ud999p7Zt2+r3v/+97rrrrvAxt912m/bt26ff/e53ys/P1xlnnKFly5apSZMmdboWn+M4Tm2f9M477+iee+5RTk6OCgsL1atXL911110aMGBAnS6uvoRCIQUCAe3aE4z4/SsAAAAAgDuEQiG1bhFQMOjNz3oVn2WvmfO+jmqaFOvl1FhJUaHmjzzNdX+XWp+BIklnnnmmXn/99bpeCwAAAAAAQIN0WAMUSVqzZo02bdok6eB1UXr37l1niwIAAAAAAGhIaj1A+eqrr3TllVfq3XffVXJysiQpPz9fv/jFL7Ro0SIdd9xxdb1GAAAAAACAmKr1bYyvu+46lZaWatOmTdq7d6/27t2rTZs2qby8XNddd1001ggAAAAAAH6Ez+dz3eZGtT4D5e2339Z7772nLl26hPd16dJFs2bN0plnnlmniwMAAAAAAGgIan0GSrt27VRaWnrI/gMHDqht27Z1sigAAAAAAICGpNYDlJkzZ2r06NFas2ZNeN+aNWs0ZswYPfDAA3W6OAAAAAAA8NN8PvdtblSjr/CkpKRU+Y7Svn37dOqppyo+/uDTy8rKFB8fr2uvvVYXXXRRVBYKAAAAAAAQKzU6A+WRRx7Rww8/HN6efvppPfvss3r66aer/Pzwww9He72eUFpaqrE3j1JqyxS1bdVcfxwzWmVlZbFeVtRY6rXUKtnqpdWbrZKtXkutkq1eS62SrV5avdkq2eq11Apvq9EAZfjw4TXeENn90+7T+++u0rr1G7U25xO9t+odzbh/WqyXFTWWei21SrZ6afVmq2Sr11KrZKvXUqtkq5dWb7ZKtnottcLban0NlMr279+vUChUZUNk8+Y+qwkT71RqaqpSU1N1W+YdmjtndqyXFTWWei21SrZ6afVmq2Sr11KrZKvXUqtkq5dWb7ZKtnottcZKnM/nus2Naj1A2bdvn0aNGqVWrVopMTFRKSkpVTb8tLy8PH391VdKS0sP70tLS9fOHTsUDAZjt7AosdRrqVWy1UurN1slW72WWiVbvZZaJVu9tHqzVbLVa6kV3lfrAcptt92mFStW6Mknn1RCQoL+8pe/aMqUKWrbtq3mz58fjTV6SmFhoSQpkJwc3lfxc0FBQQxWFF2Wei21SrZ6aT34s9daJVu9llolW72WWiVbvbQe/NlrrZKtXkut8L5aD1BefvllPfHEE7rkkksUHx+vM888U3feeaemTZumBQsWRGONnpKUlCRJClWatlb83KxZs5isKZos9VpqlWz10urNVslWr6VWyVavpVbJVi+t3myVbPVaao2lWN+S2MptjGs9QNm7d686duwoSfL7/dq7d68k6YwzztDKlSvrdnUelJKSomOPO045OdnhfTk52TquXTsFAoHYLSxKLPVaapVs9dLqzVbJVq+lVslWr6VWyVYvrd5slWz1WmqF99V6gNKxY0dt375dknTSSSfphRdekHTwzJTkSqdl4cddM3ykZmRNVW5urnJzczXz/mkaee11sV5W1FjqtdQq2eql1Zutkq1eS62SrV5LrZKtXlq92SrZ6rXUCm+Lr+0TRo4cqZycHJ111lm6/fbbdf755+tPf/qTSktL9dBDD0VjjZ6Tecck7d2zRxk9u0qShl51tW67fWKMVxU9lnottUq2emn1Zqtkq9dSq2Sr11KrZKuXVm+2SrZ6LbXC23yO4zhH8gJffvml1q5dq86dO+vkk0+uq3VpxIgRys/P1+LFi6vsf+utt9SvXz/l5eWpSZMm+sMf/qC1a9dq06ZNGjJkyCHH10QoFFIgENCuPUH5/f66CQAAAAAAxFQoFFLrFgEFg978rFfxWfa6v32oo5omxXo5NVZSVKi/XP1z1/1dan0Gyg+1b99e7du3r4u11NqBAwd09NFH6+abb9bf//73mKwBAAAAAAB4X40GKI899liNX/Dmm28+7MXUVmJiop588klJ0rvvvqv8/PwaPa+4uFjFxcXhx6FQKBrLAwAAAAAAHlGjAcrDDz9coxfz+Xz1OkA5XFlZWZoyZUqslwEAAAAAwBGL02HcISaG3LTWymo0QKm46059e+WVV8L3Da9w4MCBI37dzMxMjRs3Lvw4FAqpXbt2R/y6AAAAAADAmxr04Kdfv37Kzs6usv3lL3854tdNSEiQ3++vstXWC88v0rArLz/itdSFAwcOqE96T23etClq72Gp11KrZKvXUqtkq9dSq2Srl9bY4N9x3bLUKtnqtdQq2esFfqhBD1ASExPVuXPnKtuxxx4b62WpvLxckydNVObESeF9oVBIw39zlVo196v9sa2VNfXen3yNSMdnThivtq2a6+e90rRp48bw/u3btunU3unav39/eF+jRo00dtytumtSdG4FZqnXUqtkq9dSq2Sr11KrZKuXVm+2SrZ6LbVKtnottUr2eoHqNOgBSkO17LVX1TyluXr07BneN27MaOXt3atPt+3QG2++o2dnP6MFf53/o6/xU8evWb1aLy9ZrM1bv9Dwkb/VnRMnhJ83ZvSNmv7AQ2rSpEmV1/v1JZfqrRXLtWPHjjqutdVrqVWy1WupVbLVa6lVstVLqzdbJVu9llolW72WWiV7vW7j8/lct7mR6wcoGzduVHZ2tvbu3atgMBj+qk80LX15ic7qd3b4cVFRkV58YZEmT7lPycnJOuHEE3XDTaM1d87sap8f6fjt27epV+8+8vv96n/uAG3b9rkkadFzC9W6dRv1rfTeFRITE9W7zyla9upSemml13irtV5LrdZ6afVmq7VeS63Wei21WuwFquP6Acp5552njIwMvfzyy3rrrbeUkZGhjIyMqL5nTk62unQ5Kfz40y1bVFJSorT09PC+tLR0fbxhfbXPj3R89+49tG7tGuXn52vF8jfUvUdP5eXlaeb0abp/5oM/uq6TunbT+pzsI2qrjqVeS62SrV5LrZKtXkutkq1eWr3ZKtnqtdQq2eq11CrZ6wWqc1gDlHfeeUdXX321TjvtNH399deSpL/+9a9atWpVnS1s7ty5Wrx48SH7+/btK8dxlJycLEn64osv5DjOIVs05efnqVmlC88W7itUYmKi4uP/e1OjQCBZBQUF1T4/0vHdunfXTaPGaOA5ffXG6/9S1vQHNHHCeN1y6wRt2rRRg849W4MHnKN3f/Df2+/3Ky8/ry5TJdnqtdQq2eq11CrZ6rXUKtnqpdWbrZKtXkutkq1eS62SvV6gOjW6jXFlf//73/Wb3/xGw4YN00cffaTi4mJJUjAY1LRp0/Tqq6/W+SIbmuTkFBWEQuHHSYlJKioqUllZWfh/EEKhoJo1a1bt82ty/A03jdINN42SJK16Z6V27tyhoVcN04md2uv15W/LcRwNGnC2tmz9Ivz9sVAopJTkFHpppdd4q7VeS63Wemn1Zqu1Xkut1nottVrsdRufT4pz0WVFXHoJlNqfgXLffffpqaee0jPPPKPGjRuH959++ulat25dnS6uoUpLS9eWLZvDj0/s0kWNGzfW+pyc8L6cnGx179GzuqfX6viSkhKNHzdWj856Qrt379aBsjJ16NhRHTt1UmlJiXbv3h0+dvOmjTo5Lb0OCquy1GupVbLVa6lVstVrqVWy1UurN1slW72WWiVbvZZaJXu9QHVqPUDZsmWLfvnLXx6yPxAIKD8/vy7W1OCdN+R8vf3Wm+HHTZs21aWXXaF77p6kYDCorZ99picfn6WR115X7fNrc/zM6Vm6+NLL1KlzZx1zzDEqLi7W+pwcbVi/XiUlJWrRooWkgxdlWrtmtQYOPo9eWuk13mqt11KrtV5avdlqrddSq7VeS60We4FqObXUoUMH5/XXX3ccx3GSkpKczz//3HEcx5k3b57TtWvX2r5cgxAMBh1Jzq49Qef7UifiVri/zGl//PHOmo82hPft2hN0LrtiqJOUlOS0bNnSmTR5SpXnDBg4yJly79QaH/99qePkfLzZ6dWrtxMqKgnvm79gkdMmNdVJbdvWWbDoxfD+2XPmO0POv6BG66/tZqnXUqu1Xkut1nottVrrpdWbrdZ6LbVa67XU6ubeXXsOftYLBoOx/tgZFRWfZW98brXzx39ucs1243OrXfl38TlO7a64mpWVpb/97W969tlnde655+rVV1/Vl19+qT/+8Y+aNGmSRo8eHY05T1SFQiEFAgHt2hOUv9KFkX7K84ue08tLFutvC5+P8uoiKy8v16m90zV/wSJ17dYtKu9hqddSq2Sr11KrZKvXUqtkq5fW2ODfcd2y1CrZ6rXUKrmzNxQKqXWLgILBmn/Wc5OKz7I3PrdaCU2TYr2cGisuKtQTV57iur9LrQcojuNo2rRpysrKUlFRkSQpISFBt956q+69996oLDLaDmeAAgAAAABo2BigNExuHaDU+i48Pp9Pd9xxh8aPH6+tW7eqsLBQ3bp1U1KSe/5YAAAAAAAAtVHrAUqFo446St2idGoYAAAAAACoGZ/PF761sxu4aa2V1XqA0q9fv5+MXbFixREtCAAAAAAAoKGp9QAlPT29yuPS0lJlZ2fr448/1vDhw+tqXQAAAAAAAA1GrQcoDz/8cLX77777bhUWFh7xggAAAAAAQM3F+Q5ubuGmtVYWV1cvdPXVV+vZZ5+tq5cDAAAAAABoMOpsgPL++++rSZMmdfVyAAAAAAAADUatv8Jz8cUXV3nsOI6+/fZbrVmzRpMmTaqzhQEAAAAAADQUtR6gBAKBKo/j4uLUpUsX3XPPPRowYECdLQwAAAAAAETm8x3c3MJNa62sVgOUAwcOaOTIkerZs6dSUlKitSYAAAAAAIAGpVbXQGnUqJEGDBig/Pz8KC0HAAAAAACg4an1V3h69Oihbdu2qUOHDtFYDwAAAAAAqIU4n09xLvpejJvWWlmt78Jz33336dZbb9Urr7yib7/9VqFQqMoGAAAAAADgNTU+A+Wee+7RLbfcovPOO0+SdMEFF8hXaWrkOI58Pp8OHDhQ96sEAAAAAACIoRoPUKZMmaI//OEPevPNN6O5HgAAAAAAgAanxgMUx3EkSWeddVbUFgMAAAAAAGonTodxfY4YctNaK6vVun0uvdALAAAAAADAkajVXXhOPPHEiEOUvXv3HtGCAAAAAAAAGppaDVCmTJmiQCAQrbUAAAAAAAA0SLUaoAwdOlStWrWK1loAAAAAAEAt+XwHN7dw01orq/E1ULj+CQAAAAAAsKrGA5SKu/AAAAAAAABYU+Ov8JSXl0dzHQAAAAAA4DDEyac4F31rJE7uWWtlbr39MgAAAAAAQL1hgAIAAAAAABABAxQAAAAAAIAIanUbYwAAAAAA0LBwG+P6wRkoAAAAAAAAETBAAQAAAAAAiICv8AAAAAAA4GJxvoObW7hprZVxBgoAAAAAAEAEDFAAAAAAAAAiYIACAAAAAAAQAddAAQAAAADAxXw+Kc5F9wZ20VKr4AyUGCgtLdXYm0cptWWK2rZqrj+OGa2ysrJYLytqLPVaapVs9dLqzVbJVq+lVslWr6VWyVYvrd5slWz1WmqFtzFAiYH7p92n999dpXXrN2ptzid6b9U7mnH/tFgvK2os9VpqlWz10urNVslWr6VWyVavpVbJVi+t3myVbPVaaoW3MUCJgXlzn9WEiXcqNTVVqampui3zDs2dMzvWy4oaS72WWiVbvbR6s1Wy1WupVbLVa6lVstVLqzdbJVu9llrhbQxQ6lleXp6+/uorpaWlh/elpaVr544dCgaDsVtYlFjqtdQq2eql1Zutkq1eS62SrV5LrZKtXlq92SrZ6rXUGks+n/s2N2KAUs8KCwslSYHk5PC+ip8LCgpisKLostRrqVWy1UvrwZ+91irZ6rXUKtnqtdQq2eql9eDPXmuVbPVaaoX3MUCpZ0lJSZKkUKVpa8XPzZo1i8maoslSr6VWyVYvrd5slWz1WmqVbPVaapVs9dLqzVbJVq+lVngfA5R6lpKSomOPO045OdnhfTk52TquXTsFAoHYLSxKLPVaapVs9dLqzVbJVq+lVslWr6VWyVYvrd5slWz1WmqNpTif+zY3YoASA9cMH6kZWVOVm5ur3Nxczbx/mkZee12slxU1lnottUq2emn1Zqtkq9dSq2Sr11KrZKuXVm+2SrZ6LbXC2+JjvYDaGjFihPLz87V48WKtXLlSM2fO1Nq1a/Xtt9/qpZde0kUXXRTrJUaUecck7d2zRxk9u0qShl51tW67fWKMVxU9lnottUq2emn1Zqtkq9dSq2Sr11KrZKuXVm+2SrZ6LbXC23yO4zixXkRtVB6gvPbaa3r33XfVu3dvXXzxxYc9QAmFQgoEAtq1Jyi/31/3iwYAAAAA1LtQKKTWLQIKBr35Wa/is+yd/1ynJonuuabM/n0Fuu/CXq77u7juDJTKBg8erMGDB8d6GQAAAAAAxIzvP//HLdy01spcPUA5XMXFxSouLg4/DoVCMVwNAAAAAABo6ExeRDYrK0uBQCC8tWvXLtZLAgAAAAAADZjJAUpmZqaCwWB427lzZ6yXBAAAAADAYYn1LYm5jbGHJSQkyO/3V9lq64XnF2nYlZdHYXW1d+DAAfVJ76nNmzZF7T0s9VpqlWz1WmqVbPVaapVs9dIaG/w7rluWWiVbvZZaJXu9wA+ZHKAcqfLyck2eNFGZEyeF94VCIQ3/zVVq1dyv9se2VtbUe3/yNSIdnzlhvNq2aq6f90rTpo0bw/u3b9umU3una//+/eF9jRo10thxt+quSdG5FZilXkutkq1eS62SrV5LrZKtXlq92SrZ6rXUKtnqtdQq2esFquPqAUphYaGys7OVnZ0tSdq+fbuys7O1Y8eOqL7vstdeVfOU5urRs2d437gxo5W3d68+3bZDb7z5jp6d/YwW/HX+j77GTx2/ZvVqvbxksTZv/ULDR/5Wd06cEH7emNE3avoDD6lJkyZVXu/Xl1yqt1Ysj0q7pV5LrZKtXkutkq1eS62SrV5avdkq2eq11CrZ6rXUKtnrBarj6gHKmjVrlJGRoYyMDEnSuHHjlJGRobvuuiuq77v05SU6q9/Z4cdFRUV68YVFmjzlPiUnJ+uEE0/UDTeN1tw5s6t9fqTjt2/fpl69+8jv96v/uQO0bdvnkqRFzy1U69Zt1LfSe1dITExU7z6naNmrS+mllV7jrdZ6LbVa66XVm63Wei21Wuu11Gqx121ifT0TroHSQM2dO1eLFy+WJPXt21eO4xyyzZ07N6pryMnJVpcuJ4Uff7pli0pKSpSWnh7el5aWro83rK/2+ZGO7969h9atXaP8/HytWP6Guvfoqby8PM2cPk33z3zwR9d1UtduWp+TfURt1bHUa6lVstVrqVWy1WupVbLVS6s3WyVbvZZaJVu9llole71AdVw3QGkI8vPz1KzShWcL9xUqMTFR8fHx4X2BQLIKCgqqfX6k47t1766bRo3RwHP66o3X/6Ws6Q9o4oTxuuXWCdq0aaMGnXu2Bg84R++uWlXldf1+v/Ly8+oyVZKtXkutkq1eS62SrV5LrZKtXlq92SrZ6rXUKtnqtdQq2esFqhMf+RD8UHJyigpCofDjpMQkFRUVqaysLPw/CKFQUM2aNav2+TU5/oabRumGm0ZJkla9s1I7d+7Q0KuG6cRO7fX68rflOI4GDThbW7Z+IZ/P95/XCCklOYVeWuk13mqt11KrtV5avdlqrddSq7VeS60We93G5/OF/5u4gZvWWhlnoByGtLR0bdmyOfz4xC5d1LhxY63PyQnvy8nJVvcePat7eq2OLykp0fhxY/XorCe0e/duHSgrU4eOHdWxUyeVlpRo9+7d4WM3b9qok9PS66CwKku9llolW72WWiVbvZZaJVu9tHqzVbLVa6lVstVrqVWy1wtUhwHKYThvyPl6+603w4+bNm2qSy+7QvfcPUnBYFBbP/tMTz4+SyOvva7a59fm+JnTs3TxpZepU+fOOuaYY1RcXKz1OTnasH69SkpK1KJFC0kHL8q0ds1qDRx8Hr200mu81VqvpVZrvbR6s9Var6VWa72WWi32AtVy4ASDQUeSs2tP0Pm+1Im4Fe4vc9off7yz5qMN4X279gSdy64Y6iQlJTktW7Z0Jk2eUuU5AwYOcqbcO7XGx39f6jg5H292evXq7YSKSsL75i9Y5LRJTXVS27Z1Fix6Mbx/9pz5zpDzL6jR+mu7Weq11Gqt11KrtV5LrdZ6afVmq7VeS63Wei21url3156Dn/WCwWCsP3ZGRcVn2XteyXZmvPm5a7Z7Xsl25d/F5ziOE9sRTuyFQiEFAgHt2hOUv9KFkX7K84ue08tLFutvC5+P8uoiKy8v16m90zV/wSJ17dYtKu9hqddSq2Sr11KrZKvXUqtkq5fW2ODfcd2y1CrZ6rXUKrmzNxQKqXWLgILBmn/Wc5OKz7L3Lc1Wk8Tqrz/TEO3fV6A7f5Xuur8LAxQd3gAFAAAAANCwMUBpmNw6QOEaKAAAAAAAABEwQAEAAAAAAIggPtYLAAAAAAAAh8/nO7i5hZvWWhlnoAAAAAAAAETAAAUAAAAAACACvsIDAAAAAICLxfl8inPR92LctNbKOAMFAAAAAAAgAgYoAAAAAAAAETBAAQAAAAAAiIBroAAAAAAA4GJxvoObW7hprZVxBgoAAAAAAEAEDFAAAAAAAAAi4Cs8AAAAAAC4mU9y1Z2B3bTWSjgDBQAAAAAAIAIGKAAAAAAAABEwQAEAAAAAAIiAa6AAAAAAAOBicfIpzkUXFnHTWivjDBQAAAAAANBgff3117r66qvVokULHX300erZs6fWrFkT/r3jOLrrrruUmpqqo48+Wv3799dnn31W5+tggAIAAAAAABqkvLw8nX766WrcuLFee+01bdy4UQ8++KBSUlLCx8yYMUOPPfaYnnrqKX3wwQdKTEzUwIEDtX///jpdC1/hAQAAAADAxXwuu41xxVpDoVCV/QkJCUpISKiyb/r06WrXrp3mzJkT3tehQ4fwz47j6JFHHtGdd96pCy+8UJI0f/58tW7dWosXL9bQoUPrbN2cgQIAAAAAAOpdu3btFAgEwltWVtYhxyxZskR9+vTRZZddplatWikjI0PPPPNM+Pfbt29Xbm6u+vfvH94XCAR06qmn6v3336/T9XIGCgAAAAAAqHc7d+6U3+8PP/7h2SeStG3bNj355JMaN26cJk6cqNWrV+vmm2/WUUcdpeHDhys3N1eS1Lp16yrPa926dfh3dYUBCgAAAAAAqHd+v7/KAKU65eXl6tOnj6ZNmyZJysjI0Mcff6ynnnpKw4cPr49lhvEVHgAAAAAAXCzO576tplJTU9WtW7cq+7p27aodO3ZIktq0aSNJ2rVrV5Vjdu3aFf5dXWGAAgAAAAAAGqTTTz9dW7ZsqbLv008/Vfv27SUdvKBsmzZttHz58vDvQ6GQPvjgA5122ml1uha+wgMAAAAAABqkP/7xj/rFL36hadOm6fLLL9eHH36op59+Wk8//bQkyefzaezYsbrvvvt0wgknqEOHDpo0aZLatm2riy66qE7XwgAFAAAAAAA0SKeccopeeuklZWZm6p577lGHDh30yCOPaNiwYeFjbrvtNu3bt0+/+93vlJ+frzPOOEPLli1TkyZN6nQtPsdxnDp9RRcKhUIKBALatScY8QI2AAAAAAB3CIVCat0ioGDQm5/1Kj7LPvLGBh2d2CzWy6mx7/cVaGz/nq77u3ANFAAAAAAAgAgYoAAAAAAAAETANVAAAAAAAHAxn+/g5hZuWmtlnIECAAAAAAAQAQMUAAAAAACACBigxEBpaanG3jxKqS1T1LZVc/1xzGiVlZXFellRY6nXUqtkq5dWb7ZKtnottUq2ei21SrZ6afVmq2Sr11IrvI0BSgzcP+0+vf/uKq1bv1Frcz7Re6ve0Yz7p8V6WVFjqddSq2Srl1Zvtkq2ei21SrZ6LbVKtnpp9WarZKvXUmusxMmnOJ+LNrnzIigMUGJg3txnNWHinUpNTVVqaqpuy7xDc+fMjvWyosZSr6VWyVYvrd5slWz1WmqVbPVaapVs9dLqzVbJVq+lVngbA5R6lpeXp6+/+kppaenhfWlp6dq5Y4eCwWDsFhYllnottUq2emn1Zqtkq9dSq2Sr11KrZKuXVm+2SrZ6LbXC+xig1LPCwkJJUiA5Obyv4ueCgoIYrCi6LPVaapVs9dJ68GevtUq2ei21SrZ6LbVKtnppPfiz11olW72WWmOp4jbGbtrciAFKPUtKSpIkhSpNWyt+btasWUzWFE2Wei21SrZ6afVmq2Sr11KrZKvXUqtkq5dWb7ZKtnottcL7GKDUs5SUFB173HHKyckO78vJydZx7dopEAjEbmFRYqnXUqtkq5dWb7ZKtnottUq2ei21SrZ6afVmq2Sr11IrvI8BSgxcM3ykZmRNVW5urnJzczXz/mkaee11sV5W1FjqtdQq2eql1Zutkq1eS62SrV5LrZKtXlq92SrZ6rXUCm+Lj/UCamvEiBHKz8/X4sWLtXLlSs2cOVNr167Vt99+q5deekkXXXRRrJcYUeYdk7R3zx5l9OwqSRp61dW67faJMV5V9FjqtdQq2eql1Zutkq1eS62SrV5LrZKtXlq92SrZ6rXUGitxctfZEW5aa2U+x3GcWC+iNioPUF577TW9++676t27ty6++OLDHqCEQiEFAgHt2hOU3++v+0UDAAAAAOpdKBRS6xYBBYPe/KxX8Vn2iRUf6+gk91xT5vvCAt14dg/X/V1cdwZKZYMHD9bgwYNjvQwAAAAAAOBxrh6gHK7i4mIVFxeHH4dCoRiuBgAAAACAw+fz+eRz0b2B3bTWytz61aMjkpWVpf/P3n2HNXW2YQC/wxCV7QIU3BsVEGfdC/ceddXVpVVbt2LdVnG2tX7aYVtHq+Kqe++9FVyICwUHiAgJQ/b7/YEGImgchJC898/rXBc5OefkuT3JSfLkDFtbW/Xg4uKi75KIiIiIiIiIKBeTsoHi7e0NpVKpHkJCQvRdEhERERERERHlYlI2UCwsLGBjY6MxvK/163zRp1cPHVT3/lJSUlDDvSpuBgTo7DFkyitTVkCuvDJlBeTKK1NWQK68zKoffB5nL5myAnLllSkrIF9eotdJ2UD5WKmpqZg6eSK8J05Wj1OpVOj/WW8UKWCDEsUc4DNr5luXoW167/FjUbRIAdSq7oaAGzfU44Pu3UNtT3fEx8erx5mammLEqDGYMlk3lwKTKa9MWQG58sqUFZArr0xZAbnyMqtxZgXkyitTVkCuvDJlBeTLa2gUBjgYJGFg+vfvLzp27CiEECI6OlpcvnxZXL58WQAQP/74o7h8+bJ48ODBey1TqVQKACIsQileJAmtw6Yt20X16p4a4/r07SdaeLUUT8IjxZXrgcLZxUX8+ffKNy7jbdMfP3VOlClbVoRFKMWCHxeJNm3bqedr4dVS7N53MNPynkXFCGtraxF498E7ZXifQaa8MmWVLa9MWWXLK1NW2fIyq3FmlS2vTFllyytTVkPOGxaR9l1PqVTq4Nup/r36Lvvb4eti5flggxl+O3zdINeLwe2BkpqaCjOztIsHXbhwAR4eHvDw8AAAjBo1Ch4eHpgyZYpOa9i5fRsaNWmqvh0XF4cN630xdfoPsLOzQ7ny5TFk6HCsWP5XlvNrmz4o6B6qe9aAjY0Nmrfwwr17dwEAvmvXwMHBEY0zPPYrlpaW8KxRE3t27WReZmVeybPKllemrLLlZVbjzCpbXpmyypZXpqwy5iXKisE1UJ4+fQpHR0cAQOPGjSGEyDSsWLFCpzX4+/uhQoWK6tu3AgORmJgIN3d39Tg3N3dcu3oly/m1Te/qWgWXLl5AVFQUDh08ANcqVREZGYn5c2djzvyFb6yrYqXKuOLv91HZsiJTXpmyAnLllSkrIFdembICcuVlVuPMCsiVV6asgFx5ZcoKyJeXKCsG00CJjIzEjh07cOTIETRv3lyvtURFRcI6w4lnY2JjYGlpqd4zBgBsbe0QHR2d5fzapq/s6oqhw75Dy2aNcWD/XvjMXYCJ48di9JjxCAi4gVYtmqK1VzOcPHFCY7k2NjaIjIrMzqgA5MorU1ZArrwyZQXkyitTVkCuvMxqnFkBufLKlBWQK69MWQH58hoaE4XC4AZDZKZ9ktxh0KBBOH/+PEaPHo2OHTvqtRY7O3tEq1Tq21aWVoiLi0NycrJ6g6BSKWFtbZ3l/O8y/ZChwzBk6DAAwInjxxASEoyevfugfJkS2H/wKIQQaOXVFIF37kPx8smnUqlgb2fPvMzKvJJnlS2vTFlly8usxplVtrwyZZUtr0xZZcxLlBWD2QNl8+bNePjwIWbNmqV+seiLm5s7AgNvqm+Xr1AB5ubmuOLvrx7n7+8H1ypVs5z/faZPTEzE2FEjsGjxUoSHhyMlORmlSpdG6TJlkJSYiPDwcPW0NwNuoJqbezYk1CRTXpmyAnLllSkrIFdembICcuVlVuPMCsiVV6asgFx5ZcoKyJeXKCsG00DJTdq0a4+jRw6rb+fPnx/dun+KGdMmQ6lU4s7t2/h1yWIMHPRFlvO/z/Tz5/qgS7fuKFO2LAoVKoSEhARc8ffH1StXkJiYiIIFCwJIOynTxQvn0bJ1G+ZlVuaVPKtseWXKKlteZjXOrLLllSmrbHllyipjXkOk78sSG/0ljAHDu4yxLrzvZYxj4pNFiZIlxYXLVzUuj9X9057CyspKFC5cWEyeOl1jHq+WrcT0mbPeefoXSUL4X7spqlf3FKq4RPW4Vat9haOTk3AqWlSs9t2gHv/X8lWiXfsO2X65MtnyypRVtrwyZZUtr0xZZcvLrMaZVba8MmWVLa9MWQ05ryyXMf7jyA3x74UQgxn+OHLDINeLQggh9NvC0T+VSgVbW1uERShhk+HESG+zzncttm/bgn/XrNNxddqlpqaitqc7Vq32RaXKlXXyGDLllSkrIFdembICcuWVKSsgV15m1Q8+j7OXTFkBufLKlBUwzLwqlQoOBW2hVL77dz1D8uq77B9HbiC/Vdbnn8mN4mKi8VXjyga3XthAwYc1UIiIiIiIiCh3YwMldzLUBorBXIWHiIiIiIiIiDJTKNIGQ2FItWbEk8gSEREREREREWnBBgoRERERERERkRY8hIeIiIiIiIjIgCkUCigM6LgYQ6o1I+6BQkRERERERESkBRsoRERERERERERasIFCRERERERERKQFz4FCREREREREZMBMYFh7RxhSrRkZat1ERERERERERDmGDRQiIiIiIiIiIi3YQCEiIiIiIiIi0oLnQCEiIiIiIiIyYAqFAgqFQt9lvDNDqjUj7oFCRERERERERKQFGyhERERERERERFrwEB4iIiIiIiIiA6Z4ORgKQ6o1I+6BQkRERERERESkBRsoRERERERERERasIFCRERERERERKQFz4FCREREREREZMB4GeOcwT1QiIiIiIiIiIi0YAOFiIiIiIiIiEgLHsJDREREREREZMBMYFh7RxhSrRkZat1ERERERERERDmGDRQiIiIiIiIiIi3YQCEiIiIiIiIi0oLnQCEiIiIiIiIyYLyMcc7gHihERERERERERFqwgUJEREREREREpAUP4SEiIiIiIiIyYIqXg6EwpFoz4h4oRERERERERERasIFCRERERERERKQFGyhERERERERERFrwHChEREREREREBkyhSBsMhSHVmhH3QNGDpKQkjPh2GJwK26NokQIY+d1wJCcn67ssnZEpr0xZAbnyMqtxZgXkyitTVkCuvDJlBeTKy6zGmRWQK69MWcm4sYGiB3Nm/4DTJ0/g0pUbuOh/HadOHMe8ObP1XZbOyJRXpqyAXHmZ1TizAnLllSkrIFdembICcuVlVuPMCsiVV6asZNzYQNGDlSv+xviJk+Dk5AQnJyeM8/4eK5b/pe+ydEamvDJlBeTKy6zGmRWQK69MWQG58sqUFZArL7MaZ1ZArrwyZSXjxgZKDouMjMSjhw/h5uauHufm5o6Q4GAolUr9FaYjMuWVKSsgV15mNc6sgFx5ZcoKyJVXpqyAXHmZ1TizAnLllSmrPplAYXCDIWIDJYfFxMQAAGzt7NTjXv0dHR2th4p0S6a8MmUF5MrLrGl/G1tWQK68MmUF5MorU1ZArrzMmva3sWUF5MorU1Yyfmyg5DArKysAgCpDt/XV39bW1nqpSZdkyitTVkCuvMxqnFkBufLKlBWQK69MWQG58jKrcWYF5MorU1Yyfmyg5DB7e3sUc3aGv7+fepy/vx+cXVxga2urv8J0RKa8MmUF5MrLrMaZFZArr0xZAbnyypQVkCsvsxpnVkCuvDJl1adXlzE2pMEQsYGiB/36D8Q8n1kIDQ1FaGgo5s+ZjYGDvtB3WTojU16ZsgJy5WVW48wKyJVXpqyAXHllygrIlZdZjTMrIFdembKScTPTdwEy8v5+Mp5HRMCjaiUAQM/efTFuwkQ9V6U7MuWVKSsgV15mNc6sgFx5ZcoKyJVXpqyAXHmZ1TizAnLllSkrGTeFEELou4g3GTBgAKKiorBlyxb1uI0bN6Jv376YNWsWatasifnz5+PixYt48uQJNm/ejE6dOr3346hUKtja2iIsQgkbG5vsC0BERERERER6o1Kp4FDQFkqlcX7Xe/Vddt3p28hvZTjnlImLicandcsZ3HoxqEN4/vzzT/Tp0we//vorRo8ejdjYWLi5uWHJkiX6Lo2IiIiIiIhILxQG+M8QGcwhPPPmzcPUqVPh6+uLzp07AwBat26N1q1b67kyIiIiIiIiIjJ2BtFAGT9+PJYuXYodO3agWbNmH728hIQEJCQkqG+rVKqPXiYRERERERERGa9c30DZvXs3tm7dioMHD6Jp06bZskwfHx9Mnz49W5ZFREREREREpE+GdmlgQ6o1o1x/DpRq1aqhZMmSmDp1KmJiYrJlmd7e3lAqleohJCTkvZexfp0v+vTqkS31fKyUlBTUcK+KmwEBOnsMmfLKlBWQK69MWQG58sqUFZArL7PqB5/H2UumrIBceWXKCsiXl+h1ub6BUqxYMRw5cgSPHj1Cq1atEB0d/dHLtLCwgI2NjcbwPlJTUzF18kR4T5ysHqdSqdD/s94oUsAGJYo5wGfWzLcuQ9v03uPHomiRAqhV3Q0BN26oxwfdu4fanu6Ij49XjzM1NcWIUWMwZbJuLgUmU16ZsgJy5ZUpKyBXXpmyAnLlZVbjzArIlVemrIBceWXKCsiXlyhLIhfr37+/6NixoxBCiODgYFGmTBnxySefCJVKlWlaAGLz5s0f9DhKpVIAEGERSvEiSWgdNm3ZLqpX99QY16dvP9HCq6V4Eh4prlwPFM4uLuLPv1e+cRlvm/74qXOiTNmyIixCKRb8uEi0adtOPV8Lr5Zi976DmZb3LCpGWFtbi8C7D94pw/sMMuWVKatseWXKKltembLKlpdZjTOrbHllyipbXpmyGnLesIi073pKpfIjvpnmXq++y244c0fsvBZmMMOGM3cMcr3k+j1QXnFxccGRI0fw9OlTtGzZEiqVCjExMfDz84Ofnx8AICgoCH5+fggODtZpLTu3b0OjJunnY4mLi8OG9b6YOv0H2NnZoVz58hgydDhWLP8ry/m1TR8UdA/VPWvAxsYGzVt44d69uwAA37Vr4ODgiMZNMp8LxtLSEp41amLPrp3My6zMK3lW2fLKlFW2vMxqnFllyytTVtnyypRVxryGRgEFTAxoMNTLGBtMAwUAnJ2dceTIETx79gwtW7bEoUOH4OHhAQ8PDwDAqFGj4OHhgSlTpui0Dn9/P1SoUFF9+1ZgIBITE+Hm7q4e5+bmjmtXr2Q5v7bpXV2r4NLFC4iKisKhgwfgWqUqIiMjMX/ubMyZv/CNdVWsVBlX/P0+KltWZMorU1ZArrwyZQXkyitTVkCuvMxqnFkBufLKlBWQK69MWQH58hJlJVc3UFasWIEtW7ZojCtWrBhu3bqF06dPo0OHDhBCZBpWrFih07qioiJhneG8KTGxMbC0tISZWfpFjWxt7d54vhZt01d2dcXQYd+hZbPGOLB/L3zmLsDE8WMxesx4BATcQKsWTdHaqxlOnjihsVwbGxtERkVmZ1QAcuWVKSsgV16ZsgJy5ZUpKyBXXmY1zqyAXHllygrIlVemrIB8eYmykusvY5wb2dnZI1qlUt+2srRCXFwckpOT1RsElUoJa2vrLOd/l+mHDB2GIUOHAQBOHD+GkJBg9OzdB+XLlMD+g0chhEArr6YIvHMfipfXgFKpVLC3s2deZmVeybPKllemrLLlZVbjzCpbXpmyypZXpqwy5jU0vIxxzsjVe6DkVm5u7ggMvKm+Xb5CBZibm+OKv796nL+/H1yrVM1y/veZPjExEWNHjcCixUsRHh6OlORklCpdGqXLlEFSYiLCw8PV094MuIFqbu7ZkFCTTHllygrIlVemrIBceWXKCsiVl1mNMysgV16ZsgJy5ZUpKyBfXqKssIHyAdq0a4+jRw6rb+fPnx/dun+KGdMmQ6lU4s7t2/h1yWIMHPRFlvO/z/Tz5/qgS7fuKFO2LAoVKoSEhARc8ffH1StXkJiYiIIFCwJIOynTxQvn0bJ1G+ZlVuaVPKtseWXKKlteZjXOrLLllSmrbHllyipjXqIs5exFf3Kn972McUx8sihRsqS4cPmqxuWxun/aU1hZWYnChQuLyVOna8zj1bKVmD5z1jtP/yJJCP9rN0X16p5CFZeoHrdqta9wdHISTkWLitW+G9Tj/1q+SrRr3yHbL1cmW16ZssqWV6assuWVKatseZnVOLPKllemrLLllSmrIeeV5TLGm87eFXuuPzWYYdPZuwa5XhRCCKHfFo7+qVQq2NraIixCCZsMJ0Z6m3W+a7F92xb8u2adjqvTLjU1FbU93bFqtS8qVa6sk8eQKa9MWQG58sqUFZArr0xZAbnyMqt+8HmcvWTKCsiVV6asgGHmValUcChoC6Xy3b/rGZJX32X/O3cXllZZn38mN4qNiUaXWmUMbr2wgYIPa6AQERERERFR7sYGSu5kqA0UngOFiIiIiIiIiEgLNlCIiIiIiIiIiLQw03cBRERERERERPThFC//GQpDqjUj7oFCRERERERERKQFGyhERERERERERFrwEB4iIiIiIiIiA2aiSBsMhSHVmhH3QCEiIiIiIiIi0oINFCIiIiIiIiIiLdhAISIiIiIiIiLSgudAISIiIiIiIjJgvIxxzuAeKEREREREREREWrCBQkRERERERESkBQ/hISIiIiIiIjJgCkXaYCgMqdaMuAcKEREREREREZEWbKAQEREREREREWnBBgoRERERERERkRY8BwoRERERERGRAVPAsC4NbDiVauIeKEREREREREREWrCBQkRERERERESkBRsoRERERERERERasIFCREREREREZMBMFIY3fKg5c+ZAoVBgxIgR6nHx8fEYOnQoChYsCCsrK3Tt2hVhYWEf/x/7GjZQiIiIiIiIiCjXO3/+PH7//XdUq1ZNY/zIkSOxfft2bNiwAUePHsXjx4/RpUuXbH98NlCIiIiIiIiIKFeLiYlBnz59sGzZMtjb26vHK5VK/PXXX/jxxx/RtGlTeHp6Yvny5Th16hTOnDmTrTWwgUJERERERERkwBQG+A8AVCqVxpCQkPDGjEOHDkXbtm3RvHlzjfEXL15EUlKSxviKFSuiePHiOH36dLb+P7OBQkREREREREQ5zsXFBba2turBx8cny+l8fX1x6dKlLO8PDQ1Fnjx5YGdnpzHewcEBoaGh2VqvWbYujYiIiIiIiIjoHYSEhMDGxkZ928LCIstpvvvuO+zfvx958+bNyfIy4R4oRERERERERJTjbGxsNIasGigXL17E06dPUb16dZiZmcHMzAxHjx7FL7/8AjMzMzg4OCAxMRFRUVEa84WFhcHR0TFb6+UeKEREREREREQGTKFIGwzF+9TarFkzXL16VWPcwIEDUbFiRYwfPx4uLi4wNzfHwYMH0bVrVwBAYGAggoODUbdu3ewsmw0UIiIiIiIiIsqdrK2tUaVKFY1xlpaWKFiwoHr8559/jlGjRqFAgQKwsbHB8OHDUbduXdSpUydba2EDhYiIiIiIiIgM1k8//QQTExN07doVCQkJaNmyJZYuXZrtj8MGChEREREREZEBU7wcDMXH1nrkyBGN23nz5sWSJUuwZMmSj1zy2/EkskREREREREREWrCBQkRERERERESkBRsoRERERERERERasIGiB0lJSRjx7TA4FbZH0SIFMPK74UhOTtZ3WTojU16ZsgJy5WVW48wKyJVXpqyAXHllygrIlZdZjTMrIFdembLqiwkUMFEY0GBQZ2xJxwaKHsyZ/QNOnzyBS1du4KL/dZw6cRzz5szWd1k6I1NembICcuVlVuPMCsiVV6asgFx5ZcoKyJWXWY0zKyBXXpmyknFjA0UPVq74G+MnToKTkxOcnJwwzvt7rFj+l77L0hmZ8sqUFZArL7MaZ1ZArrwyZQXkyitTVkCuvMxqnFkBufLKlJWMGxsoOSwyMhKPHj6Em5u7epybmztCgoOhVCr1V5iOyJRXpqyAXHmZ1TizAnLllSkrIFdembICcuVlVuPMCsiVV6as+qQwwMEQsYGSw2JiYgAAtnZ26nGv/o6OjtZDRbolU16ZsgJy5WXWtL+NLSsgV16ZsgJy5ZUpKyBXXmZN+9vYsgJy5ZUpKxk/NlBymJWVFQBAlaHb+upva2trvdSkSzLllSkrIFdeZjXOrIBceWXKCsiVV6asgFx5mdU4swJy5ZUpKxk/NlBymL29PYo5O8Pf3089zt/fD84uLrC1tdVfYToiU16ZsgJy5WVW48wKyJVXpqyAXHllygrIlZdZjTMrIFdembKS8WMDRQ/69R+IeT6zEBoaitDQUMyfMxsDB32h77J0Rqa8MmUF5MrLrMaZFZArr0xZAbnyypQVkCsvsxpnVkCuvDJl1Rt9n9BEkpOgmOm7gHcxYMAAREVFYcuWLZnu++OPP7BmzRpcunQJ0dHRiIyMhF2G4+tyI+/vJ+N5RAQ8qlYCAPTs3RfjJkzUc1W6I1NembICcuVlVuPMCsiVV6asgFx5ZcoKyJWXWY0zKyBXXpmyknFTCCGEvovQ5m0NlJ9//hnx8fEAAG9v7w9qoKhUKtja2iIsQgkbG5tsqJiIiIiIiIj0TaVSwaGgLZRK4/yu9+q77IHLD2BpbTj5YqNVaO5RwuDWi0HsgfI2I0aMAAAcOXJEr3UQERERERERkfEy+AbKh0hISEBCQoL6tkql0mM1RERERERERB9O8fKfoTCkWjOS8iSyPj4+sLW1VQ8uLi76LomIiIiIiIiIcjEpGyje3t5QKpXqISQkRN8lEREREREREVEuJmUDxcLCAjY2NhrD+1q/zhd9evXQQXXvLyUlBTXcq+JmQIDOHkOmvDJlBeTKK1NWQK68MmUF5MrLrPrB53H2kikrIFdembIC8uU1KApAYUCDgR7BI2cD5WOlpqZi6uSJ8J44WT1OpVKh/2e9UaSADUoUc4DPrJlvXYa26b3Hj0XRIgVQq7obAm7cUI8PuncPtT3d1VceAgBTU1OMGDUGUybr5lJgMuWVKSsgV16ZsgJy5ZUpKyBXXmY1zqyAXHllygrIlVemrIB8eYmyJAxA//79RePGjcXly5c1huDgYPHkyRNx+fJlsWzZMgFAHDt2TFy+fFlERES88/KVSqUAIMIilOJFktA6bNqyXVSv7qkxrk/ffqKFV0vxJDxSXLkeKJxdXMSff6984zLeNv3xU+dEmbJlRViEUiz4cZFo07ader4WXi3F7n0HMy3vWVSMsLa2FoF3H7xThvcZZMorU1bZ8sqUVba8MmWVLS+zGmdW2fLKlFW2vDJlNeS8YRFp3/WUSqUOv7Hqz6vvsgf9gsXZu1EGMxz0CzbI9WIwe6AcOXIEHh4eGsP06dPx22+/wcPDA19++SUAoGHDhvDw8MC2bdt0VsvO7dvQqElT9e24uDhsWO+LqdN/gJ2dHcqVL48hQ4djxfK/spxf2/RBQfdQ3bMGbGxs0LyFF+7duwsA8F27Bg4Ojmic4bFfsbS0hGeNmtizayfzMivzSp5VtrwyZZUtL7MaZ1bZ8sqUVba8MmWVMS9RVgyigbJixQoIITINf/75J6ZNm5blfQMGDNBZPf7+fqhQoaL69q3AQCQmJsLN3V09zs3NHdeuXslyfm3Tu7pWwaWLFxAVFYVDBw/AtUpVREZGYv7c2Zgzf+Eb66pYqTKu+Pt9VLasyJRXpqyAXHllygrIlVemrIBceZnVOLMCcuWVKSsgV16ZsgLy5TU0CgMcDJFBNFBym6ioSFhnOPFsTGwMLC0tYWZmph5na2uH6OjoLOfXNn1lV1cMHfYdWjZrjAP798Jn7gJMHD8Wo8eMR0DADbRq0RStvZrh5IkTGsu1sbFBZFRkdkYFIFdembICcuWVKSsgV16ZsgJy5WVW48wKyJVXpqyAXHllygrIl5coK2baJ6HX2dnZI1qlUt+2srRCXFwckpOT1RsElUoJa2vrLOd/l+mHDB2GIUOHAQBOHD+GkJBg9OzdB+XLlMD+g0chhEArr6YIvHMfCoXi5TJUsLezZ15mZV7Js8qWV6assuVlVuPMKltembLKllemrDLmJcoK90D5AG5u7ggMvKm+Xb5CBZibm+OKv796nL+/H1yrVM1y/veZPjExEWNHjcCixUsRHh6OlORklCpdGqXLlEFSYiLCw8PV094MuIFqbu7ZkFCTTHllygrIlVemrIBceWXKCsiVl1mNMysgV16ZsgJy5ZUpKyBfXoOj7+NxJDmGhw2UD9CmXXscPXJYfTt//vzo1v1TzJg2GUqlEndu38avSxZj4KAvspz/faafP9cHXbp1R5myZVGoUCEkJCTgir8/rl65gsTERBQsWBBA2kmZLl44j5at2zAvszKv5FllyytTVtnyMqtxZpUtr0xZZcsrU1YZ8xJlKceu95OLve9ljGPik0WJkiXFhctXNS6P1f3TnsLKykoULlxYTJ46XWMer5atxPSZs955+hdJQvhfuymqV/cUqrhE9bhVq32Fo5OTcCpaVKz23aAe/9fyVaJd+w7Zfrky2fLKlFW2vDJllS2vTFlly8usxplVtrwyZZUtr0xZDTmvLJcxPuQfLM7dizKY4ZC/YV7GWCGEEPpt4eifSqWCra0twiKUsMlwYqS3Wee7Ftu3bcG/a9bpuDrtUlNTUdvTHatW+6JS5co6eQyZ8sqUFZArr0xZAbnyypQVkCsvs+oHn8fZS6asgFx5ZcoKGGZelUoFh4K2UCrf/bueIXn1XfaQfzCsrA0nX0y0Ck3dihvcemEDBR/WQCEiIiIiIqLcTZYGymH/EINroDRxczG49cJzoBARERERERERacEGChERERERERGRFmb6LoCIiIiIiIiIPpxCkTYYCkOqNSPugUJEREREREREpAUbKEREREREREREWrCBQkRERERERESkBc+BQkRERERERGTAFC8HQ2FItWbEPVCIiIiIiIiIiLRgA4WIiIiIiIiISAs2UIiIiIiIiIiItOA5UIiIiIiIiIgMGU+CkiO4BwoRERERERERkRZsoBARERERERERacFDeIiIiIiIiIgMmOLlP0NhSLVmxD1QiIiIiIiIiIi0YAOFiIiIiIiIiEgLNlCIiIiIiIiIiLTgOVCIiIiIiIiIDJhCkTYYCkOqNSPugUJEREREREREpAUbKEREREREREREWvAQHiIiIiIiIiIDpng5GApDqjUj7oFCRERERERERKQFGyhERERERERERFqwgUJEREREREREpAXPgUJERERERERkyHgSlBzBPVCIiIiIiIiIiLRgA4WIiIiIiIiISAsewkNERERERERkwBQv/xkKQ6o1I+6BQkRERERERESkBRsoRERERERERERasIFCRERERERERKQFz4FCREREREREZMAUirTBUBhSrRlxDxQiIiIiIiIiIi3YQNGDpKQkjPh2GJwK26NokQIY+d1wJCcn67ssnZEpr0xZAbnyMqtxZgXkyitTVkCuvDJlBeTKy6zGmRWQK69MWcm4sYGiB3Nm/4DTJ0/g0pUbuOh/HadOHMe8ObP1XZbOyJRXpqyAXHmZ1TizAnLllSkrIFdembICcuVlVuPMCsiVV6asZNzYQNGDlSv+xviJk+Dk5AQnJyeM8/4eK5b/pe+ydEamvDJlBeTKy6zGmRWQK69MWQG58sqUFZArL7MaZ1ZArrwyZdUXhQEOhogNlBwWGRmJRw8fws3NXT3Ozc0dIcHBUCqV+itMR2TKK1NWQK68zGqcWQG58sqUFZArr0xZAbnyMqtxZgXkyitTVjJ+bKDksJiYGACArZ2detyrv6Ojo/VQkW7JlFemrIBceZk17W9jywrIlVemrIBceWXKCsiVl1nT/ja2rIBceWXKSsaPDZQcZmVlBQBQZei2vvrb2tpaLzXpkkx5ZcoKyJWXWY0zKyBXXpmyAnLllSkrIFdeZjXOrIBceWXKqlf6Ph5HkmN42EDJYfb29ijm7Ax/fz/1OH9/Pzi7uMDW1lZ/hemITHllygrIlZdZjTMrIFdembICcuWVKSsgV15mNc6sgFx5ZcpKxo8NFD3o138g5vnMQmhoKEJDQzF/zmwMHPSFvsvSGZnyypQVkCsvsxpnVkCuvDJlBeTKK1NWQK68zGqcWQG58sqUlYybmb4LkJH395PxPCICHlUrAQB69u6LcRMm6rkq3ZEpr0xZAbnyMqtxZgXkyitTVkCuvDJlBeTKy6zGmRWQK69MWcm4KYQQQt9FZDRgwACsXLkSX3/9NX777TeN+4YOHYqlS5eif//+WLFiBY4dO4b58+fj4sWLePLkCTZv3oxOnTq992OqVCrY2toiLEIJGxubbEpCRERERERE+qRSqeBQ0BZKpXF+13v1XfZMwGNYWRtOvphoFepUKmpw6yVXHsLj4uICX19fvHjxQj0uPj4ea9asQfHixdXjYmNj4ebmhiVLluijTCIiIiIiIiKSRK48hKd69eq4e/cu/vvvP/Tp0wcA8N9//6F48eIoVaqUerrWrVujdevW+iqTiIiIiIiIiCSRK/dAAYBBgwZh+fLl6tt///03Bg4cmC3LTkhIgEql0hiIiIiIiIiIDJFCYXiDIcq1DZS+ffvixIkTePDgAR48eICTJ0+ib9++2bJsHx8f2NraqgcXF5dsWS4RERERERERGadc20ApXLgw2rZtixUrVmD58uVo27YtChUqlC3L9vb2hlKpVA8hISHvvYz163zRp1ePbKnnY6WkpKCGe1XcDAjQ2WPIlFemrIBceWXKCsiVV6asgFx5mVU/+DzOXjJlBeTKK1NWQL68RK/LtQ0UIO0wnhUrVmDlypUYNGhQti3XwsICNjY2GsP7SE1NxdTJE+E9cbJ6nEqlQv/PeqNIARuUKOYAn1kz37oMbdN7jx+LokUKoFZ1NwTcuKEeH3TvHmp7uiM+Pl49ztTUFCNGjcGUybq5FJhMeWXKCsiVV6asgFx5ZcoKyJWXWY0zKyBXXpmyAnLllSkrIF9eoiyJXKZ///6iY8eOQgghkpOTRdGiRUWxYsVEcnKyEEKIjh07iv79+2eaD4DYvHnzBz2mUqkUAERYhFK8SBJah01btovq1T01xvXp20+08GopnoRHiivXA4Wzi4v48++Vb1zG26Y/fuqcKFO2rAiLUIoFPy4Sbdq2U8/Xwqul2L3vYKblPYuKEdbW1iLw7oN3yvA+g0x5ZcoqW16ZssqWV6assuVlVuPMKltembLKllemrIacNywi7bueUqn8wG+oudur77Lnbj4WNx7FGMxw7uZjg1wvuXoPFFNTUwQEBODGjRswNTXNdH9MTAz8/Pzg5+cHAAgKCoKfnx+Cg4N1WtfO7dvQqElT9e24uDhsWO+LqdN/gJ2dHcqVL48hQ4djxfK/spxf2/RBQfdQ3bMGbGxs0LyFF+7duwsA8F27Bg4Ojmic4bFfsbS0hGeNmtizayfzMivzSp5VtrwyZZUtL7MaZ1bZ8sqUVba8MmWVMS9RVnJ1AwXAWw+xuXDhAjw8PODh4QEAGDVqFDw8PDBlyhSd1uTv74cKFSqqb98KDERiYiLc3N3V49zc3HHt6pUs59c2vatrFVy6eAFRUVE4dPAAXKtURWRkJObPnY058xe+sa6KlSrjir/fR2XLikx5ZcoKyJVXpqyAXHllygrIlZdZjTMrIFdembICcuWVKSsgX16irOS6BsqKFSuwZcuWN96/ZcsWrFixAgDQuHFjCCEyDa/u15WoqEhYZ2jqxMTGwNLSEmZmZupxtrZ2iI6OznJ+bdNXdnXF0GHfoWWzxjiwfy985i7AxPFjMXrMeAQE3ECrFk3R2qsZTp44obFcGxsbREZFZmdUAHLllSkrIFdembICcuWVKSsgV15mNc6sgFx5ZcoKyJVXpqyAfHmJsmKmfRJ6nZ2dPaJVKvVtK0srxMXFITk5Wb1BUKmUsLa2znL+d5l+yNBhGDJ0GADgxPFjCAkJRs/efVC+TAnsP3gUQgi08mqKwDv3oXh5EW2VSgV7O3vmZVbmlTyrbHllyipbXmY1zqyy5ZUpq2x5ZcoqY16Do3g5GApDqjWDXLcHiiFwc3NHYOBN9e3yFSrA3NwcV/z91eP8/f3gWqVqlvO/z/SJiYkYO2oEFi1eivDwcKQkJ6NU6dIoXaYMkhITER4erp72ZsANVHNzz4aEmmTKK1NWQK68MmUF5MorU1ZArrzMapxZAbnyypQVkCuvTFkB+fISZSlnz1mbO73vVXg2bt4mqnvW0BjXu89nomWr1iL0WZS4euOWcCle/K1noH7X6SdNmSZm/DBbvEgSIiY+Wdjb24uzF/zEuYv+okCBAiImPlm8SBIiQhkrrK2txc0797P9jNsy5ZUpq2x5ZcoqW16ZssqWl1mNM6tseWXKKltembIacl5prsIT+FjceBxjMMO5QMO8Cg8bKOL9Gygx8cmiRMmS4sLlqxovzO6f9hRWVlaicOHCYvLU6RrzeLVsJabPnPXO079IEsL/2k1RvbqnUMUlqsetWu0rHJ2chFPRomK17wb1+L+WrxLt2nfI9g2lbHllyipbXpmyypZXpqyy5WVW48wqW16ZssqWV6ashpxXlgbK+cAnIuBxrMEM5wOfGOR6UQghhP72f8kdVCoVbG1tERahfOMVf163zncttm/bgn/XrNNxddqlpqaitqc7Vq32RaXKlXXyGDLllSkrIFdembICcuWVKSsgV15m1Q8+j7OXTFkBufLKlBUwzLwqlQoOBW2hVL77dz1D8uq77PnAJ7CyNpx8MdEq1KzgZHDrhQ0UfFgDhYiIiIiIiHI3NlByJ0NtoPAkskREREREREREWvAyxkREREREREQGTKFIGwyFIdWaEfdAISIiIiIiIiLSgg0UIiIiIiIiIiIteAgPERERERERkQFTvBwMhSHVmhH3QCEiIiIiIiIi0oINFCIiIiIiIiIiLdhAISIiIiIiIiLSgudAISIiIiIiIjJkPAlKjuAeKEREREREREREWrCBQkRERERERESkBQ/hISIiIiIiIjJgipf/DIUh1ZoR90AhIiIiIiIiItKCDRQiIiIiIiIiIi3YQCEiIiIiIiIi0oLnQCEiIiIiIiIyZApAYUinFTGkWjPgHihERERERERERFqwgUJEREREREREpAUbKEREREREREREWvAcKEREREREREQGTAHDOq2IIdWaEfdAISIiIiIiIiLSgg0UIiIiIiIiIiIt2EAhIiIiIiIiMmQKAxzekY+PD2rWrAlra2sUKVIEnTp1QmBgoMY08fHxGDp0KAoWLAgrKyt07doVYWFh7/4g74gNFCIiIiIiIiLKlY4ePYqhQ4fizJkz2L9/P5KSkuDl5YXY2Fj1NCNHjsT27duxYcMGHD16FI8fP0aXLl2yvRaeRJaIiIiIiIiIcqU9e/Zo3F6xYgWKFCmCixcvomHDhlAqlfjrr7+wZs0aNG3aFACwfPlyVKpUCWfOnEGdOnWyrRbugUJEREREREREOU6lUmkMCQkJWudRKpUAgAIFCgAALl68iKSkJDRv3lw9TcWKFVG8eHGcPn06W+tlA4WIiIiIiIjIgCkM8B8AuLi4wNbWVj34+Pi8NWdqaipGjBiBevXqoUqVKgCA0NBQ5MmTB3Z2dhrTOjg4IDQ0NFv/n3kIDxERERERERHluJCQENjY2KhvW1hYvHX6oUOH4tq1azhx4oSuS8sSGyhERERERERElONsbGw0GihvM2zYMOzYsQPHjh2Ds7OzeryjoyMSExMRFRWlsRdKWFgYHB0ds7VeHsJDREREREREZMAUCsMb3pUQAsOGDcPmzZtx6NAhlCpVSuN+T09PmJub4+DBg+pxgYGBCA4ORt26dbPrvxgA90AhIiIiIiIiolxq6NChWLNmDbZu3Qpra2v1eU1sbW2RL18+2Nra4vPPP8eoUaNQoEAB2NjYYPjw4ahbt262XoEHYAOFiIiIiIiIiHKpX3/9FQDQuHFjjfHLly/HgAEDAAA//fQTTExM0LVrVyQkJKBly5ZYunRpttfCBgoRERERERER5UpCCK3T5M2bF0uWLMGSJUt0WgsbKEREREREREQGTPFyMBSGVGtGPImsHiQlJWHEt8PgVNgeRYsUwMjvhiM5OVnfZemMTHllygrIlZdZjTMrIFdembICcuWVKSsgV15mNc6sgFx5ZcpKxo0NFD2YM/sHnD55Apeu3MBF/+s4deI45s2Zre+ydEamvDJlBeTKy6zGmRWQK69MWQG58sqUFZArL7MaZ1ZArrwyZSXjxgaKHqxc8TfGT5wEJycnODk5YZz391ix/C99l6UzMuWVKSsgV15mNc6sgFx5ZcoKyJVXpqyAXHmZ1TizAnLllSmr3igMcDBAbKDksMjISDx6+BBubu7qcW5u7ggJDoZSqdRfYToiU16ZsgJy5WVW48wKyJVXpqyAXHllygrIlZdZjTMrIFdembKS8WMDJYfFxMQAAGzt7NTjXv0dHR2th4p0S6a8MmUF5MrLrGl/G1tWQK68MmUF5MorU1ZArrzMmva3sWUF5MorU1Yyfmyg5DArKysAgCpDt/XV39bW1nqpSZdkyitTVkCuvMxqnFkBufLKlBWQK69MWQG58jKrcWYF5MorU1Yyfmyg5DB7e3sUc3aGv7+fepy/vx+cXVxga2urv8J0RKa8MmUF5MrLrMaZFZArr0xZAbnyypQVkCsvsxpnVkCuvDJl1SeFAf4zRGyg6EG//gMxz2cWQkNDERoaivlzZmPgoC/0XZbOyJRXpqyAXHmZ1TizAnLllSkrIFdembICcuVlVuPMCsiVV6asZNzM9F3AmwwYMAArV66Ej48PJkyYoB6/ZcsWdO7cGUIIxMfHY/Dgwbh48SICAgLQrl07bNmyRX9FvyPv7yfjeUQEPKpWAgD07N0X4yZM1HNVuiNTXpmyAnLlZVbjzArIlVemrIBceWXKCsiVl1mNMysgV16ZspJxUwghhL6LyMqAAQOwbt065M2bF/fu3YO9vT0AzQZKbGwsxowZg+rVq2PTpk3ImzfvBzVQVCoVbG1tERahhI2NTTYnISIiIiIiIn1QqVRwKGgLpdI4v+u9+i57NegprK0NJ190tApVSxUxuPWSqw/had68ORwdHeHj45Pl/ZaWlvj111/x5ZdfwtHRMYerIyIiIiIiItI/BQCFwoAGff+HfaBc3UAxNTXF7NmzsXjxYjx8+DDblpuQkACVSqUxEBERERERERG9Sa5uoABA586d4e7ujqlTp2bbMn18fGBra6seXFxcsm3ZRERERERERGR8cn0DBQDmzp2LlStXIiAgIFuW5+3tDaVSqR5CQkKyZblEREREREREOU1hgIMhMogGSsOGDdGyZUt4e3tny/IsLCxgY2OjMbyv9et80adXj2yp52OlpKSghntV3MymBlNWZMorU1ZArrwyZQXkyitTVkCuvMyqH3weZy+ZsgJy5ZUpKyBfXqLXGUQDBQDmzJmD7du34/Tp0/ouBampqZg6eSK8J05Wj1OpVOj/WW8UKWCDEsUc4DNr5luXoW167/FjUbRIAdSq7oaAGzfU44Pu3UNtT3fEx8erx5mammLEqDGYMlk3lwKTKa9MWQG58sqUFZArr0xZAbnyMqtxZgXkyitTVkCuvDJlBeTLS5QlkUv1799fdOzYUWPcZ599JvLmzSsyln39+nVx+fJl0b59e9G4cWNx+fJlcfny5fd6LKVSKQCIsAileJEktA6btmwX1at7aozr07efaOHVUjwJjxRXrgcKZxcX8effK9+4jLdNf/zUOVGmbFkRFqEUC35cJNq0baeer4VXS7F738FMy3sWFSOsra1F4N0H75ThfQaZ8sqUVba8MmWVLa9MWWXLy6zGmVW2vDJllS2vTFkNOW9YRNp3PaVSmQ3fUnOfV99lrwc9FcER8QYzXA96apDrxWD2QAGAGTNmIDU1VWNcmzZt4OHhge3bt+PIkSPw8PCAh4eHTuvYuX0bGjVpqr4dFxeHDet9MXX6D7Czs0O58uUxZOhwrFj+V5bza5s+KOgeqnvWgI2NDZq38MK9e3cBAL5r18DBwRGNMzz2K5aWlvCsURN7du1kXmZlXsmzypZXpqyy5WVW48wqW16ZssqWV6asMuY1NHq/LPEHDIYo1zZQVqxYgS1btmiMK1myJBISEiCEUI+7f/8+hBCZBl3y9/dDhQoV1bdvBQYiMTERbu7u6nFubu64dvVKlvNrm97VtQouXbyAqKgoHDp4AK5VqiIyMhLz587GnPkL31hXxUqVccXf76OyZUWmvDJlBeTKK1NWQK68MmUF5MrLrMaZFZArr0xZAbnyypQVkC8vUVZybQMlN4uKioR1hhPPxsTGwNLSEmZmZupxtrZ2iI6OznJ+bdNXdnXF0GHfoWWzxjiwfy985i7AxPFjMXrMeAQE3ECrFk3R2qsZTp44obFcGxsbREZFZmdUAHLllSkrIFdembICcuWVKSsgV15mNc6sgFx5ZcoKyJVXpqyAfHmJsmKmfRJ6nZ2dPaJVKvVtK0srxMXFITk5Wb1BUKmUsLa2znL+d5l+yNBhGDJ0GADgxPFjCAkJRs/efVC+TAnsP3gUQgi08mqKwDv3oXi5/5NKpYK9nT3zMivzSp5VtrwyZZUtL7MaZ1bZ8sqUVba8MmWVMa/hMbSLAxtSrem4B8oHcHNzR2DgTfXt8hUqwNzcHFf8/dXj/P394Fqlapbzv8/0iYmJGDtqBBYtXorw8HCkJCejVOnSKF2mDJISExEeHq6e9mbADVRzc8+GhJpkyitTVkCuvDJlBeTKK1NWQK68zGqcWQG58sqUFZArr0xZAfnyEmUpR09Zm0u971V4Nm7eJqp71tAY17vPZ6Jlq9Yi9FmUuHrjlnApXvytZ6B+1+knTZkmZvwwW7xIEiImPlnY29uLsxf8xLmL/qJAgQIiJj5ZvEgSIkIZK6ytrcXNO/ez/YzbMuWVKatseWXKKltembLKlpdZjTOrbHllyipbXpmyGnJeWa7Cc+N+uAh5nmAww4374Qa5XthAEe/fQImJTxYlSpYUFy5f1Xhhdv+0p7CyshKFCxcWk6dO15jHq2UrMX3mrHee/kWSEP7Xborq1T2FKi5RPW7Val/h6OQknIoWFat9N6jH/7V8lWjXvkO2byhlyytTVtnyypRVtrwyZZUtL7MaZ1bZ8sqUVba8MmU15LxsoOTOwVAbKAohdHzJGgOgUqlga2uLsAglbDKcGOlt1vmuxfZtW/DvmnU6rk671NRU1PZ0x6rVvqhUubJOHkOmvDJlBeTKK1NWQK68MmUF5MrLrPrB53H2kikrIFdembIChplXpVLBoaAtlMp3/65nSF59lw14EK5xkt/cLlqlQqUShQ1uvbCBgg9roBAREREREVHuxgZK7mSoDRSeRJaIiIiIiIiISAtexpiIiIiIiIjIgPEixjmDe6AQEREREREREWnBBgoRERERERERkRZsoBARERERERERacFzoBAREREREREZMIUibTAUhlRrRtwDhYiIiIiIiIhICzZQiIiIiIiIiIi0YAOFiIiIiIiIiEgLngOFiIiIiIiIyIApXv4zFIZUa0bcA4WIiIiIiIiISAs2UIiIiIiIiIiItOAhPERERERERESGTPFyMBSGVGsG3AOFiIiIiIiIiEgLNlCIiIiIiIiIiLTgITwZXAuOgqV1qr7L0LnCNhb6LiFH5c9jqu8ScsxTVYK+S8hRDrZ59V1CjnnwLE7fJeQo5wL59F1Cjrn2WKnvEnKUXd48+i4hx8QlJeu7hBxV3sFa3yXkmHDJ3m/rdPTWdwk5xsK1jr5LyDGj+8uRNT42Wt8lkBFhA4WIiIiIiIjIgPEUKDmDh/AQEREREREREWnBBgoRERERERERkRY8hIeIiIiIiIjIgCkUaYOhMKRaM+IeKEREREREREREWrCBQkRERERERESkBRsoRERERERERERa8BwoRERERERERAZM8fKfoTCkWjPiHihERERERERERFqwgUJEREREREREpAUbKEREREREREREWvAcKERERERERESGTPFyMBSGVGsG3AOFiIiIiIiIiEgLNlCIiIiIiIiIiLTgITxEREREREREBoxH8OQM7oFCRERERERERKQFGyhERERERERERFqwgZIDFkwfhw71XdHUrTja16uMn37wRlJiovr+retWoUeLmmhctRg6NaqGY/t36bHaj5OQkADvUd+gUY1KqFaqCFp84o4Na1aq74+OVmHE4AFwK+2AWpVLYvFCHz1W+/H+/mMpWjauixJFrDGgdzf1+GfhT/HNl/1RvXJplHMphBYNamHvru16rPTjJSYkYPq44WhdryrqViqKjk08sXndP+r7b1y5jP5dvPBJ5WJoU68atm9co8dqP95ffyyFV6M6KF7YCgN6d9W4b+4PU9G4rgeKFciHyRNG66nC7LVg2li0r+eKJm4uaPdJJfw4c4J6OzWkd1vUr1QEjasWUw/hYU/0XPGHWf7HUrRuUhelHKwxqE83jfu+7N8THhVLoELxQqjjVh4/LzDs7VNGCfEvMKBVLXSpUzbTfZHPnqJr3fIY0qWJHirLXrI8jzNKiH+BPl410a5mafW4vxf5YFD7Bmjm6oD/zf5ej9VlD9let3OmjEHLOpVQz7UYWtSqgHnTx6ufxzHRKkwYPgj1XIuhqWcZ/LForp6rfT+DP22IE6vHIersT1j/45ca9+1d9h2izv6E8JML1YNTYVv1/VO+aYvz6yci+vwizB/T9fVF5zpftiiPwzNaI2x5L6we0Ug9vpCNBf4YUg/Xf+mM4GU9cOyHNmhd3TnT/CPbu+LKT53w6M+euDC/AzzLFMzJ8rPFzdMHsXRwe8xsXw3ze9bD+R1rEPX0MX7o4KYxTGtVEaunfK3vconeCc+BkgO69vkcQ8dORb78loh6HoGJwwfgn2W/YNDQMdjiuwJrl/+KmT//hfKVq+J5RDji4+L0XfIHS0lORpEijli1YSeKlywFv4vnMahXJzg6FUODJs0x3Xs0oiKf4/jlQEQ8C8dn3dqimHNxdPm0j75L/yAOjkUxYswEHD96CI8fPVKPj42NQdVqbpg0bRYcnYriwN5dGPz5Z9h96BQqVKykx4o/XHJKMgo5OOCP1VvhXKIUrl4+j2/6dYODU1FUcfPE0AHdMGTkRHTtPQDXr1zCkL6dUax4KVSvVVffpX8QR0cnjBzrjWNHDuHJ44ca95UsXQaTZ/hg9Yq/9FRd9uva9wsMHTdNvZ3yHtYf//yxCIOGjQUADB03Db0GfqPnKj+eg1NRfDdmAo4fOYQnjx9p3Ddq3CSULlsOFhYWeBQSjD7d28PFpQS6ftpbT9Vmn1X/mwuHoi5QRT3PdN+SWd4oW6lqlvcZGlmexxkt/2UOHIo6QxkZoR5XtHgpfD12KnZu+OctcxoO2V63PT77Et9NmI58+S0R+TwCY4f0w4rffsaX347DnKljoVJGYvep64iMCMfXfTrCydkF7bsaRt4n4UrMXbYHTWtXQDEHu0z3T1q0Ff9bcyTLee+GPMP3i7ZgUOdPdFtkNgmNfIEFW6+ikasTihXIrx5vaWGOKw+eY9q6y3gSGYeW7sXw19AGaDplNwIfKwEAk3u445MKRdBxzgEEhcXApaAlElNS9RXlg9w+fww7Fk9F1/ELUaJKDSTExSAm8hnsihTFpG3+6umSkxKxoFd9VG3cVo/VGgeFIm0wFIZUa0bcAyUHlCpbAfnyWwIAhBAwMTHBw/t3kZKSgj9+9sGoST6o4FoNCoUCBQsVQbHiJfVb8EfIb2mJkROmoESp0lAoFPCoUQt16jXEhbOn8CIuDju3bMAo76mwsbVDqTLl0O/zIRp7qBiath06oXW7jihQoJDG+BIlS2PI8FEoWswZJiYm8GrdDmXKlsel82f1VOnHy5/fEkNHT4JLybR1W616LdT8pAEunz8N/4tnkSePBXp89jlMTU1RzaMmmrZqj82+hrxuO6et24KZf/H5tHc/NGvRClY2NnqoTDey2k6F3L+r56qyX5v2ndCqbUcUKFgo032VXKvAwsIi7YZCAROFCYLu3cnhCrPf7ev+uHDiMHp8PizTfacO7Ua0MgrN2nfXQ2XZT5bn8SuB1/xw7vgh9PriW43xrTr3RO2GzZHf0lpPlWUv2V63pctlfh4/uH8XL17EYe/2TRg6ZjJsbO1QonQ59Oz/NbasM5xG2dZD/th+5AqeRcW+97yrt5/FvpM3oIqN10Fl2W/7hRDsvPgQz2MSNMY/CI/B/3YF4PHzOAgB7Ln8CHdCVahZNu35bWeZB0NbVcKwZacRFBYDAAiJiEVY1Iscz/AxDq78GY37DEMpt9owMTVFPmtbFC5eJtN0N08dgBCpqFS/pR6qJHp/bKDkkFW//YQm1ZzRunY53L55Dd37fYXge7fx/NlTBF73R6dG1dC+nitmT/wOsdEqfZebbRLi43Hl8gVUdK2Ke3dvITExEZWruKnvr1ylGm7euKbHCnPGs/CnuHPrJipVqarvUrJNQnw8rvldRPmKVZCamgohhMb9IjUVt25e11N19CFW/vYTGlcthla1yuL2zWvo0S99d9rlSxagRfWS+Kx9A+z6b60eq9Qt79HDUaaoHWpVLYvY2Fj06P2Zvkv6KCnJyfh56igMmzQHZuZ5NO6LjVbhj3lT8O3U+XqqTjdkeR6nJCdj4ZSR+G7KXJibm+u7HL0yttctAPy99EfUreSEptVL41bAVfQa8DUe3L2NpMREVKhcTT1dBdequBVgPO+1479ohUdH5uL02vHo3a6WvsvJEYVsLFC+qA2uhUQCAGqWLYSE5BR0q1sSAYu74MpPnTDtUw+YmxrO17bEF3F4cvsaVBFhWDSwBeZ9WhfrZg5HdMTTTNNe3LMB1Zp2gHkeCz1USvT+DOeVaOD6DR6Jw1ceYu2eM+jcayAKFioCpTJtQ3nu1FGs2HwY/2w/hscPH+DnWYZ/vDKQ9quJ98hvUKJ0WbRs2xFxsbHIn98SZmbpR47Z2NoiNiZaj1XqXmJiIgYP+gztO3WDu4envsvJFkIITBs/DMVLlUGz1h3g5lkLL+JisXbF70hKSsLl82dwaO8Oo2oGyqD/4JE4cvURfPeeRZdeg1CgcBEAwDdjpuK/w5ex++xtDB07DQumj8eRvYZ9Tp838Vm4GLcfPseuQ6fQrWcf2NrZ67ukj7Jh+RKUqVQVVWtkPpTuz4XT0aJjTxQrUTqLOQ2XLM9j37/+h7KVqsGtpmEczqBLxva6BYBB34zC6YAn+O/AeXTrMwiFCjsgLi4W+V77HGVtY4u4WOP4HDVl8Ta4dpiGEs29MfmXbfhxXHd0aFJN+4wGzNzUBH8PbYAtZx/ALyjtMEp7KwvY5s+D0o42qDFmG9r8sA8t3IpiRHtXPVf77l7EKCGEQMCpA+g/ZwW+W3EApuZ5sHHuGI3posIe4d7lU/BsZRx7QeqfwqD+GeqFjNlAyWGlylZAuYpVMGP8UOTPbwUg7cOeXYGCsCtQEP0Hj8SJQ3v0XOXHE0JgyrjvcO/uLfy+ch1MTEyQ39ISL17EITk5WT1dtEoFSyvj2MU4K4mJifiyX0/ky58PC375Vd/lZAshBGZ9PwoP7t7Gz8vWwMTEBHb2BfHL3+uxe+sGNPMsi0VzpqJj9z6wtS+g73LpA5QqWwHlKlXBzHFp54qoWr0WrKxtYWZujjoNm6FzrwHYv3OznqvUHRMTE7h5eMLKygozJ0/Qdzkf7NGDe9i5biW+GD01031XL57B9cvn0ePz4XqoLGcY8/P40YN72L5uBQaPnabvUnINY3ndvq50uQooX7kqpowegvz5LRH/2ueomGiV0RyqdfZKEFQx8UhOTsWB0wH4a9MJdGtpHD88ZcXc1ASrvmuIuMRkfPtn+iHesfFJAACfTf6ITUjGw4g4/Lb3Jlp5FNNXqe8tT760Q9DqdOwHO4disMhniab9vsV9/zNIfJF+rsdLezfBqUxlOJYxzPMDkpx4Elk9SE5OwsP7d1G8dFlYWOTVdznZTgiBqeNHwP/SefyzaResbdLOoF66THmYm5sj4PoVVHWrDgC4ce0KKlQynI76+0hMTMRX/XshMSkRK9ZsQp48ebTPlMsJITB70ihc9buAZWu2qdctAHjUrINVmw+ob4/9ZgBq1K6vjzIpGyQnJ73x3BEmJnL03pOSkhF013DPpXD90jlERoTj87Zpe58kJyfhRWwMuteriPJV3BH68AF6v/x1NykxAQkJ8eheryJ+23IUBQs76LP0bGOsz+OrF8/i+bNwfNaqNgAgJTkJcbEx6FinPHx+X4vKbsb7pVMbQ3/dZiU5KQnB9++iRJlyMDM3x62Aq6hc1QMAEHj9KspVrKznCnUj9bVDg42JuakJVn7bAOZmJuj94xEkZThB7LXgSD1Wlj3yWdnAtkjRLO8TSFuvqampuLxvExr25NV3yLAY7qcHAxEXG4MdG1cjWpW2K9udwOtYvmQhajdohrx586Flxx745/dFUCmjEK1S4p/fF6FB8zb6LvujTJswEhfPncHKDTs0dqPNlz8/2nTshp/mzEC0Somge3ew6q9f0aPPAP0V+5GSk5MRHx+P5JRkiNRUxMfHIzExEUlJSfhqQG/ExcVi+eqN6Se4M3A+k0fD78JZ/L56C2xe20U64Jo/EhMSEB//ApvWrMCFM8fR53PDvdrFq3WbkpyM1AzrFgCSkpIQHx+P1JQUpKSkID4+HklJSXqu+MPFxcZg+8Z/Ea2KUm+n/l6yALUbNEO0KgonD+9D/Is4pKSk4PzJo/hvzd9o2qqDvsv+IOrX7Gvr9WHwA+zcthmxMTFITU3F+bOn8fcfS9CoaQt9l/zBGrbqgOW7z2LppkNYuukQRs74CfksrbB00yFMXLgMf+08rb6v37DxcC5ZFks3HYJdgcwn6jQEMj2PG7fuiNX7zuPPLUfw55YjGPPDz8hvaYU/txxBuUpVkZyUhMSEeKSmpiA1JQWJCfFINuBtlEyv27jYGGxZ/y9UyrTn8e2b1/Hn4vmo27AZ8uXLj5btumDJgh8QrVLiQdAd+K78HZ179tN32e/M1NQEFnnMYGZqAoVJ2t/mZqawtcqHlvUrI19ec5iYKNC4Vnl80a0+thzwU89rZpY2vamJSfpyzHLvVxlTEwUszE1gaqKAycu/zU1NYGaqwIrhDZDfwgx9fjqCxGTNq+s8CI/F4atPML5zVeTLYwpHu3z4yqsCdl18+IZHyp1qtPkUZ7eugupZKJIS4nHk3yUo5V4XFi/3Trl76STilJGo2qS9nislej8K8fqZHyWkUqlga2uLg5cfwNI6e6+q8SIuFuOG9EXgdX8kJSbCvmAhNGnZHl9+5428+fLjRVws5k8bi2P7d8I8jwUaNGuN7yb+oNPDWgrb6O7L/KOQYDT0rIg8FhYwM03fwaljt574YcFiREerMGnMcBzetxsW+fKh36CvMXzMRJ3VAwD585jqbNkLfGZi4dwfNMbVrdcQY7wno2u7FsibNy9MTNMf/9tR4/Hd6PE6q+epKkH7RB/o8cNgtP6kCvJYWMA0w7pt2/lTTPb5GZNHD8HhvTuQnJwMN89aGDtlDspW0O0umQ62utuDa77PDCyc89q6rd8Qm3cewLdDPsf6NZpXPejR+zP88qvuLmv84JnuLm/+Ii4W4wb3wU2N7VQHfDXCGy/i4jD6y08RdPcWAKBoMRd8OnAIOnTX7UkanQvk08lyF86ZiR+zeM3+vPRPDP96IG7euIZUkQoHRyd07dEHw0eN0/meCtdeXrZS1/zPncT0b/vjvzOZf53ft9kXm//5Hb/+d1jnddjl1c3eeLnxeRyXlKx9omzgd/YEJg3rhx3n7wEA5kwYhr1bfDWmadmpJybM+Z9O6yjvoJvPLrnxdRuuo/fbF3GxGPFlb9y85ofExEQUKFgYzVp3wJBRE5EvX37ERKvww8TvcOzgXljkzYue/b/C19/p/pClOh29s2U533/dBpMGa/5QeOzCbfQZ9xf+WzQYFUql7f324PFz/G/NYazaekY93R/T++KzDnU05v1n2xl8NfXfbKntFQvXOtonegcTulTDhC6a53A5ERAGn03+2DnJCy8Sk5GSmv417Mdt1/DjtrQTAheyscCiQXXQ0NUR0S+SsP5kEH7Y6IfklOz92ja6f/ZkzUpqSgr2/TkPfvvTDpUs5VYbbYZOgXWBwgCAdT98C/M8edFl3Dyd1fBKfGw0ZneuDqVSCRsjuoLiK6++y95/8tyg8qlUKpR0KmBw60WvDZQBAwZg5cq0y5yamZnB2dkZ3bt3x4wZM5A3b9oXI8UbLhC9du1a9OzZE0eOHEGTJk0y3f/999/jhx9+yGLOzHTZQMmNdNlAyY102UDJbXTZQMmNdNlAyW102UDJjXTVQMmNcqqBklvoqoGSG+VUAyW30FUDJTfSVQMlt8quBoohyK4GiiHQZQMlN2EDJXcy1AaK3s+B0qpVKyxfvhxJSUm4ePEi+vfvD4VCgblz56qnWb58OVq1aqUxn52dncbtwMBAjf94KysrndZNRERERERERPLQewPFwsICjo6OAAAXFxc0b94c+/fv12ig2NnZqad5kyJFimRqqrxJQkICEhLSfzlQqXipVSIiIiIiIiJ6s1x15qVr167h1KlTOr9aiY+PD2xtbdWDi4uLTh+PiIiIiIiIiAyb3hsoO3bsgJWVFfLmzYuqVavi6dOnGDt2rMY0vXr1gpWVlcYQHBysMY2zs7PG/REREW98TG9vbyiVSvUQEhKik2xEREREREREZBz03kBp0qQJ/Pz8cPbsWfTv3x8DBw5E165dNab56aef4OfnpzEULap5bfHjx49r3G9vr3mJ1YwsLCxgY2OjMXyI/Ts24fvhAz9o3uwWGxONrk08EPX8zY2jj7F98wYM/6KvTpb9vmJiotGkpiueRzzTyfK3bFqPrwb01smy31dMdDTquFdEhI6yAsDubRsxdkh/nS3/fTwKeYCOTTyRmKCbk/Nt2bgOX/bvpZNlv6+Y6GjUdtPtut23fRMmDh+gs+W/j9iYaHRp4q6zbdTWTevx9cDc87r9xKOizrZRAHBk12b8MOoLnS3/fYQ+Csbn7T5BYqJuXrcyPY8B4NDO/zBtxOc6W/77CH0YjH6t6+hs3cr0ut2zbSPGfpM73mtjY6LRrkE1RGbj87h7S0/8O3dQti1Pl4o7FYDff5OQx/zDzmTQpU4JLB/eIJur0o3ihSxxbl575PmIyz1fPbwD6374Nhur0p3I0If4ZVBLJOtom0UE5IIGiqWlJcqWLQs3Nzf8/fffOHv2LP76S/NSoI6OjihbtqzGYGamudErVaqUxv26vnxdamoqfl04EwOHjVGPG9K7HRpUckCTas7qITzsidZlRTx7Ci/PUvisffrGOCUlBdNGf43mHiXw1aetNJZz5dJZDOndDhkvoGRpZY3WnXtixdKF2ZQwXWpqKhbOmopho9Ivkzd0UG/UqVIKbqUd0KhGJfzvxzlvXUZY6GMM6tUJVUoWQn2P8vD952/1fSkpKRj9zedwL+uEHu2aISz0sfq+i+fOoHenlhpZrays0blHbyz9Ofsve5aamgqfGZMxcmz6pZXn/jANTT6pDueC+TF5wmityzh7+iTaNm+A8sULw6NSKcyaPgmpqakA0rIO+2ogKhQvgg6tmiD0SXrW82dPo0vbFppZra3RvWdfLFrw9v/fD5WamorFc2fgq+/GZbovIvwpGlQtjh6t6r3Tsm4H3oBnmYIY8UV6gyImWoVhA7qhnqszhg3ojrjYGPV9u7dtxMTvvtRYRjGXEqhWvRY2/Jv9lwNOTU3F7BlTMGpcxnU7FY3reqBYgXxa1+2ZUydQuqi9xuBkZ4Hvx40E8GrdDkD54oXRoWXjTOu2c9vmmddtrz46Xbe/LpyBQcPS9+gb0rst6lcqgsZVi6mHt22jJgzthzZ1KqCJmws6NaqGv/83X31fSkoKpo7+Cs3ci+PLHq9toy6exZDebTNto9p07onlSxdkc9K0rHNmTsaIl5dEfxb+FMO+7A9P19KoULwQvBrWwr5d29+6jNAnj/FZ9w4oW8weNauUxeqV6c/BlJQUDP96ICqVKIJOWbxuu7XL/Lrt1rMvFi3U3bpdvmgW+gwepTF+98Z/8XnbuuhQoyT6tfDEqUO737gMIQR8ly1Cvxae6FCjJAa1qYObVy4CSPtyNXlIb3SuXQaTv+mDFxlet0d2bca8Cd9oLMuxWHFUcquBnetWZmPKNDI9j4G0vH/+NAv9hmhuj3Zu+Af9WtVGa4/i6NnUAycO7spy/pCgO5g8rB+61K+MdjVLY1ivNrh66az6/tiYaEz4uifa1SgF7697aazbQzv/w+xxQzSW5+hcHK7uNbHNd0X2hXxJptdtamoqFs+bga++zfq9tmG14ujR+u3vta3rVUHt8kVQt5IT6lZyQv2q6Yefp6Sk4PsRX6J+VRcM6OqFpxmex34XzuLzT9tkeh6369oLf2Z4LXwMhUKB6cPaw2fZHgBAYXsrLJ/VH3f2zETY8fk4vXY82jaqqp6+bPEiWLfwSwTtn40nx+bh0PKRqOtW+q2P0a5xVZxb542w4/MRsGMahvdJvwKntWVe/PfLYIQem49NiwbDMl/6qQC6t/TEXzP7aSwr+MlznL1yH192q/8BWYEpPdwxf/NV9bjvu7nhpE9bPFvZGz59PTPN07iKI47+0AYhyz7Fmbnt0Kya01sfo3xRG+yZ4oXHf/XEhfkd0Lq6c3rWfOZYN6YJHvzRA76jG8PSIv37UJc6JfD74E80sz6Lxfk7zzCoWbn3zgqkPXcPLF+Ixn2GqscdXPET/vdVW0xrVRG7fs18xVNVRBj++f4LzGxfDQv7NMSFXeve+hhbf5qERYO8MLVleZz6b7nGffGx0fhn0heY1ckD/07+EgkvYtX3XT28A5vmjtGY3t7RGS6V3XF+59oPiWvwFArDGwyR3hsoGZmYmGDixImYNGkSXrx4oe9y3urUkX2wsbVH2QquGuOHjpuGw1ceqofCDm/fSALAwmnjUL6y5nXij+zdjiePgrHrzC24utXAyt9+AgAkJyVh4fTxGD/zx0yXeG7TpRd2bFqN+BfZe7nTIwf2wNbeHhUqV1GP+3bMRBy9eBP+98Kwdss+bP9vPbZsePPGasTXA1C4iAPOXX+A//35L+ZM/x5nTx0HAOzduRUPQx7g7PX7cK9eE78uSvtgmpSUhOkTR2Hm/F8yZe3yaV9sXPsPXsRlb9aD+3bDzr4AKrmmZy1VugwmTZ8Nr9bttM6fkpKCgb27oWWb9ggICsW2PYexddN6/PvyQ92u7VsQEvIAV26HoLpnTSz+Ma0JlJSUhO/HjcTcnxZnytqjV1+sW70KcdmcFQCOH9oLWzt7lKvomuk+n8ljUNHV7Z2Wk5qaihnjh8O9hubl8DauXg5LK2sc9b+PvHnzYcPqtDdGlTIKfyyah7FTM39Q7dCtN3xX/vEBad7uwL7dsLO3RyXX9A9xJUuXweQZPmj5Duu2zif1ce9xpHo463cTpqam6NS1BwBg57bNCAl+gKu3H8KjRi38knHdjh2BeT/9L4t1+xl8V6/Uybp92zbqyNVH6uFt26gvvh2PLUev4LB/CH5buxN7t2/E7i1pH4SO7N2OJw9DsPvsbVRx98TKX38EkLaNWjB9HMbP/CnrbdTG7N9Gvf66jY2NQZVqbti+7xgC7j/FGO8p+ObLfrh1M+CNyxj6RT8UdnDElVsP8fuKNfhhijdOnzwGIO11+zD4AfxuhcCjRk3876f0dTt5/EjM+THz67Z7r75Yv3pVtm+jAODcsQOwtrVHqfKV1eN2rV+FTSt/hfeCP7D1fBAWrd2NUuUqvXEZyxfNxrmj++Hz5wZsPR8Enz83oLCT88tlrUR+S2tsPBkIC4u82LlhFQAgRqXE6t9+xNfjZ2ZaXouOn2LbmuxvfMr0PAaAs0f3w9rWDqUrpK/b7etWYv3ypZj84zLsuvQAS9fvRekM6z6jmGgVajVohr+3HcPWM7fRqnNPeH/VE8rIiJfLWgFLS2tsPXMbFnnzYfvLpleMSolVvy7EUO/MX4ZadvoUW1b/me1ZZXrdnjj8lvfaKe/+Xuuz+G+cDniC0wFPcOJq+uHnB/dsw+OHwTh04Q6qutfA30vSfkxLSkrC3KljMWnWz5mytu/aG1s3rMaLbHget6rviueqOFy/k9akssxvAf+bD9Go30I4NhyHmb/uxEqfAahYOu2CEHbW+bD35A3U7DEbxRqPxz/bzmLz4iEoaGeZ5fIL21vh37mD8OOK/XBoMBY9Ri3DxK9bo3ndtG3cF13rITo2HsWajEd8QiK+eNkYsbXKhwlftsK4hZsyLfPf7WcxuGfD987q5VYMkTGJuPEwSj3uXlg0pq69jN2XHmaavkRhK/w7ohFmb/RH8a/WYeray1j1bSOUKJz11ULNTBXwHd0Ex66HotTg9fh+9UUs+6YeSjmkTT+gaTlEv0hC6cEbEJ+YgoFN0xojtvnNMbZTVXj/ezHTMtcev4cvW1R476wAcPvcEeSztoNDqfT5CxQtAa8vxqFC3aZZzrNh9khYFSiE8evPoMekX7Bv2VwEXTmb5bQA4FimItoNn4ZiFapluu/CTl/kzW+FCZvOwzxPXlzY6QsAeBGjwpE1S9Bq8MRM87i36IKzW/9936hE7yxXNVAAoHv37jA1NcWSJUvU46KiohAaGqoxxMbGvmUpunf84B541v343feO7d8FlTISrTp9qjH+Uch9uHnWQR4LC9Sq1xiPgoMAAP8u+wX1m7ZCyTLlMy2rqHNx2NoVwKVzJz+6rowO7N2JuvUba4yrULkKLCws0m4oFFCYmOD+vTtZzv8g6B4unD2Fsd/PQH5LS7h71kLHrp9iw5q0D+Uh94NQo/YnsLCwQL3GTRF8/x4AYNmSn9DMqw3KlMu80XcuXgL29gVw9vTx7AsKYO/uHajfsLHGuB69P0OzFq1gba39UC+VSonIyOfo0asvTE1N4VKiJBo0boabN64BAB7cD0KtOmlZGzZphvsvsy795Ud4tWqLcuUrZlqmS4mSsC9QQP2hMDsd3b8btT7J/AHi8L6dUEZFol3Xnu+0nDV//4pSZSugRm3NX9AeBt9HjToNYGZmhlr1GuHhg7S8P8+eggGDv4N9gYKZluVeow7CnjzGvduBH5DozfbtyrxuP+3dD81atILVBxzGt27NPyhVpixq1q4LQHPdNmrSDPeDXq7bRQvRonXW67Z4iZKwty+ok3V77MBu1Kj7/h8OMypbwRV5Xr7OFVDARGGCkPt3AbzcRtV4tY1qgofB9wGkbaMaNHvTNqoEbO0L4NLZ7N1G7d+zA/UaNFbfLlGyNAYPH4WixZxhYmICr9btUKZseVy6kPUHuPtBd3HuzEl4T5mJ/JaWqF6jFjp37wnff9O+XAZnfN02boYHL9ftr7/8iBat2qJsVq/b4rp73Z45vBfutdJ/OU1JScGq/83FkAk/oGylqlAoFLAvVAROLiWznF8VFYn/Vv6GUT8sQrESpaFQKOBQ1AUFCzsAAJ48fIBqtT6BqZkZ3Os0wJOX6/bPhTPQfeBQ2Npnft26etTCs7AnCL57K1uzyvQ8BoCTh/bAo47m3qjLF8/FsImzUa5yNSgUChQoVARF37BuK1Wrjvaf9oddgUIwNTVFux79YGJqiruB1wEAT0IewK1WPZiamaF63QZ4FJKW97f509Bz0LAs122V6rURHvoED7J53cr0uj2yfzdqvuG9VhUVibZd3u299k0eBd+HR826yGNhgToNmiDkQdpnxpW/L0LD5q1Qqmzm53ExlxKws7PHxTMnPuqxAaBto6o4ei79Pfv+owj8/M9BPHoaBSEEdh27hlv3n6JW1ZIAgAvXH+Dv/07iWWQMUlMFlm8+hZTUVFQpVyzL5RdzsINCoYDv7gsAgKu3HuHi9WBUKZt2OH9J50I4duE2UlJScfjcLZRyLgQAmDWiE35aeQARUZm/M5z2v4tiRexQoZTDe2VtXd0Zx26Eaoxbe/weDlx5jOgXSZmmb+5WFFfuP8dev0cQAtjr9wiX7j1DrwZZ73HzSUUHFLDKg3lbriIhKRV7/R7h5M2n6FkvbfqSha1wIiAMKakCR6+HouTLxsr0ntWxeOcNPI/JfOjKmVtPUbRAfpQv+v6fdW6ePoRS7po/jnl4dUH5Wo1gkT9zE+j54wcIvn4RLQaNQZ58+eFSyR3VmnbA5T2Zm1iv1O7QF2U8PoFZHotM90WGhqBktdowNTVDaY+6eP447RyY+5bNQ/3uX8DStkCmeYq7VofqWSjCg7P+XkL0sXJdA8XMzAzDhg3DvHnz1E2SgQMHwsnJSWNYvHixXuu8HXAVJUtnfkNavnQBvDxLoV/7hti12fety4iJVmLR7O8xbuaPme4rW6Ey/C6cRnz8C1w4fRRlKlRGyP17OLR7K/oPHvnGZZYsWwG3b1x94/0fIuDaFZQplznrlHHfwbVEQTTwKI+42Bh07flZlvPfvHEVRRwcUahI+ptUpSrVEPiyzvKVXXH+zEnEv3iBU8eOoEKlKrh/7y52b/sPg78bm+UyAaBshYoIuHblI9Npun71Cspm0bB5V/b2BdCr7wCs+WcFkpKScD/oLo4fOYhmXq0BAJUqu+Ls6ZN48eIFThw9jEqVqyDo3h3s2LIJw0dl3rX3lfIVKuH61ezNCgA3b1xBydc+WEWrlFgwwxuTfH56p2U8fhiM1ct/w6jvM/8qXa5iZZw7dQyJCQk4f/o4ylV0xaVzpxHyIAgdu/fJcnnm5uZwKVkagTeyN++1q/5ZfmD+UL7/rkDvz9LPgVTJtYp63R4/chiVKrsi6O4dbN+yCd+OGv/G5ZSvWBHXr/hnW12v3A64ihJlMu+yu3zJArSoXhKftW+AXf9p38V13pTRaOjqhA4NquBFXAzadU07X0GZ8pXhdz5tG3X+1FGUfbmNOrh7C/q/dmhJRqXKVsCtgOzdRl2/egVly7/5dfss/Cnu3LqpsfdRRgHXr6GIoxMKZ9hGuVZ1Q8D1tDorvva6rfjqdbt1E4aNfPPrtpyOXrd3b16DS+n0dfsw6A4iI8Jx58ZV9GvhiT5N3fDTlFGIjYnOcv6bVy7CPE8eHNn1H3o1rop+LTzx58IZSEpMBACUKlcJfmdPIDExAf7nTqJk+Uq4dvEMnoTch1fnrL/omZmbo2jxUrh781q2ZpXpeQykrdviGdZtSNAdRD57its3rqBnUw90b1QVCyaPeOO6fd29wBuIi41ByTJpr49S5Svj8tnjSExMwOWzJ1C6fCVcvXgGj0Puo1WXrM8PZWZujmLFS+EOX7cfLPDGFZQqk/m9duFMb3w/+93eawHgh4nfobF7SfTr1AzHD+1Vjy9boTIunTuF+PgXOHvyKMpVdEXw/bvYv3MLPv/mzYenli5XUf1Z7GO4VSiGwPthb7y/sL0VKpZywLXbj7O837VsUVjnz4ub97I+FM8/8BGOX7yDPu1rw8REAfeKzqhavhgOnEnbO+n67cdoXLM88piboVGNcrh+5zE+cS+N0s6F8O/2rBtwycmpuBvyDG4VnLO8/02qlrDH7ceqd57eRIFMxymYKBRwdbHLcvoqLnYIeKhEckr6IVdXHzyHa/G06W88jELDyg7IY2aCBpUdcCMkCnXKF0YpByusOX4vy2UmpwjcC4tG1RKZmw3ahN4LQGGXtx9epTl9IKwLFIaVfSH1OMcylRAadPO9HxsAHEpWQJD/GSQnJiDI/ywcSpXHg2sX8PxJMDy8umY5j6mZOQoULYEnd9+89xrRx9BrA2XFihXYsmVLpvETJkzA06dPYWlpCSFElsOECWnn42jcuDGEELCzs8vR2lXKKFhaWWuM+2bMFGw6dAm7ztzCN2OnYuH0cTiyb8cbl/G/uVPRtmtvFC9ZJtN9nzT2QvXa9fF51+YID3uCfl+PxPypYzBysg9OHN6LIb3bYcSgbgi6o/krvaW1NVSqqGzJ+IoyKgpWWex9MWPeIlwNCsfmfcfRuXtv2L5hHcTFxsLaVvM+G1s7xMSkHXvdpHkr1K7XAF1aNUJY6GMM/nY0po4fgcmzFuDwvt3o3aklBvbsiDu3NDe+VlY2UEZFZUdENWVUJKw/8KTCr7Tv3BWrV/6FUo62qOtRGS1atkHT5i0BAM28WuOT+g3Rtll9hD55jGEjx2LC6G8xY85C7N+zC13atkDvbu1xK1Bzo29lYwNlVORH1ZWVaGUUrF57Hv80ewo6dO+DEqXKvtMyZnp/h29GTYRdFr9cdv60H+zs7dGrXSPY2dujbedPMXfaOEzy+QnrVi3DoO6tMfKrPngaqvmhycrKGipl1AfnyooyKgrW1tbaJ3wHZ06dwIP7QejeK/3Eys1frts2zeoh9MkjDB85DhNGf4uZc3/E/j070bltc/TqmnndWlvbIEoH6zZtG6X5XP5mzFT8d/gydp+9jaFjp2HB9PE4svft5xgYN2Mhjlx9hBWbD6N1557q13K9Jl7wrF0fn3dplraNGjwS86aOxqjJc3Di8B4M6d0WIwZmsY2yskF0Nq/bqKjILLdRAJCYmIghn3+Gdp26wc0j87HpABAbEwNbG1uNcTa2duovqc28WqNu/YZo3zztdTt05FhMHPMtZvgsxIG9u9CtXQv07dYet7NYt7p43caoopDfMv25HK1Me4xLZ45i8fp9WLrpEEIfPcDvcydnOX+0MhJxMdF49OAe/t55GgtWbsWFEwex/q+0HyZadu0DG1t7DO/hBRtbezRr3x2/+nyPb6fOx/a1f2NM/46Y8d0ARDzV/BU2v6UVYrL5/Uem5zEARKuiYJnFur14+ih+33QAf245gicPg7HEZ5LWZcWolJgx+kv0+WoECrzcu6hNtz6wsSuAwV2bw8auAFq0747Fs7wxatoCbFnzN77r2x5ThvfHs9fOKZPfyhrRKmU2JpXrdZvVZ8affaagQ7d3f6+d9dMf2HniKvadDUTPAV9jzODPcM0/7XCNBk1bokad+visY1M8DX2MQd+MxOxJozBu2lwcO7gHn3/aBkP7dcm0Z6dlNr3X2tnkR3RsfJb3mZuZYtWcgdi0/zIu3QjOdL+tVT6smjMQ8/7ei7CIrBuDQgj8u+0M5o3uAuXZn3Fy9TgsWnVQ3ZBZseU0IpSxOLl6HCKUsVi78zwWjOuO4bN98VX3Btj353fwXfAFnAprPl+iY1/Azjr/+2W1zANVFnuavMnha6GoXqog2no6w9REgbaezqhdvjCs85lnOb1lXnMo4xI1xinjkmCVN236f47cwfOYBBye2RrPYxKw/mQQ5variZF/n8Pnzctj5/ct8M93DeFol08z64sk2Fnmwft6Ea3Mck+TN0mMj0NeS83XdV4rGyS++LAjB6q36oZ81nb4bVgX5LO2g1uzjti1dCY6fDcD57atxl+je2Pt9G+gitBs4Fnkt0J89Ls3uoyFwgD/GaIPO/00abxJv1K1ei3133UaNkOnXgNwYOd/aOyV+fwKfudP4crFs1i59egbH2PwqEkYPCrtQ9Luzb5wKOqM0uUro2/b+li96yRuB1zDrAnD8efGfep5YqOjUeYtx71/CFs7O8S8YSNkYmKCau6eOHPiGHymecPnp18zTZPf0jLTB69olRJWVukb5NHe0zDaexoAYPP6NSjq7IzyFSujbeNa2HX0PAKuX8WE7wZj4+4j6nliYlQob5f1ceAfytbOHtGqD9/g3rkdiIG9u+F/f6xAq7YdEPEsHMO/HohZ077HpOmzAQATJk3HhEnTAQAbfFfD2bk4KlZ2RbN6NXDo1EXcuHoFo4Z9jR3703chjlGpYFsp87HTH8va1g4xGZ7Hl86egt+FM1i3690Ojdrxny9SklPQvmvWv1zmsbDA97PSf137fdFcNGvVHslJyfBduQzrdh3H7m0bsfCHiZj7v/QTh8XERMPmtabbx7K1s0N09Lv9aqvNmn+Ww6t1OxQqVFhj/ITJMzBh8gwAwAbff1HM2QUVK7miaT1PHD51CdevXcHIoV9h54H0/9/oaBUq6mDdpm2jNJ/Lr2+jOvcagP07N6Nxy/ZvXZaJiQkqVfPAhTPH8YvPZHzvk/ZFe/DoSRg8Om0btWuzLxxfbqP6tK2HNbtO4XbAVfwwYRj+2rhfvazYGBVKl8/ebZSdnX2W26jExER81b8n8uXLh/mLMm+bXrG0soLqtdd9tEqp8YVn/KTpGP/ydbvRdzWKORdHhcquaF6/Bg6evIgb165g1PCvsX1f+us2OlqFCjpYt1Y2doiLTX8u58ufdt6Anl9+pz4Eo+eX38Fn7OAs5381/WdDxyGfpRXyWVqhU9+vsHP9KvQZMhp58lhg+JT0k3Sv/nUh6jVvi+SkJGxbuxxLNh7AkZ2b8ce8qfBe8Lt6urjYGFjZ2GVrVpmexwBgbWOH2CzWbe+vRqjXbe+vRuCH0V+9dTkx0SqM/aI7qlavjQHD0/eAy5PHAiOnpp84dNXSBWjQoh2Sk5OxZc1f+OO/Qzi04z/8OncKJv+4TD1dXEw0rF9rVnwsmV63r39mvHQu7b3Wd+e7H4ZcvVb6yUHbdOqBw/t24uDubajiltZgGjZ2CoaNnQIA2PHfWjgVdUHZCpXQveUn2LjvNAJvXMO0sd9g1ZaD6uXEZtN7bZQqDtaWeTONNzczxZoFXyAuPgnfzFiT6X4bq7zYvnQoTl2+ix9+y/rEyADQqGZ5/PJ9T3T97necuHQHJYsVhO/CLxAdF49lG04gMSkZI3zWq6ef8GUrbD3kB3MzU3z9aUPU7TUXPVp5Ys6ozujvvUI9nbVlPkRFv985YKJiE2HzhuZHVu48UWHQ/45jQpdq+N+XdXH2Vjg2nXkAc9OsvzjGxifBJr9mo8Mmnzli4tOaNonJqRiz4rz6vrGdqmL7+WCYm5rgi+bl0WjSLnSrWxI/9PHEF0vSD8+yzmeOqFjNxsy7yGdti4S4GO0TvpQnb37Ex2p+1kqIjUaefFmf30YbszwWaP/tdPXtI//+D5XqeSElOQlnt/+LIUu24OqRHdj7uw+6T/w5/THjYpD3HQ69J/oQue4QHkNRrlJV3L/39uOBTRRv/u89f+oYHoU8QLt6ldCyZhn8OGM87t0KQMuaZfDstV/1lJHP8c8fizB8wgyE3L8LB6disLG1Q1WPmrj92u7S9+8EolzlrHd3/VCVqlTD3dtvz5qcnIT79+5meV/FylXxNPQJnoU/VY8LuHYF5StVyTRt5PMI/P6/HzFh6mzcv3cHTkWdYWtnD48atRHw2m6mdwJvolKVzCec+hiuVavhzkece+PmjetwKloM7Tp2gZmZGRwcndC912c4sC/z1TCeP4/AkkULMHmmD4Lu3kHRYs6ws7OHZ606uP7aoUm3AgPgWjV7swJAxcrVcP9O+ro9e/IIHgXfR/Oa5dHIrSTmTBmLO4E30MitJMLDQjPNf/bEEVz1u4BGbiXRyK0kVvy2CCeO7EdTz8y/qN2/dxuH9+7EwMEjcPvmdZSvlHZeArfqtXDrRvrzOCkpCSH376FC5ezNW6WqW6a9mD5EtEqF7Vs2oU//N1+u8fnzCPzv5wWYMnMOgu69XLf29qhRqw5uvL5ub96Ea7V3O4Hg+yhXqSoe3L391mne92plKclJ6nNHZKSMfI5/fl+E4RNmvraNqoXbAZrbqKA7gShfKXu3Ua5Vq+HOLc3XbWJiIr4e0AtJSYlYtmod8uR58y9vlVyrICz0scY26vpVf1SsnHkb9fx5BJYuWoBJM1573dbMvG5v6+h1W6ZiFYTcS1+3zqXKIo9F5i8vb1K6wrt/OXx4/y5OHdqN7oOGIeh2AEqXr4w8eSxQyb0G7r08rwaQdtLVx8FBKFMx8//Zx5DpeQykrdvgDOvW5T3XLZDWPBn3RXeULFsRo6YvzHTy0FdCgu7gxIFd6Pn5cNy7dQNlXq7byu411OdMAdLW7aPgIJTl6/aDVahcDUF3M7/XtqhVHo3dS2Lu1LG4G3gDjd2zfq/NiolJ1us1KjICy3/9GSO/n4ngoLtwdCoGG1v7tPfa157H927fRIVs+MzoH/gIFUpqnkvE3MwUq+d/jjxmpug15k8kJado3G9jlRfblwzFjbtPMHzW2w9596jogvPX7uP4xdsQQiDo4TNsPuCHVvUzr+uyxYugfeNq+HHFAVQpWxTXbj9CYlIyzl4JQtXy6edYMTMzQRmXQvAPzHzi17e5+iAS5d7zXCK7Lj1Ew0m7UGrwBvT88QjKOFjj5M2nWU57LSQKlYrZwixDg6VqCXvcCInKNG0ZR2u09XTGoh03UNnFDtdDopCYnIpzd56hystDfoC0E9OWdrDG1QfP36tuAHAsXQnhIVkfGpT19BUQ/fwpYiLTL5H95G6AxkloP9Szh0EIOHUA9Xt8ibCgW3AsVQFmeSzgUskDoffSP9+lJCfh+eMHcCqT/U1uIoANlA9Wv2lLXMpw4q1olRKnjuxD/Is4pKSk4Pypo9i8djmatOyQ5fy9B32DDfvP459tx/DPtmP48jtvFC9VDv9sOwb7gpq/av8yZzIGfDMaNrZ2cCrmguD7d/A09DHOnTwM5+Il1dM9eRSMqMgIeNT8BNmpmVcbnDmZvqfMo5Bg7Nm+BbExMUhNTcXFc2ewctlSNGjSPMv5S5QqDc9adbFw9lS8iIuD/6Xz2LppHXr06Z9pWp9p3vhmxDjY2tmjmHNxBN27g9Anj3Dy6EEUL5l+DOajkGA8fx6BWnXe/xJ0b+PVqi1OHtfcKygpKQnx8fFISUlBakoK4uPjkZSU9e6b1dw8EBr6BLt3bEVqaiqePQvHxnWrUaWqe6ZpZ0yagO9GT4CdnT2cXYrj3p3bePL4EY4dPoCSpdKzhgQ/wPPnEaj7yceftPh1DZu3xvnT6b+8ffblMGw7egnr95zE+j0n8c3oiShRuhzW7zmJAq/tbQEAY6f4YMuh8+rpu/UdhJp1G2Dtzswn4Jv9/WiMnz4P5nnywLlESVzzu4holRKnjx+Cc4lS6un8L55FEUcnlP6Ic9Fkxav1m9dtakoKUrSs21c2b1yHAgUKonHTFm+cZvqk8RgxZgLs7DXX7dHDB1Ai07p9hjo6WLcNmrXCxTMZ9nRRReHk4QzbqJNH8d+av9G0VdbbqCePgnFoz1bExaa9zq9cPIt1K39HnQbNMk27yGeS5jYqKOM2qpTGMqOeP4dHrezdRjVv2RanTqSv26SkJAwe2BtxcbH469+N6Se8foOSpcqgZu1PMGfmFLyIi8Pli+exeYMven02INO0MydPwLdvet1m2EY9DH6A5xEROlm3dRp7wT/DycIt8uZD03bdsP6vxYhWRiFGpcT6vxajbpNWWc7v6FwCHnUbYvWvCxH/Ig4RT0OxdfWfqNs08/SLZ47DN96zYJ4nD5ycSyDw2iXERqtw6dRRjZPU3vA7j4JFHFE8i5OufgyZnscA8EmTlvA7m/7ZwiJvPrTo0A2+y35Rr1vfZb+gXrOs121sTDTGf9EDziXLYOwPma+8ktHPM8Zh+CQfmOfJg6IuJXDz6mXERKtw8dRRjZPUXrt8DoUcnFAim9etTK/bRq+/134xDFuPXMK63SexbvdJDBmV9l67bnfW77VPHoXg4tmTSExIQFJSEvbu+A9H9u1CE6+2mab9cdYkfDFsDGxs7eFUrDgeBN1BWOhjnDmh+V77+GEwoiKfw/O1k79/iF3HrqJRzfTnh5mZCVbPGwTLfHnQY9QfSExK1pje2jIvti0ZitvBTzEkiz1TXnf2ShA8K5dQX+q4uJM9OjVzh39gSKZpF03sgTHzNiIpOQVBD5+hhmsJ2FjlRbM6FXEv5Jl6ujpupfH4qRKBQW8+d0tW9lx+iAaVHTXGmZkqYGFuAlMTBUxN0v7O2ABxL1UApiYKWOU1w7hOVWFvlQdrj2f9o+Opm2GIjE3EmI5VkcfMBC3ciqJ+JQesPZG5ibFwQC2MX3UBSSmpuB8eA8/SBWGTzxxNqjjiflj6XiO1yxXGk8gXuPUe5255pUKdJgjy1zyPTEpyEpISEyBSUyFSU5GUmICU5LTPTgWKlkDxytVxYPlCJMa/wMOb/rhyaBuqt+r2xsdITkpULy81JSVteSnJmabbsXga2gydDDPzPCjg5IKHgVcQHxuNu5dOwt6puHq64BuXYFPIAYWLv9vhccZE35ck5mWM6a0+aeyFqMgI3L11A0DaLzR/Lp6HNnUqokX1Uvh51kR8N3EWmrXppJ5nxKBuWLE07dJyltY2KOJUTD1Y29rBzNwMRZyKwdTUVD3PxTMnEBEeBq/2aRuegoUdMGjoWPRr3xA/zfTGmGnpu+Lu3rwObbv2Vu/ym10aN2+FyOcRCAxI/0Vq+R//Qz33cvAo64QJIwaj3xdDMPjb9Guxt2rgia0b039R+Pn3FQh98hg1KxfHN4N6Y/yUH1D7tQ8oZ04eQ/jTMHToknZZ2MIOjhg2agLaN62Lmd+PxfQ56YeC/Ld+Nbr27Iv8ltmbtZlXazyPeIabN9Kzjvl2CEo52mLT+jX4e9mvKOVoizHfDlHf36iOOzatTzuJYfGSpfDbX//gx3mzUbGkA5rUrY5ChYtgus98jcc5dfwonj4NReduaVdfKuLgiBFjvdG8QS1MnjAas+cvUk+7wXc1evT6LNuzAkCDpl6IjHyO24Fpz2Mraxs4OBVTDza29jA3N4dDhudl52a1sHNz2iVAbezsNaa3srKGhUVeODgW1XicrRtWw6VkaXjUTDuTexU3TzRr3QFt6lXFpjUrMMJ7hnra7ZvW4tP+X2Z71rR1G4GADHu7jP52MEo62GDjujX4+4+lKOlgg9Hfph/20LC2Gzat1/xwt+af5fi0T783/up98vhRPA0LQ+duaSfbLOLgiJHjJqJZ/ZqYPH40fBb8op52g++/+LR3P1jqYN2qt1GBr7ZRyfhr8Vy0rlMBzauXxM+zvPHd969towamb6MAwHf5b2hfrzKaeZTADxOGoUe/r9DvtZNYXzxzHM/Dn6JlhwzbqGFj8Vn7Bvhx5gSMnZ7+3N/1ny/adu2V7duo11+3F86dxt5d23Hh7GlULVsU5ZwLoJxzAfyycK56niZ13fHf+vSTjy75cxVCHz9C1XLF8GW/nvh++mzUrad51YxTJ44iPCwUnV573Xo1rIWp3qMxK+Prdt1q9Oitm9dtzYbNoYx6jvu308/dMGTCTBQs7Ij+LWvg83Z1UcTJBV+PT39dfdmhAQ7t2Ki+PWHur4iNUaFnQ1cM/9QLnvWaoMegYRqPs2+zL4oWLwXX6rUBABWqeqBe87bo51UDuzeuwuej0s+xcmDbenTo9ea9sj6UTM9jAKjdqAWUkc8RdCt93Q71noWCRRzRq3l19GtdBw7FnPFNhktJD2hXD/u3bwAAHN+/Ezf8L+D4vh1oW6MUWlcvgdbVS6jvf2XPf2tRrHgpVH25bitWrY4GLdqid/Pq2LF+Fb4aM1U97b6t69Gpd/avW5let/WbeCHq+XPcect7rdlr77Vdmqe/18bFxmLutHFo5F4STaqXxqo/fsG8pStRLcPhbABw/vRxRDwNQ+uO3QEAhYo44Mtvx6Fn63qYN208vGemvy52bFqL9t2y5zPjnhPXUdDOEpXLpF1OvI5babRv4oa6bqXx8PBchJ9ciPCTCzF2kBcAoENTN9SuVgqdmrnj6YkF6vt7tq6hXmb4yYWo55F2jsDT/vcw/sf/sHRKbzw9sQCHV4zGab97mPvnXo06+ravjbshz3DaP63ZcPFGMLYe8kfAjukY1KUeJv2yVT1tn3a18Pv697/a0j6/xyhoZYFKzumHtP3yeR2ELe+NT+uXxldeFRG2vDd++Tz9yjVTe3gg6PfuuP5LF7gWt0P72QcQl5C+R87DPz9F3QppjbPkFIFeC4+gcRVHPPijB+Z8VgNfLT2JoDDNw2h6NyiNe2HROHs7HABw+V4Etl8Igf9PnTCgSTlM9b2snrZn/dL4c/+H7V1dvlZjxCkjERaUvgfV1p++x8x2VeB/cCvObv0HM9tVwdafvlff333iT1A9C8Pc7rXhO2MYvL4Yh1LVaqvvX/xla/gfTF8Xq7wHYma7Knhw7QL2LZuLme2q4OjqpRp1XN63CQWKFkcJ17RD1opVqIbK9bzwU78muLBrHby+SL/ohP/+LajV5AQG1gAAKEFJREFUvi+IdEUhhBDaJzNuKpUKtra2OHj5ASzf43i5fds34uj+XZj1y986rO7dxMZEo3+HRli2YR/sCxZ667SFbd7+q05Wtv23Hvt3b8fiZf98aInZJiYmGu2b1sXGXYdRMItfal6XP4+p1mky2rxxHfbs3Ibfl6/+0BKzTUx0NFo0rIXt+49lOt9GVp6qMl++TpvdWzfg0N6dmL90xQdUmL0ePwzGN/26YP3uk+rLjr6Ng+377dq+eaMvdu/Yhj9WaP/FS9dioqPRvEEt7Djwbuv2wbP3O04bAPZu24hj+3di1uLl2ifWsdiYaPRr3xB/btyvdRsFAM4F8mmdJqMtG9dhz65t+O3v3PG6bdmoFrbtO/ZO26hrj9//5JyHd/6HU4d24/uFy7RPrGNhj0Pw/dc9sXTTIeTJ4jKUr7PL+34nMjTk53FcUuZfUbU5uGMTThzchak//fUhJWar0EchGP9lDyzbcuSd1m15h/c7Ubchv27D3/P9dvfWDTi8byfmLVnxgRVmn9iYaPRsUx8rNx9EgXd4HgNAnY7eb72/RytPtG9cDZ9N0P/rVJviTvbY+r+hqN1zTqa9YwDAwrVOFnOl61q3JNp6OmPQ/z7+EtC65lLQEhvHNUWD73ciMTk10/2j+789KwBcObwdN08dQI/vF2mdVt+iwh5h1cRB+ObXbRqXRY6PjcbsztWhVCph85EXi8iNXn2XDQmLNKh8KpUKLg72Brde2EDBhzdQDNWHNFAM2fs2UAzZhzRQDNn7NlAM2Yc0UAzZ+zZQDNmHNFAM2fs2UAzZhzRQDNn7NlAM2fs2UAydtgaKMdHWQDEm79JAMQZsoOROhtpA4VV4iIiIiIiIiAyY4uVgKAyp1ox4DhQiIiIiIiIiIi3YQCEiIiIiIiIi0oKH8BAREREREREZMh7DkyO4BwoRERERERERkRZsoBARERERERERacEGChERERERERGRFjwHChEREREREZEBU7z8ZygMqdaMuAcKEREREREREZEWbKAQEREREREREWnBQ3iIiIiIiIiIDJhCkTYYCkOqNSPugUJEREREREREpAUbKEREREREREREWrCBQkRERERERESkBc+BQkRERERERGTAFC8HQ2FItWbEBgoAIQQAIDYmWs+V5Iy8Cgt9l5CjUsxN9V1CjomJTtB3CTkqvyJR3yXkmJjoOH2XkKOizZL0XUKOkeW95xWzJHN9l5BjXiSn6LuEHBWdT+i7hBwj2/utSJHn/VYkvdB3CTkmPlaO95+EuBgA6d/5iD4GGygAoqPTNh4dGlTRcyVERERERKQvCVeX6buEHDN7vb4ryFnR0dGwtbXVdxlk4NhAAVC0aFGEhITA2toaihy6npJKpYKLiwtCQkJgY2OTI4+pTzLllSkrIFdembICcuVlVuMlU16ZsgJy5ZUpKyBXXmbVPSEEoqOjUbRo0Rx7TDJebKAAMDExgbOzs14e28bGxug3lhnJlFemrIBceWXKCsiVl1mNl0x5ZcoKyJVXpqyAXHmZVbek2POEJ0HJEbwKDxERERERERGRFmygEBERERERERFpwUN49MTCwgJTp06FhYUcV8SRKa9MWQG58sqUFZArL7MaL5nyypQVkCuvTFkBufIyK2UXxct/hsKQas1IIXg9JyIiIiIiIiKDo1KpYGtri9BnSoM6j45KpYJjIVsolYZVNw/hISIiIiIiIiLSgg0UIiIiIiIiIsrVlixZgpIlSyJv3ryoXbs2zp07l+M1sIFCREREREREZMAUCsMb3se6deswatQoTJ06FZcuXYKbmxtatmyJp0+f6uY/9A3YQCEiIiIiIiKiXOvHH3/El19+iYEDB6Jy5cr47bffkD9/fvz99985WgevwkNERERERERkwFQqlb5LeC+v6n29bgsLi0xXakpMTMTFixfh7e2tHmdiYoLmzZvj9OnTui82AzZQiIiIiIiIiAxQnjx54OjoiHKlXPRdynuzsrKCi4tm3VOnTsW0adM0xj179gwpKSlwcHDQGO/g4ICbN2/qukwNbKDomRACivc9AIyIiHSC22QiIqKcERISAqVSiSpVqui7FIOWN29eBAUFITExUd+lvLesPne9vvdJbsMGih4EBQXhyZMn+OSTT6BQKKT5wJ6cnAwzM+N+yt28eRN+fn7o2bOnvkshHZHl9ZqamgoTE3lOk/X48WMULlwY5ubm0mU3RklJSTA3N9d3GTni2rVrsLGxQfHixfVdCumQLO89JI/Lly+jefPm+PPPP9lAyQZ58+ZF3rx59V2GzhQqVAimpqYICwvTGB8WFgZHR8ccrYWfEHNYYGAgatSogS5dumDfvn0AoG6iGKPr16+jc+fOiIuLg5mZGZKTk/Vdks7cvn0bNWvWRO/evbF06VJ9l5Mjbt68CV9fX32XkSMiIiIAGPfr9ZU7d+5g2bJl6szG7ubNm3B2dkaDBg2QlJQEExMTpKam6rssnXn48CGuXLmi7zJ05ubNmxg5ciRu3Lih71J07t9//0W1atXg4+OD0NBQfZeTo548eYLr16/ruwydu3v3Lvz9/aV473klODgYR44cQVJSkr5L0Tlj3x6/ib+/Pxo0aIDPP/8cnTt31nc5ZADy5MkDT09PHDx4UD0uNTUVBw8eRN26dXO2GEE5JiwsTLRs2VI0b95c9OzZU7i6uordu3er709NTdVjddnv3r17omTJkkKhUIi6deuK2NhYIYQQSUlJeq4s+0VFRYlevXqJbt26iSlTpgiFQiF++eUXfZelU7du3RJWVlZCoVCIJUuW6Lscnbp+/bowNTUVQ4cOVY8zttfrK7du3RK2trZCoVCIefPmiaioKH2XpFOhoaGiUaNGwsvLS5QtW1bUr19fJCYmCiGESElJ0XN12S8gIEA4OjqKunXrijNnzui7nGx39+5d4ezsLBQKhejUqZO4ffu2vkvSmdOnT4tKlSqJ3r17CwsLCzF48GDx5MkTfZeVI27cuCHy588vGjRoIK5fv67vcnQmMDBQmJqaCoVCIU6fPi2EMN73nldu3rwp8ubNKxwdHcWBAwdEcnKyvkvSGWPfHr+Jv7+/sLKyEhMnTtQYHxoaqqeKyFD4+voKCwsLsWLFCnHjxg3x1VdfCTs7uxx/7rCBkoP8/f1FmzZtxKFDh8S5c+dE3759NZooxvSmGBsbK4YNGya6du0q1qxZIzw9PUWNGjWMtony+PFjMXr0aLF582YRHx8v5s+fr9FEMaZ1K0Raw6h3795SNIwePXokatWqJWrUqCGsrKzE8OHD1fcZ23qNjo4Wffr0EYMGDRIzZswQCoVCzJ4926ibKLt37xa9e/cWR44cERcuXBBlypQx2ibK48ePRaNGjUTdunVFy5YtRevWrdVfyozBixcvxKhRo0Tv3r3F4cOHhb29vWjdurVGE8WYXrPr1q0TAwcOFLGxsWLfvn3CxMREiiZKeHi4aNCggejcubNwdnY22iZKRESEaN++vejSpYvo0aOHsLS0FCdPnhRCGNfzOKOIiAjRunVr0b9/f9GkSRNRtGhRsX//fqNsohj79vhNlEqlKF26tKhatarG+MmTJ4uKFSuK6OhoPVVGhmLx4sWiePHiIk+ePKJWrVp6aT6ygZLDMr7JnzlzRvTp0yfTnijG8kaxePFisXbtWpGamiqOHDmSqYliDDmfPn0qzp8/LwIDA0VcXJx6fGxsrJg3b16mxkJSUpJ49uyZPkrNVg8fPhRjx44VW7ZsMfqG0erVq0W3bt3EyZMnxbp160S+fPk0mijG8gU7NjZWPH/+XCxYsECsX79eCCHEL7/8YvRNlISEBHHo0CH17XPnzqmbKAkJCUKItHWcmppq8M/rEydOiKZNm4pTp06Jbdu2CS8vL40P7Yac78WLF0KItKbC6tWrhRBpe6Nk1UQxdKdOnRJ3794V8fHxws/PTz1+9+7d6ibK48eP1eNfPY+NgVKpFGfPnhUDBw4Ufn5+IiwsTBQrVswomyhXrlwRX3/9tdi1a5cIDw8Xn332mUYTxVjeezK6d++eGD58uHqb7OXlZbRNlGPHjhnt9vhtkpKSxC+//CLy5s0rZs6cKYQQwsfHRxQpUkTs2LFDz9URvRs2UHLIm/a4OHv2rLqJsmfPHiGEECNGjBDbt2/PyfKyTWpqqkbWV28ASUlJ4tChQ5maKHFxceL+/fsG+UHg+vXrol69esLLy0t07tw505v7ixcvxNy5czUaC99++62YNGmSUXygvX//vvpvY2wYBQYGipMnT4pnz56Jbdu2qcevXbs2UxPF0D/oXLhwQZQtW1Y8evQo06/XixYtUjdRlEqlECKt+fno0SN9lJotQkJCxL///it+++03ERYWlun+rPZE+fnnn8XBgwdzutRsERQUJCIjI4UQaQ2iV7Zs2ZLpQ7sQ6V/MDGW7/PDhQ9G9e3f1F8uMbt++namJkpqaKi5cuJDTZWaLoKAgUatWLdGhQwcRFBQkhEjL8+p5mrGJEhYWJsLDw8WoUaOM4vCAgIAA0bJlSxEQECAuXryofn4+efJE3US5du2aenpj+MLt7++v/jssLEz07dtXWFpaihMnTggh0tZ9SkqK+jOVoQoKClJ/eb579676+SyEEC1atBBFixYV+/btU6/T5ORkjWkMyf3799WfGTK+Lo1le/wmgYGBYseOHSI1NVUkJCSIX3/9VSgUCtGwYUPh4OAg9u7dq+8Sid4ZGyg6FBAQICZOnKixsXwl44bw3Llzok+fPqJatWqiTZs2QqFQiMuXL+dwtR8vMDBQDBs2THTu3FnMmzdPPf5VQyUlJUWjifL8+XMxdOhQ0aRJE4N787927Zqws7MTEydOFA8ePHjjG9uLFy/EvHnzRJ48eUTt2rWFQqEQly5dyuFqs0dsbKwIDw8XBw8eFA8fPtT4Mi2EcTWMLl++LCwtLcX//ve/TPclJycLX19fjSZKcnKy+Oeff8SVK1dyutSP5ufnJ6ytrcW3336rMT7jh9Off/5Z3UR5+vSpGDNmjOjbt6/6V39Dcu3aNeHm5ib69u0rxo0bl+U0r75glylTRjRs2FAMHjxYKBQKcevWrRyu9uMlJiaKJk2aCAcHB3UTJaOtW7dm+tA+cuRIg3oPunv3rqhTp45o27atOH78uBBCs6lw69YtdRPlxo0bYujQoaJ+/fpZ/n8Ygj/++EM0bdpUdO/eXd0USk5OVm+L9+zZI8zMzMSAAQOEu7u7qFKlilE0E5YvXy7q1KmjMe7Ve0toaKjGnigJCQli7ty5YtWqVfoo9aO9qSkfHh6ubqK8ahhOnDhR/P777wb7BfvRo0eiUKFCokKFCsLX11c9PuNeva+aKPv37xexsbFi6tSpYubMmQb340V8fLyoU6eOKFGihLr2jOvNGLbHWfHz88v0A1tiYqJYtmyZsLW1FX369NFjdUTvjw0UHUlMTBQ1a9YUCoVClCtXTowZM0a9W3zGaV45efKkKFq0qLC3t9f4xcFQ+Pn5icKFC4tOnTqJnj17CnNzczF//nz1/Rk76IcOHRK1atUSFhYWwtLSUpw9e1ZfZX+QiIgIUb9+/UxfON/0Rh4VFSWqV68uChQoYJBfsIVIa47169dPVKxYUeTNm1fY2dmJ3r17i/Pnz2tM9//27jyqivN+A/h3RFZBWQwqLmgEBVsFVECrIqYsIhYMKubEBeuSKCXWBQUS+bkdIbZVq7YiBgOuEeOWVG2VWheSqGk1cqyskiBY7TFNoj3oQeDy/P7g3Mm9AYIkIJ3r8/kLZoZ7v5eZO8sz7/uOKQRG169fh42NDRITE5tcpqamxqg7z6JFi9CxY0fcvn37GVb64+Xl5cHGxqbBQG76ixLDi64tW7bA3Nwc3t7eMDMz0+QJ3T//+U84ODhg5cqVagAIAB9++KFRKyO9S5cuQVEUODo64urVq8+y1FZ148YN+Pr6YtCgQfj6668BGB9/9Cft4eHhiIqK0mSIX1xcjPHjxyM0NFS9Ow98uy2XlJTA2dkZjo6OsLS01OT6NDzGZGZmIiAgANHR0UYhiv5Ye/DgQSiKAj8/P5MZzyclJQXDhw9vcKzV36TRd+cJDAzElClTYGlpiYKCgvYo9Qd52kBaH6LY29tj0qRJUBQFN27caOPq2s65c+fQoUMH+Pr6IjIyEllZWeq8qqoq9efg4GD06dMHYWFhMDMz0+RnrqurQ25uLn7605/Cx8dH3ZYNP6cp7I8NffbZZ42eZwBAZWUl0tPT0aFDB6xdu1adrrVgjJ4/DFDa0G9+8xts2rQJZ86cwapVq+Dg4IAZM2Zg+/btRjuHmpoaLFmyBFZWVpo8IOTl5cHa2lrdOep0OsTFxWHx4sWNnhA8fvwYYWFhcHR0NGpuqxU3b95E//79ceHChUZPSL+7buPj46EoimbDk7y8PPTo0QMLFixAVlYWCgoKkJCQADc3N3h4eBhdrADaDoyaChTOnDnToPVBbW0tDhw4AEVR4ODgoLkuAeXl5ejatSuio6ONpm/evBnx8fFqeGK4Pfv7+8PJyUlz6xWoDz4DAgIQFxdnNP3tt9+Goih46aWXjEKU6upqvPbaa7CxsdHs2AqGdzgLCgrws5/9DMOGDVNbXhiGKEePHkWXLl1gb29vNK6GljQVoug/5/Tp0+Hk5KTJ445ecyGKTqfD3bt3MWrUKPj4+KjhglYHbjc8h1i7di2CgoIaXU6/v6qoqFBDTy2F9/puaIbjMTW3fK9eveDo6KjZ76uhOXPmwNvbG5MnT8ZLL72EvXv3qvMMW7Da29vDyclJ04GCTqfDpUuX4OHh0WSIYgr7Y6B+HJ9OnTph5cqVRtMPHjyodu+urq5GWloazMzMsH79+vYok6jFGKC0oXPnzqFz587qXfq7d+9i9erVsLGxwYgRI7Bz504UFRXh7t278Pf319TBXk9/ETZ16lSj6dOmTYO3tzc8PDwwfvx47N69W52XkpICCwsLzR4A9+/fj44dOzba/FLv0aNH+Mc//oG6ujosW7ZMswdAfaCQlJTU4AQ8OzsbPj4+8PPzMzp5X758uSYDo6YChXXr1qF3794N7mTqdDrMnTsXdnZ2yM/Pf5altoovvvgCvr6+iIiIUC82U1NT0blzZ5w7d85o2erqasTFxWlyverl5+ejf//++Nvf/qZ+Z9PS0mBubo4//vGPCA4OxoQJE/DBBx8AqB+fyt/fv0ErKy0wvOg0DEmWLVsGRVHg5eWltkSpqamBTqfD4sWLYWdnp+lwAWg8RNHpdOrTwrR63DHUXIhy6tQp+Pv7q+teq+GJPlQ4c+YMAGDVqlWYNm0agG+Pu4atbiorKxEXFwdbW1vNhZ6lpaUYOXIkwsPDG9yU+K66ujp1f6zFm26G9KHByZMnMXv2bJw+fRpRUVEICAjAvn371OUqKyuxcOFCWFhYaG4fde/evQZP16mursaVK1fg7u7eIEQxlf3xnTt3oCgKXn31VaPp+psWhjednjx5gvT0dCiKYtR6neh/FQOUNhYfH4/p06erJ7TTpk2Dh4cHYmJiEBAQAHNzc2RmZho1J9eSpi7CbGxssG7dOmRkZMDT0xPu7u5qiLBjxw5NXnDqffzxx7CyssLhw4ebXGbbtm0IDg4GYHxXQUsaC8e+O0jwzp070blzZ+zcuRPAt49z1mJg1NS23LVrV6OnZAH1/4c///nP6Nu3ryYvsPX0F5sRERGYP38+nJ2dGx3I7fHjx9i0aZMmQ169vXv3wszMzOjis6KiAhcvXgRQ383l5z//Ofz8/FBYWIiqqipNDoDc1J3sDRs2wMnJCRkZGRg+fLhRd56///3v6N69u+ZaUTXFMETRjxORk5OjuYvq72O4Hb/77rtqiFJaWgrg2xYZWg1PgG9DhbCwMFy9ehVJSUmYOXNmk8vn5+cjMDBQswPmNtWCynBd19bWIicnB6+99ppm98fl5eU4evSo0bT79+/Dw8MDf/jDH3D//n1ERUUhMDBQDVEePHiAV155xWgQbC0oLy+Hk5MTFEVBYGAgkpKScPbsWfWc/9NPP4WPjw+GDBmirucrV66YzP54yJAhGDRokLo9b9iwAV27dlVDUcNtu66uDhkZGZq+PqDnBwOUNvb+++9j5MiR6t3qbt26qYlyYWEhNm/erOmEGTC+CJs3b16Di7Dbt29DURSkpaW1Y5Wt586dO3B2dkZERITRk2gMDwTLli1DQkKCpvtxGgYK+oEZ9Qw/V0BAACZPnqz+rrUBYw19N1B44YUXGg0USkpKoNPpGjyxRouKiooQHBwMa2tr/O53v2tyOa2Pn5CbmwtLS0scOXIEgPE2rP9sO3fuhK+vL+7cudMuNbYG/UXnhAkTjIJAR0dH5OTkAKi/0Bw6dCi8vLzw1VdfAYBmB1RtSnFxMSZOnIgRI0Zo7qLraTUWosycORMVFRUN5mtVSUkJQkNDERUVhWHDhmHo0KGYNWsWZs+ejTlz5mDGjBmYNWsWpk6ditjYWM3ejNJrKkQB6o+tS5cuxYwZMxp9epgWGAYKEyZMQHZ2NoqKigDUj0U1ZswY3L9/H/n5+YiKikJQUBB27doFAJp86k5ZWRm8vb0xcOBADB8+HDExMbCysoK3tzdmzpyJ7OxsHDp0CAMHDsS4cePU76wp7Y/1428tXLgQTk5OjT7N7vLly/jvf//bDtUR/TAMUJ6BgIAAdOjQAS4uLpq8M/80GrsI0z8F4c6dO/Dy8sL777+vTte6I0eOwNLSEjNnzjS6q/no0SMkJSXB1dVVPSnQMsOTOcMQxXAdBgYGNmiiqWVNbcv6z7xy5Uq4uLho/kTd0K1bt9SR/5taz1pXUVHRaPBpaNmyZZg6darmT+T039vIyMgmg8CCggL069cPfn5+0Ol0JrWu9QoKCjB58mTNDe7cEobrbdeuXfD391cvOE1lnRYWFiIsLAy2trZwcnLCggULEBISgtDQUEyePBmRkZEICwvTfHcWvcZClCdPnqjddrTcDa2srAzDhw/HyJEjMXToUMybNw+urq5IT09HdnY2Jk6ciFOnTgGoH28uKCgIEydO1PTxtqSkBC+//DIiIyNx+fJl3L59G++99x5GjRoFPz8/2NjYYPDgwVAUBREREQC0e8OivLwcGRkZ2Llzp1EryDFjxkBRFGzevLnB3yQmJsLT0xP3799/hpUS/TgMUNqQ/uTl5MmTGDBgAI4dO2Y03dQYXoTpm8UDQHJyMvr164fy8vJ2rK516XQ67NixAx07doSHhwd++ctfYuHChYiIiICzs7Nmm9Y2pqk7YjqdDhUVFQgLC1NHzTeVbfv7tmUrKyuTaFr7Xd9359NUHD58GBYWFg2Cz4cPH2L58uVwcHDQfItAvaZaFhmemBcVFeHzzz9vj/KeGS23iHtahvvd8PBwREZGtl8xbaSkpATh4eEIDg7W7DhMLWG4Pz537hxWrFgBa2trkzi3KC4uRlRUFCZNmoSjR4/i2LFjCAwMVJ8o5O/vr35vCwsL1RZVWlZYWIjQ0FAEBwcbtYj75ptvsGfPHrz55pvw8fHR9PrNy8uDq6sr/Pz84OTkhP79++PAgQPq/NGjR8Pd3R0XL15Uj0P6cypTbSVIposByjPw73//G25ubg1GoTZFhgf9a9euYcOGDbCystL0QeH7XLlyBVOmTIG3tzfGjBmDhISEBk9rMQVNtURJSEiAl5eXSZzgfFdT27Iphid6ht0evjvonSmora01Cj7nzJmD119/HRMnTkT37t1Nbj/VVMsird7dpKbpQ5TY2Fi88sorJhkcFRUVITQ0FKGhoUbBNmA64b0h/f7YwcEBFhYWmnz0dlP0rYpCQkJQVFSEyspKXLp0CRMnTlSfwGNq67S4uFjdfs+fP99gvpbHK9I/cCAxMRGPHj1CTk4OevbsifDwcDx48EBdztfXF/3798e1a9eQnJwMS0tLkz6nItPFAOUZ2bt3Lzp16oQrV660dyltTn/Qd3Z2hrm5ucnvHPWD9Zm6xgIFW1tbk+2WBjx/2zJQ3+1hypQpJt3t4fLly4iKioKXlxdGjx6NxMRE9QkmpuZ5aFlE9b788kuMGjXKZLqyNMYw5NXqYLEtUVhYiIiICJNpGWeouLgYISEhCAkJeW72TY0NcK11TT2N09fXFwMGDMCDBw+MwiF9dx47OzuTCgXp+aIAgFCb+9e//iUzZsyQvXv3Sq9evdq7nDZXVFQkK1askJSUFPnJT37S3uW0KQCiKEqDn01RSUmJLF26VD799FP55ptv5NKlSzJs2LD2LqtNPU/bsl51dbVYWFi0dxltSqfTiZmZWXuX8Uzov7f/+c9/ZPPmzTJixIj2LonaSFVVlVhZWbV3GW2qsLBQkpOTZePGjdKnT5/2LqfN1dTUiLm5eXuX0SZKSkpk0aJFAkBWrlwpo0ePbu+S2pyp7Y/LysokOjpaevToIStWrJBRo0ZJamqqvPXWW+Lr6yvdunWTrl27yuDBg+X1118XS0tLmT9/vrzxxhvi4+PT3uUT/SAMUJ6h5+HExpApH/SfZ89joMBt2fQ8T8GnyPN30Umm7XkIeZ8XphYoPA1T2x/rgzALCwtxdnaWDz74QLZv3y5+fn5y7do1uXnzpmzbtk0ASFBQkOzbt8/kj7lk2higEFGLMVAg0h5edBLR/yJTCxSehqntj4uLiyUuLk5yc3Nl3bp1Eh8fbzT/q6++knPnzomXl5e4u7u3U5VErYMBChERERERtRtTCxSeR6WlpRIbGytmZmby5ptvql2yeNONTA0DFCIiIiIiIvpRDMe1SU5OllGjRrV3SUStrkN7F0BERERERETa5u7uLlu3bhVzc3OJj4+Xy5cvt3dJRK2OAQoRERERERH9aO7u7vLb3/5WevXqJS4uLu1dDlGrYxceIiIiIiIiajUc14ZMFQMUIiIiIiIiIqJmsAsPEREREREREVEzGKAQERERERERETWDAQoRERERERERUTMYoBARERERERERNYMBChERERERERFRMxigEBERERERERE1gwEKERHR/6DZs2fLpEmT1N8DAwNl8eLFz7yO8+fPi6Io8uDBgyaXURRFjh8//tSvuXr1avH29v5RdZWVlYmiKHL9+vUf9TpERERET4sBChER0VOaPXu2KIoiiqKIhYWFuLm5ydq1a6W2trbN3/vo0aOybt26p1r2aUIPIiIiImqZju1dABERkZaMHz9eMjMz5cmTJ3Lq1Cn51a9+Jebm5pKUlNRg2erqarGwsGiV93V0dGyV1yEiIiKiH4YtUIiIiFrA0tJSunfvLq6urrJw4UIJCgqSDz/8UES+7Xazfv16cXFxkYEDB4qISEVFhURHR4u9vb04OjpKZGSklJWVqa+p0+lk6dKlYm9vL05OTrJixQoBYPS+3+3C8+TJE0lISJDevXuLpaWluLm5ya5du6SsrEzGjRsnIiIODg6iKIrMnj1bRETq6uokNTVV+vXrJ9bW1uLl5SWHDx82ep9Tp07JgAEDxNraWsaNG2dU59NKSEiQAQMGiI2Njbz44ouSnJwsNTU1DZZLT0+X3r17i42NjURHR8vDhw+N5mdkZIinp6dYWVmJh4eHbN++vcW1EBEREbUWBihEREQ/grW1tVRXV6u/nz17VoqKiiQnJ0dOnDghNTU1EhoaKnZ2dpKbmysff/yx2Nrayvjx49W/27hxo2RlZcm7774rH330kXz99ddy7Nix733fWbNmyXvvvSdbt26VgoICSU9PF1tbW+ndu7ccOXJERESKiork3r17smXLFhERSU1NlT179siOHTvk5s2bsmTJEpkxY4ZcuHBBROqDnqioKPnFL34h169fl3nz5kliYmKL/yd2dnaSlZUl+fn5smXLFnnnnXdk8+bNRsvcunVLDh06JH/605/kL3/5i3z22WcSGxurzt+/f7/83//9n6xfv14KCgokJSVFkpOTZffu3S2uh4iIiKg1sAsPERHRDwBAzp49K6dPn5Y33nhDnd6pUyfJyMhQu+7s27dP6urqJCMjQxRFERGRzMxMsbe3l/Pnz0tISIj8/ve/l6SkJImKihIRkR07dsjp06ebfO/i4mI5dOiQ5OTkSFBQkIiIvPjii+p8fXcfZ2dnsbe3F5H6FispKSny17/+VUaOHKn+zUcffSTp6ekyduxYSUtLk/79+8vGjRtFRGTgwIFy48YN2bBhQ4v+NytXrlR/7tu3r8THx8vBgwdlxYoV6vSqqirZs2eP9OzZU0REtm3bJuHh4bJx40bp3r27rFq1SjZu3Kj+T/r16yf5+fmSnp4uMTExLaqHiIiIqDUwQCEiImqBEydOiK2trdTU1EhdXZ28+uqrsnr1anX+4MGDjcY9ycvLk1u3bomdnZ3R61RVVUlpaak8fPhQ7t27J/7+/uq8jh07yvDhwxt049G7fv26mJmZydixY5+67lu3bsnjx48lODjYaHp1dbX4+PiIiEhBQYFRHSKihi0tkZ2dLVu3bpXS0lKprKyU2tpa6dy5s9Eyffr0UcMT/fvU1dVJUVGR2NnZSWlpqcydO1fmz5+vLlNbWytdunRpcT1ERERErYEBChERUQuMGzdO0tLSxMLCQlxcXKRjR+NDaadOnYx+r6yslGHDhsn+/fsbvNYLL7zwg2qwtrZu8d9UVlaKiMjJkyeNgguR+nFdWsulS5dk+vTpsmbNGgkNDZUuXbrIwYMH1VYtLan1nXfeaRDomJmZtVqtRERERC3BAIWIiKgFOnXqJG5ubk+9/NChQyU7O1ucnZ0btMLQ69Gjh1y5ckUCAgJEpL6lxdWrV2Xo0KGNLj948GCpq6uTCxcuqF14DOlbwOh0OnXaoEGDxNLSUsrLy5tsueLp6akOiKt3+fLl5j+kgU8++URcXV3lrbfeUqfdvn27wXLl5eVy9+5dcXFxUd+nQ4cOMnDgQOnWrZu4uLjI559/LtOnT2/R+xMRERG1FQ4iS0RE1IamT58uXbt2lcjISMnNzZUvvvhCzp8/L4sWLZI7d+6IiMivf/1refvtt+X48eNSWFgosbGx8uDBgyZfs2/fvhITEyNz5syR48ePq6956NAhERFxdXUVRVHkxIkT8uWXX0plZaXY2dlJfHy8LFmyRHbv3i2lpaVy7do12bZtmzow64IFC6SkpESWL18uRUVFcuDAAcnKymrR53V3d5fy8nI5ePCglJaWytatWxsdENfKykpiYmIkLy9PcnNzZdGiRRIdHS3du3cXEZE1a9ZIamqqbN26VYqLi+XGjRuSmZkpmzZtalE9RERERK2FAQoREVEbsrGxkYsXL0qfPn0kKipKPD09Ze7cuVJVVaW2SFm2bJnMnDlTYmJiZOTIkWJnZycvv/zy975uWlqaTJkyRWJjY8XDw0Pmz58vjx49EhGRnj17ypo1ayQxMVG6desmcXFxIiKybt06SU5OltTUVPH09JTx48fLyZMnpV+/fiJSPy7JkSNH5Pjx4+Ll5SU7duyQlJSUFn3eiIgIWbJkicTFxYm3t7d88sknkpyc3GA5Nzc3iYqKkgkTJkhISIgMGTLE6DHF8+bNk4yMDMnMzJTBgwfL2LFjJSsrS62ViIiI6FlT0NQIdUREREREREREJCJsgUJERERERERE1CwGKEREREREREREzWCAQkRERERERETUDAYoRERERERERETNYIBCRERERERERNQMBihERERERERERM1ggEJERERERERE1AwGKEREREREREREzWCAQkRERERERETUDAYoRERERERERETNYIBCRERERERERNSM/wcLJG0smhskDAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = X_test.shape[-1]\n",
    "hidden_dim = config['hidden_dim']  # Replace with the hidden dimension used during training\n",
    "output_dim = config['classes']  # Replace with the number of output classes used during training\n",
    "model = LSTMModel(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# # Load label encoder\n",
    "label_encoder_path = '/content/drive/MyDrive/PhD/Colab Notebooks/label_encoder_2.pkl'\n",
    "le = joblib.load(label_encoder_path)\n",
    "\n",
    "# Load pretrained model\n",
    "model_path = '/content/drive/MyDrive/PhD/Colab Notebooks/trained_models/lstm_model_13_nothickness.pth'\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_test = X_test.unsqueeze(1).to(device)\n",
    "    y_test = torch.tensor(y_test).long().to(device)\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "# Decode the predicted labels\n",
    "\n",
    "# print(predicted)\n",
    "\n",
    "predicted_labels = le.inverse_transform(predicted.cpu().numpy())\n",
    "\n",
    "# Print the results\n",
    "print(\"Predicted labels:\", predicted_labels)\n",
    "print(\"Classes in label encoder:\", le.classes_)\n",
    "print(\"Number of classes:\", len(le.classes_))\n",
    "\n",
    "# Calculate percentage of correct predictions\n",
    "correct_predictions = (predicted == y_test).sum().item()\n",
    "total_samples = len(y_test)\n",
    "print(f\"Number of correct predictions: {correct_predictions}/{total_samples}\")\n",
    "accuracy = correct_predictions / total_samples * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Give the most repeated prediction\n",
    "most_repeated_prediction = Counter(predicted_labels).most_common(1)[0][0]\n",
    "print(f\"Most repeated prediction: {most_repeated_prediction}\")\n",
    "\n",
    "\n",
    "# Calculate the classification report\n",
    "print(classification_report(le.inverse_transform(y_test.cpu().numpy()), predicted_labels))\n",
    "\n",
    "## Confusion Matrix\n",
    "conf_matrix = confusion_matrix(le.inverse_transform(y_test.cpu().numpy()), predicted_labels, labels=le.classes_)\n",
    "\n",
    "# Confusion matrix with matplotlib\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(le.classes_))\n",
    "plt.xticks(tick_marks, le.classes_, rotation=45)\n",
    "plt.yticks(tick_marks, le.classes_)\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "conf_matrix_normalized = np.nan_to_num(conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis])\n",
    "\n",
    "# Print the normalized values inside the matrix as percentages\n",
    "thresh = conf_matrix.max() / 2.\n",
    "for i, j in np.ndindex(conf_matrix.shape):\n",
    "    plt.text(j, i, f\"{conf_matrix[i, j]}\\n ({conf_matrix_normalized[i, j] * 100:.1f}%)\",\n",
    "             horizontalalignment=\"center\",\n",
    "             verticalalignment=\"center\",\n",
    "             fontsize=9,\n",
    "             color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktsgpY21NcOV"
   },
   "source": [
    "| Original Label | Encoded Value |\n",
    "|----------------|---------------|\n",
    "| A1             | 0             |\n",
    "| B1             | 1             |\n",
    "| C1             | 2             |\n",
    "| D1             | 3             |\n",
    "| E1             | 4             |\n",
    "| E2             | 5             |\n",
    "| E3             | 6             |\n",
    "| F1             | 7             |\n",
    "| G1             | 8             |\n",
    "| H1             | 9             |\n",
    "| I1             | 10            |\n",
    "| J1             | 11            |\n",
    "| K1             | 12            |\n",
    "| L1             | 13            |\n",
    "| M1             | 14            |\n",
    "| N1             | 15            |\n",
    "| REF            | 16            |"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
