{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peUcVKtbGPfD"
   },
   "source": [
    "# Classification\n",
    "\n",
    "### Import Libraries and seed\n",
    "Import the necessary libraries for data processing, model building, training, and evaluation. Adding a seed ensures reproducibility by making sure that the random number generation is consistent across different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15527,
     "status": "ok",
     "timestamp": 1731066553135,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "9a9HvzrNG8iw",
    "outputId": "19350658-7b5d-48e7-e90c-e7ddb3a578b1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from math import e\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.decomposition import PCA, FastICA, IncrementalPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    return seed\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = set_seed(42)\n",
    "\n",
    "# Get the current directory of the notebook\n",
    "notebook_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1731066584724,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "hAY8BfVX9XCK"
   },
   "outputs": [],
   "source": [
    "def load_data_from_directory(input_path, return_file_count=False):\n",
    "    data_frames = []\n",
    "    file_count = 0\n",
    "    for file in os.listdir(input_path):\n",
    "        if file.endswith('.csv'):\n",
    "            file_count += 1\n",
    "            df = pd.read_csv(os.path.join(input_path, file), delimiter=';', header=0)\n",
    "            data_frames.append(df)\n",
    "    data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    if return_file_count:\n",
    "        return data, file_count\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiCTw-qcKXhn"
   },
   "source": [
    "### Preprocessing Data\n",
    "Function to preprocess the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1731066584724,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "zJoZqi5LDPl8"
   },
   "outputs": [],
   "source": [
    "def calculate_averages_and_dispersion(df, data_percentage):\n",
    "\n",
    "    results = []\n",
    "    for (sample, freq), group in df.groupby(['Sample', 'Frequency (GHz)']):\n",
    "        window_size = max(1, int(len(group) * data_percentage / 100))\n",
    "        # print(f\"Processing sample: {sample}, frequency: {freq} with window size: {window_size}\")\n",
    "        for start in range(0, len(group), window_size):\n",
    "            window_data = group.iloc[start:start + window_size]\n",
    "            mean_values = window_data[['LG (mV)', 'HG (mV)']].mean()\n",
    "            std_deviation_values = window_data[['LG (mV)', 'HG (mV)']].std()\n",
    "            results.append({\n",
    "                'Frequency (GHz)': freq,\n",
    "                'LG (mV) mean': mean_values['LG (mV)'],\n",
    "                'HG (mV) mean': mean_values['HG (mV)'],\n",
    "                'LG (mV) std deviation': std_deviation_values['LG (mV)'],\n",
    "                'HG (mV) std deviation': std_deviation_values['HG (mV)'],\n",
    "                'Sample': sample,\n",
    "            })\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXiN30xA1kKX"
   },
   "source": [
    "### Pivoting Frequency values to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1731067118965,
     "user": {
      "displayName": "DANIEL MORENO PARIS",
      "userId": "17921422726169854440"
     },
     "user_tz": -60
    },
    "id": "Kijzt73J1nyo"
   },
   "outputs": [],
   "source": [
    "def freq_as_variable(df, data_percentage):\n",
    "    '''Modify df to have Frequency values (100,110,120 and so on) as input variables in the columns'''\n",
    "\n",
    "    if data_percentage > 0:\n",
    "        # 1s window_size 100/27s = 3.7% of the data is used for each window\n",
    "        df_window = calculate_averages_and_dispersion(df, data_percentage) \n",
    "\n",
    "        # Add a unique identifier column to avoid duplicate entries in the index\n",
    "        df_window['unique_id'] = df_window.groupby(['Sample', 'Frequency (GHz)']).cumcount()\n",
    "\n",
    "        # Pivot the DataFrame to wide format\n",
    "        df_pivot = df_window.pivot(index=['Sample', 'unique_id'], columns='Frequency (GHz)')\n",
    "\n",
    "        # Flatten the MultiIndex columns - Ordered by Frequency + (HG mean, HG std deviation, LG mean, LG std deviation)\n",
    "        df_pivot.columns = [' '.join([str(col[1]), str(col[0])]) for col in df_pivot.columns]\n",
    "\n",
    "        # Drop columns with all NaN values\n",
    "        df_pivot = df_pivot.dropna(axis=1, how='all')\n",
    "\n",
    "        # Reset index to make 'Sample' and 'unique_id' columns again\n",
    "        df_pivot = df_pivot.reset_index()\n",
    "\n",
    "        # Remove 'unique_id' column\n",
    "        df_pivot = df_pivot.drop(columns=['unique_id'])\n",
    "    else:\n",
    "        # If data_percentage is 0, do not calculate mean and std deviation, use the original data\n",
    "        df['unique_id'] = df.groupby(['Sample', 'Frequency (GHz)']).cumcount()\n",
    "        df_pivot = df.pivot(index=['Sample', 'unique_id'], columns='Frequency (GHz)')\n",
    "        df_pivot.columns = [' '.join([str(col[1]), str(col[0])]) for col in df_pivot.columns]\n",
    "        df_pivot = df_pivot.dropna(axis=1, how='all')\n",
    "        df_pivot = df_pivot.reset_index()\n",
    "        df_pivot = df_pivot.drop(columns=['unique_id'])\n",
    "\n",
    "    # Optional - Sort the columns if needed\n",
    "    df_pivot = df_pivot.reindex(sorted(df_pivot.columns), axis=1)\n",
    "\n",
    "    return df_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from directory in defined time windows and pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_data(notebook_dir, input_dir='data/experiment_5_plastics/processed/', \n",
    "                           test_percentage=0.2, time_window_s=0.1, stabilised_time_s=0.1, split = False, verbose=True):\n",
    "    \"\"\" Load and prepare training and testing data for model development \"\"\"\n",
    "\n",
    "    # Load training data\n",
    "    input_path = os.path.normpath(os.path.join(notebook_dir, '..', '..', input_dir))\n",
    "    df, num_files = load_data_from_directory(input_path, return_file_count=True)\n",
    "    df = pd.concat([df[['Frequency (GHz)', 'LG (mV)', 'HG (mV)']], df[['Sample']]], axis=1)\n",
    "    \n",
    "    # Clean up sample labels - take first character only\n",
    "    df['Sample'] = df['Sample'].str[0]\n",
    "    \n",
    "    # Get unique labels\n",
    "    labels = df['Sample'].unique()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Training data head:\")\n",
    "        print(df.head(10))\n",
    "        print(f\"Unique labels: {labels}\")\n",
    "        print(f\"Window size: {time_window_s} s\")\n",
    "\n",
    "    num_tests = num_files/df['Sample'].nunique()\n",
    "    stabilised_time_s = 12 * 1\n",
    "    # print('Tests per sample: ', num_tests)\n",
    "    # print('Stabilised time: ', stabilised_time_s)\n",
    "    \n",
    "    # Calculate the percentage of data used for each window\n",
    "    if test_percentage == 0:\n",
    "        data_percentage = (100/stabilised_time_s) * time_window_s\n",
    "        data_percentage_test = 0  # No test data\n",
    "    else:\n",
    "        data_percentage = (100/(stabilised_time_s*(1-test_percentage))) * time_window_s\n",
    "        data_percentage_test = (100/(stabilised_time_s*(test_percentage))) * time_window_s\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Data percentage for training: {data_percentage}%\")\n",
    "        print(f\"Data percentage for testing: {data_percentage_test}%\")\n",
    "\n",
    "    if split:\n",
    "    \n",
    "        # Split into train and test sets\n",
    "        if test_percentage == 0:\n",
    "            df_train = df.copy()\n",
    "            df_test = pd.DataFrame(columns=df.columns)  # Empty DataFrame with same structure\n",
    "        else:\n",
    "            df_train, df_test = train_test_split(df, test_size=test_percentage, random_state=seed) \n",
    "\n",
    "    else:\n",
    "        # Use all data as training data\n",
    "        df_train = df.copy()\n",
    "        df_train = freq_as_variable(df_train, data_percentage)\n",
    "        if verbose:\n",
    "            print(f\"Training set shape: {df_train.shape}\")\n",
    "        return df_train, labels\n",
    "\n",
    "        \n",
    "    # Introduce Frequency values as input variables\n",
    "    df_train = freq_as_variable(df_train, data_percentage)\n",
    "\n",
    "    # Only process test data if it exists\n",
    "    if not df_test.empty:\n",
    "        df_test = freq_as_variable(df_test, data_percentage_test)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Training set shape: {df_train.shape}\")\n",
    "        print(f\"Testing set shape: {df_test.shape}\")\n",
    "    \n",
    "    return df_train, df_test, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data\n",
    "### Split data into X and y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, labels, freqs, eliminate_std_dev=False, eliminate_LG=False, drop_sample=True):\n",
    "    # Reduce number of different samples for testing\n",
    "    X_ = df[df['Sample'].isin(labels)]\n",
    "\n",
    "    y_ = X_['Sample']\n",
    "\n",
    "    if drop_sample:\n",
    "        X_ = X_.drop(columns=['Sample'])\n",
    "        \n",
    "    if freqs:\n",
    "        # Subset of specific frequencies to use as input features (or without mean)\n",
    "        columns = [f'{freq}.0 HG (mV) mean' for freq in freqs] + \\\n",
    "                  [f'{freq}.0 LG (mV) mean' for freq in freqs] + \\\n",
    "                  [f'{freq}.0 HG (mV)' for freq in freqs] + \\\n",
    "                  [f'{freq}.0 LG (mV)' for freq in freqs] + \\\n",
    "                  [f'{freq}.0 HG (mV) std deviation' for freq in freqs] + \\\n",
    "                  [f'{freq}.0 LG (mV) std deviation' for freq in freqs] + \\\n",
    "                  ['Sample']\n",
    "\n",
    "\n",
    "        # Filter columns that exist in X_\n",
    "        existing_columns = [col for col in columns if col in X_.columns]\n",
    "\n",
    "        # Check if existing_columns is empty\n",
    "        if not existing_columns:\n",
    "            print(\"No matching columns found in X_.\")\n",
    "        else:\n",
    "            X_ = X_[existing_columns]\n",
    "\n",
    "        # Sort columns by frequency value\n",
    "        X_ = X_.reindex(sorted(X_.columns), axis=1)\n",
    "\n",
    "    if eliminate_std_dev:\n",
    "        # Eliminate std dev columns from the input features\n",
    "        X_ = X_.drop(columns=[col for col in X_.columns if 'std deviation' in col])\n",
    "\n",
    "    if eliminate_LG:\n",
    "        # Eliminate LG columns from the input features\n",
    "        X_ = X_.drop(columns=[col for col in X_.columns if 'LG' in col])\n",
    "\n",
    "    return X_, y_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afvxx1KzKzYp"
   },
   "source": [
    "### Define Models\n",
    "- Random Forest\n",
    "- Naive-Bayes\n",
    "- Logistic Regression\n",
    "- Gradient Boosting\n",
    "- Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_model(X_train, y_train, seed):\n",
    "    rf_model = RandomForestClassifier(random_state=seed)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    return rf_model\n",
    "\n",
    "def naive_bayes_model(X_train, y_train):\n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    return nb_model\n",
    "\n",
    "def logistic_regression_model(X_train, y_train, seed):\n",
    "    lr_model = LogisticRegression(max_iter=1000,\n",
    "                                  random_state=seed)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    return lr_model\n",
    "\n",
    "def gradient_boosting_model(X_train, y_train, seed):\n",
    "    gb_model = GradientBoostingClassifier(random_state=seed)\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    return gb_model\n",
    "\n",
    "def support_vector_machine_model(X_train, y_train, seed):\n",
    "    svm_model = SVC(random_state=seed)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    return svm_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train all Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X_train, y_train, seed):\n",
    "    training_times = []\n",
    "    \n",
    "    # Random Forest\n",
    "    start_time = time.time()\n",
    "    rf_model = RandomForestClassifier(random_state=seed)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    training_times.append(time.time() - start_time)\n",
    "    \n",
    "    # Naive Bayes\n",
    "    start_time = time.time()\n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    training_times.append(time.time() - start_time)\n",
    "    \n",
    "    # Logistic Regression\n",
    "    start_time = time.time()\n",
    "    lr_model = LogisticRegression(random_state=seed, max_iter=1000)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    training_times.append(time.time() - start_time)\n",
    "    \n",
    "    # Gradient Boosting\n",
    "    start_time = time.time()\n",
    "    gb_model = GradientBoostingClassifier(random_state=seed)\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    training_times.append(time.time() - start_time)\n",
    "    \n",
    "    # SVM\n",
    "    start_time = time.time()\n",
    "    svm_model = SVC(random_state=seed)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    training_times.append(time.time() - start_time)\n",
    "    \n",
    "    return rf_model, nb_model, lr_model, gb_model, svm_model, training_times\n",
    "\n",
    "def save_models(rf_model, nb_model, lr_model, gb_model, svm_model):\n",
    "    joblib.dump(rf_model, 'random_forest_model.pkl')\n",
    "    joblib.dump(nb_model, 'naive_bayes_model.pkl')\n",
    "    joblib.dump(lr_model, 'logistic_regression_model.pkl')\n",
    "    joblib.dump(gb_model, 'gradient_boosting_model.pkl')\n",
    "    joblib.dump(svm_model, 'support_vector_machine_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply confidence threshold\n",
    "def apply_confidence_threshold(probabilities, threshold=0.7):\n",
    "    max_probs = np.max(probabilities, axis=1)\n",
    "    predictions = np.argmax(probabilities, axis=1)\n",
    "    # Replace predictions with -1 (unknown) where confidence is below threshold\n",
    "    predictions[max_probs < threshold] = -1\n",
    "    return predictions, max_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIC Criteria\n",
    "def aic_score(y_true, y_pred, n_features):\n",
    "    n = len(y_true)\n",
    "    residuals = y_true - y_pred\n",
    "    rss = np.sum(residuals**2)\n",
    "    epsilon = 1e-10  # Small value to avoid log(0)\n",
    "    aic = 2 * n_features + n * np.log((rss + epsilon) / n)\n",
    "    return aic\n",
    "\n",
    "# BIC Criteria (Also consider the number of samples)\n",
    "def bic_score(y_true, y_pred, n_features):\n",
    "    n = len(y_true)\n",
    "    residuals = y_true - y_pred\n",
    "    rss = np.sum(residuals**2)\n",
    "    epsilon = 1e-10  # Small value to avoid log(0)\n",
    "    bic = np.log(n) * n_features + n * np.log((rss + epsilon) / n)\n",
    "    return bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_results(models, data, time_window_s, accuracies, precisions, recalls, f1_scores, predictions,\n",
    "                        n_features, freqs, aic_scores, bic_scores, training_times=None, \n",
    "                        inference_times=None, csv_path='comparison_results.csv', verbose=0):\n",
    "    \n",
    "    # Determine test number\n",
    "    if os.path.exists(csv_path):\n",
    "        existing_results = pd.read_csv(csv_path, sep=';')\n",
    "        current_test = existing_results['Test'].max() + 1\n",
    "    else:\n",
    "        current_test = 1\n",
    "\n",
    "    # Initialize timing data if not provided\n",
    "    if training_times is None:\n",
    "        training_times = [0] * len(models)\n",
    "    if inference_times is None:\n",
    "        inference_times = [0] * len(models)\n",
    "\n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'Test': [current_test] * len(models),\n",
    "        'Data': [data] * len(models),\n",
    "        'Time Window (s)': time_window_s,\n",
    "        'Accuracy': accuracies,\n",
    "        'Precision': precisions,\n",
    "        'Recall': recalls,\n",
    "        'F1-Score': f1_scores,\n",
    "        'AIC': aic_scores,\n",
    "        'BIC': bic_scores,\n",
    "        'Predictions': predictions,\n",
    "        'Algorithm': models,\n",
    "        'Num_Features': [n_features] * len(models),\n",
    "        'Num Frequencies': [len(freqs)] * len(models),\n",
    "        'Frequencies': [', '.join([f\"{freq:.0f}\" for freq in freqs])] * len(models),\n",
    "        'Training_Time': training_times,\n",
    "        'Inference_Time_Per_Sample': inference_times\n",
    "    })\n",
    "\n",
    "    # Format float columns\n",
    "    float_columns = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AIC', 'BIC', \n",
    "                    'Training_Time', 'Inference_Time_Per_Sample']\n",
    "    results_df[float_columns] = results_df[float_columns].round(4)\n",
    "\n",
    "    # Append or create results file\n",
    "    if os.path.exists(csv_path):\n",
    "        updated_results = pd.concat([existing_results, results_df], ignore_index=True)\n",
    "    else:\n",
    "        updated_results = results_df\n",
    "\n",
    "    # Save updated results\n",
    "    updated_results.to_csv(csv_path, index=False, sep=';')\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print(f\"\\nTest #{current_test} Results:\")\n",
    "        print(results_df)\n",
    "        if verbose >= 2:\n",
    "            print(\"\\nAll Results:\")\n",
    "            print(updated_results)\n",
    "        \n",
    "    return updated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(y_test, y_pred, verbose=0):\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    aic_scores = []\n",
    "    bic_scores = []\n",
    "    predictions_list = []\n",
    "\n",
    "    # Label encode y_test\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_test)\n",
    "\n",
    "    for i, y_pred_i in enumerate(y_pred):\n",
    "        accuracy = accuracy_score(y_test, y_pred_i)\n",
    "        precision = precision_score(y_test, y_pred_i, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred_i, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred_i, average='weighted', zero_division=0)\n",
    "\n",
    "        # Calculate AIC and BIC\n",
    "        y_test_num = le.transform(y_test)\n",
    "        y_pred_num = le.transform(y_pred_i)\n",
    "        n_features = len(y_pred_i)\n",
    "        aic = aic_score(y_test_num, y_pred_num, n_features)\n",
    "        bic = bic_score(y_test_num, y_pred_num, n_features)\n",
    "\n",
    "        # Predictions made per class with its label encoded value\n",
    "        predictions = dict(sorted(Counter(y_pred_i).items()))\n",
    "\n",
    "        print(f\"Model {i+1} - Accuracy: {accuracy}\")\n",
    "\n",
    "        if verbose >= 1:\n",
    "            print(f\"Model {i+1} - Precision: {precision}\")\n",
    "            print(f\"Model {i+1} - Recall: {recall}\")\n",
    "            print(f\"Model {i+1} - F1: {f1}\")\n",
    "            print(f\"Model {i+1} - AIC: {aic}\")\n",
    "            print(f\"Model {i+1} - BIC: {bic}\")\n",
    "            print(f\"Model {i+1} - Predictions: {predictions}\\n\")\n",
    "            if verbose >= 2:\n",
    "                # Classification report\n",
    "                print(f\"Model {i+1} - Classification Report:\\n\", classification_report(y_test, y_pred_i), '\\n')\n",
    "\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        aic_scores.append(aic)\n",
    "        bic_scores.append(bic)\n",
    "        predictions_list.append(predictions)\n",
    "\n",
    "    return accuracies, precisions, recalls, f1_scores, aic_scores, bic_scores, predictions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importances(rf_model, lr_model, gb_model, nb_model, svm_model, X_train, y_train, seed, plot=True, n=10):\n",
    "    feature_names = X_train.columns\n",
    "\n",
    "    # Random Forest feature importances\n",
    "    rf_feature_importances = rf_model.feature_importances_\n",
    "    rf_feature_importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': rf_feature_importances})\n",
    "    rf_feature_importances_df = rf_feature_importances_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "    # Logistic Regression feature importances\n",
    "    lr_feature_importances = lr_model.coef_[0]\n",
    "    lr_feature_importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': lr_feature_importances})\n",
    "    lr_feature_importances_df = lr_feature_importances_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "    # Gradient Boosting feature importances\n",
    "    gb_feature_importances = gb_model.feature_importances_\n",
    "    gb_feature_importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': gb_feature_importances})\n",
    "    gb_feature_importances_df = gb_feature_importances_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "    # Naive Bayes permutation importance\n",
    "    result_nb = permutation_importance(nb_model, X_train, y_train, n_repeats=5, random_state=seed, n_jobs=1)\n",
    "    sorted_idx_nb = result_nb.importances_mean.argsort()[::-1]\n",
    "    nb_feature_importances_df = pd.DataFrame({'Feature': feature_names[sorted_idx_nb], 'Importance': result_nb.importances_mean[sorted_idx_nb]})\n",
    "\n",
    "    # SVM permutation importance\n",
    "    result_svm = permutation_importance(svm_model, X_train, y_train, n_repeats=5, random_state=seed, n_jobs=1)\n",
    "    sorted_idx_svm = result_svm.importances_mean.argsort()[::-1]\n",
    "    svm_feature_importances_df = pd.DataFrame({'Feature': feature_names[sorted_idx_svm], 'Importance': result_svm.importances_mean[sorted_idx_svm]})\n",
    "\n",
    "    if plot:\n",
    "        # Set standard font family\n",
    "        plt.rcParams['font.family'] = 'Arial'  # or 'Arial', 'Times New Roman', etc.\n",
    "        \n",
    "        # Create directory for saving feature importance plots\n",
    "        feature_imp_path = os.path.normpath(os.path.join(notebook_dir, '..', '..', 'data/results/feature_importance_detailed/'))\n",
    "        if not os.path.exists(feature_imp_path):\n",
    "            os.makedirs(feature_imp_path)\n",
    "\n",
    "        # Define enhanced color schemes for each model\n",
    "        colors = {\n",
    "            'RF': plt.cm.viridis(np.linspace(0.2, 0.8, n)),\n",
    "            'LR': plt.cm.plasma(np.linspace(0.2, 0.8, n)),\n",
    "            'GB': plt.cm.inferno(np.linspace(0.2, 0.8, n)),\n",
    "            'NB': plt.cm.cividis(np.linspace(0.2, 0.8, n)),\n",
    "            'SVM': plt.cm.magma(np.linspace(0.2, 0.8, n))\n",
    "        }\n",
    "\n",
    "        # Random Forest Plot\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        bars = ax.barh(rf_feature_importances_df['Feature'][:n], \n",
    "                      rf_feature_importances_df['Importance'][:n],\n",
    "                      color=colors['RF'], \n",
    "                      edgecolor='white', \n",
    "                      linewidth=0.8,\n",
    "                      alpha=0.85)\n",
    "        \n",
    "        # Add gradient effect to bars\n",
    "        for i, bar in enumerate(bars):\n",
    "            bar.set_facecolor(colors['RF'][i])\n",
    "        \n",
    "        ax.set_xlabel('Importance', fontsize=20, color='#2E2E2E', family='DejaVu Sans')\n",
    "        ax.set_title('Random Forest Feature Importances', fontsize=22, \n",
    "                    color='#2E2E2E', pad=20, family='DejaVu Sans')\n",
    "        ax.tick_params(axis='x', labelsize=18, colors='#2E2E2E')\n",
    "        ax.tick_params(axis='y', labelsize=18, colors='#2E2E2E')\n",
    "        \n",
    "        # Enhanced grid styling\n",
    "        ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.8, color='gray')\n",
    "        ax.set_axisbelow(True)\n",
    "        \n",
    "        # Subtle background gradient\n",
    "        ax.patch.set_facecolor('#FAFAFA')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(feature_imp_path, 'RF_detailed_feature_importance.pdf'), \n",
    "                   format='pdf', bbox_inches='tight', dpi=300, facecolor='white')\n",
    "        plt.show()\n",
    "\n",
    "        # Logistic Regression Plot\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        bars = ax.barh(lr_feature_importances_df['Feature'][:n], \n",
    "                      lr_feature_importances_df['Importance'][:n],\n",
    "                      color=colors['LR'], \n",
    "                      edgecolor='white', \n",
    "                      linewidth=0.8,\n",
    "                      alpha=0.85)\n",
    "        \n",
    "        for i, bar in enumerate(bars):\n",
    "            bar.set_facecolor(colors['LR'][i])\n",
    "        \n",
    "        ax.set_xlabel('Importance', fontsize=18, color='#2E2E2E', family='DejaVu Sans')\n",
    "        ax.set_title('Logistic Regression Feature Importances', fontsize=22, \n",
    "                    color='#2E2E2E', pad=20, family='DejaVu Sans')\n",
    "        ax.tick_params(axis='x', labelsize=16, colors='#2E2E2E')\n",
    "        ax.tick_params(axis='y', labelsize=16, colors='#2E2E2E')\n",
    "        ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.8, color='gray')\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.patch.set_facecolor('#FAFAFA')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(feature_imp_path, 'LR_detailed_feature_importance.pdf'), \n",
    "                   format='pdf', bbox_inches='tight', dpi=300, facecolor='white')\n",
    "        plt.show()\n",
    "\n",
    "        # Gradient Boosting Plot\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        bars = ax.barh(gb_feature_importances_df['Feature'][:n], \n",
    "                      gb_feature_importances_df['Importance'][:n],\n",
    "                      color=colors['GB'], \n",
    "                      edgecolor='white', \n",
    "                      linewidth=0.8,\n",
    "                      alpha=0.85)\n",
    "        \n",
    "        for i, bar in enumerate(bars):\n",
    "            bar.set_facecolor(colors['GB'][i])\n",
    "        \n",
    "        ax.set_xlabel('Importance', fontsize=18, color='#2E2E2E', family='DejaVu Sans')\n",
    "        ax.set_title('Gradient Boosting Feature Importances', fontsize=22, \n",
    "                    color='#2E2E2E', pad=20, family='DejaVu Sans')\n",
    "        ax.tick_params(axis='x', labelsize=16, colors='#2E2E2E')\n",
    "        ax.tick_params(axis='y', labelsize=16, colors='#2E2E2E')\n",
    "        ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.8, color='gray')\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.patch.set_facecolor('#FAFAFA')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(feature_imp_path, 'GB_detailed_feature_importance.pdf'), \n",
    "                   format='pdf', bbox_inches='tight', dpi=300, facecolor='white')\n",
    "        plt.show()\n",
    "\n",
    "        # Naive Bayes Plot (Enhanced Boxplot)\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        bp = ax.boxplot(result_nb.importances[sorted_idx_nb][:n].T, \n",
    "                       vert=False, \n",
    "                       labels=X_train.columns[sorted_idx_nb][:n],\n",
    "                       patch_artist=True,\n",
    "                       boxprops=dict(facecolor='#8E44AD', alpha=0.8, linewidth=1.5),\n",
    "                       whiskerprops=dict(color='#2E2E2E', linewidth=2),\n",
    "                       capprops=dict(color='#2E2E2E', linewidth=2),\n",
    "                       medianprops=dict(color='white', linewidth=3),\n",
    "                       flierprops=dict(marker='o', markerfacecolor='#E74C3C', markersize=8, alpha=0.8, markeredgecolor='white'))\n",
    "        \n",
    "        ax.set_xlabel('Permutation Importance', fontsize=18, color='#2E2E2E', family='DejaVu Sans')\n",
    "        ax.set_title('Naive Bayes Permutation Feature Importance', fontsize=22, \n",
    "                    color='#2E2E2E', pad=20, family='DejaVu Sans')\n",
    "        ax.tick_params(axis='x', labelsize=16, colors='#2E2E2E')\n",
    "        ax.tick_params(axis='y', labelsize=16, colors='#2E2E2E')\n",
    "        ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.8, color='gray')\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.patch.set_facecolor('#FAFAFA')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(feature_imp_path, 'NB_detailed_feature_importance.pdf'), \n",
    "                   format='pdf', bbox_inches='tight', dpi=300, facecolor='white')\n",
    "        plt.show()\n",
    "\n",
    "        # SVM Plot (Enhanced Boxplot)\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        bp = ax.boxplot(result_svm.importances[sorted_idx_svm][:n].T, \n",
    "                       vert=False, \n",
    "                       labels=X_train.columns[sorted_idx_svm][:n],\n",
    "                       patch_artist=True,\n",
    "                       boxprops=dict(facecolor='#E67E22', alpha=0.8, linewidth=1.5),\n",
    "                       whiskerprops=dict(color='#2E2E2E', linewidth=2),\n",
    "                       capprops=dict(color='#2E2E2E', linewidth=2),\n",
    "                       medianprops=dict(color='white', linewidth=3),\n",
    "                       flierprops=dict(marker='o', markerfacecolor='#E74C3C', markersize=8, alpha=0.8, markeredgecolor='white'))\n",
    "        \n",
    "        ax.set_xlabel('Permutation Importance', fontsize=18, color='#2E2E2E', family='DejaVu Sans')\n",
    "        ax.set_title('SVM Permutation Feature Importance', fontsize=22, \n",
    "                    color='#2E2E2E', pad=20, family='DejaVu Sans')\n",
    "        ax.tick_params(axis='x', labelsize=16, colors='#2E2E2E')\n",
    "        ax.tick_params(axis='y', labelsize=16, colors='#2E2E2E')\n",
    "        ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.8, color='gray')\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.patch.set_facecolor('#FAFAFA')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(feature_imp_path, 'SVM_detailed_feature_importance.pdf'), \n",
    "                   format='pdf', bbox_inches='tight', dpi=300, facecolor='white')\n",
    "        plt.show()\n",
    "\n",
    "    return rf_feature_importances_df, lr_feature_importances_df, gb_feature_importances_df, nb_feature_importances_df, svm_feature_importances_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_pdf(y_true, y_pred, labels, save_path=None, model_name=None):\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    cmap=plt.cm.Blues\n",
    "    cax = ax.matshow(conf_matrix, cmap=cmap)\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "    # Determine text color based on cell value for better visibility\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            # Calculate percentage\n",
    "            percentage = conf_matrix[i, j] / np.sum(conf_matrix, axis=1)[i] * 100 if np.sum(conf_matrix, axis=1)[i] > 0 else 0\n",
    "            \n",
    "            # Determine text color based on cell darkness\n",
    "            cell_value = conf_matrix[i, j]\n",
    "            if cell_value > conf_matrix.max() / 3:\n",
    "                text_color = 'white'\n",
    "\n",
    "                plt.text(j, i, f'{conf_matrix[i, j]}\\n{percentage:.1f}%',\n",
    "                     horizontalalignment=\"center\",\n",
    "                     verticalalignment=\"center\",\n",
    "                     fontsize=8, \n",
    "                     ha='center', va='center', \n",
    "                     color=text_color)\n",
    "            else:\n",
    "                text_color = cmap(1.0)\n",
    "\n",
    "                if conf_matrix[i, j] != 0:\n",
    "\n",
    "                    plt.text(j, i, f'{conf_matrix[i, j]}\\n{percentage:.1f}%',\n",
    "                        horizontalalignment=\"center\",\n",
    "                        verticalalignment=\"center\",\n",
    "                        fontsize=8, \n",
    "                        ha='center', va='center', \n",
    "                        color=text_color)\n",
    "            \n",
    "\n",
    "    \n",
    "    plt.xlabel('Predicted', fontweight='bold', fontsize=12)\n",
    "    plt.ylabel('True', fontweight='bold', fontsize=12)\n",
    "    plt.xticks(np.arange(len(labels)), labels, rotation=45, fontweight='bold')\n",
    "    plt.yticks(np.arange(len(labels)), labels, fontweight='bold')\n",
    "    plt.title('Confusion Matrix', fontweight='bold', fontsize=14)\n",
    "        \n",
    "    # Adjust layout to make room for rotated x labels\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot if a path is provided\n",
    "    if save_path:\n",
    "        # Create directory if it doesn't exist\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        \n",
    "        # Create filename\n",
    "        model_suffix = f\"_{model_name}\" if model_name else \"\"\n",
    "        filename = f\"confusion_matrix{model_suffix}.pdf\"\n",
    "        filepath = os.path.join(save_path, filename)\n",
    "        \n",
    "        # Save as PDF\n",
    "        plt.savefig(filepath, format='pdf', bbox_inches='tight', dpi=300)\n",
    "        print(f\"Confusion matrix saved to: {filepath}\")\n",
    "    else:\n",
    "        print(\"Confusion matrix plot not saved.\")\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(X, y, subset_freqs, HG_diff=True, LG_diff=True):\n",
    "\n",
    "    X['Sample'] = y\n",
    "\n",
    "    # Initialize a dictionary to store results\n",
    "    mean_std_dict = {}\n",
    "\n",
    "    for freq in subset_freqs:\n",
    "        # Calculate HG and LG mean values for each frequency\n",
    "        agg_dict = {}\n",
    "        if f'{freq}.0 LG (mV) mean' in X.columns:\n",
    "            agg_dict['LG_mean'] = (f'{freq}.0 LG (mV) mean', 'mean')\n",
    "        if f'{freq}.0 HG (mV) mean' in X.columns:\n",
    "            agg_dict['HG_mean'] = (f'{freq}.0 HG (mV) mean', 'mean')\n",
    "        \n",
    "        mean_std_dict[freq] = X.groupby('Sample').agg(**agg_dict).reset_index()\n",
    "\n",
    "        mean_std_dict[freq]['Frequency'] = freq\n",
    "\n",
    "    # Concatenate all DataFrames in the dictionary\n",
    "    mean_std_df = pd.concat(mean_std_dict.values(), ignore_index=True)\n",
    "\n",
    "    # For each frequency after first one\n",
    "    for i, freq in enumerate(subset_freqs[1:]):\n",
    "        prev_freq = subset_freqs[i]  # Get previous frequency\n",
    "        \n",
    "        # For each row\n",
    "        for idx, row in X.iterrows():\n",
    "            sample = row['Sample']\n",
    "            \n",
    "            if HG_diff:\n",
    "                # Get previous frequency's HG mean for this sample\n",
    "                prev_hg = mean_std_df[\n",
    "                    (mean_std_df['Frequency'] == prev_freq) & \n",
    "                    (mean_std_df['Sample'] == sample)\n",
    "                ]['HG_mean'].values[0]\n",
    "\n",
    "\n",
    "                # 1) Inputs: xt - (xt-1) --First-order differences\n",
    "                # 2) Inputs: (xt/(xt-1)) - 1 --Escalado relativo\n",
    "\n",
    "                # Calculate and store difference\n",
    "                X.loc[idx, f'{freq}.0 HG diff'] = X.loc[idx, f'{freq}.0 HG (mV) mean'] - prev_hg\n",
    "                # X.loc[idx, f'{freq}.0 HG relative diff'] = (X.loc[idx, f'{freq}.0 HG (mV) mean'] / prev_hg) -1\n",
    "\n",
    "\n",
    "            if LG_diff:\n",
    "                prev_lg = mean_std_df[\n",
    "                    (mean_std_df['Frequency'] == prev_freq) & \n",
    "                    (mean_std_df['Sample'] == sample)\n",
    "                ]['LG_mean'].values[0]\n",
    "\n",
    "                # Calculate and store difference\n",
    "                # X.loc[idx, f'{freq}.0 LG diff'] = X.loc[idx, f'{freq}.0 LG (mV) mean'] - prev_lg\n",
    "                X.loc[idx, f'{freq}.0 LG relative diff'] = (X.loc[idx, f'{freq}.0 LG (mV) mean'] / prev_lg) -1\n",
    "\n",
    "\n",
    "    # Drop the 'Sample' column\n",
    "    X = X.drop(columns=['Sample'])\n",
    "\n",
    "    return X\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load New Test Data\n",
    "Prepare new sample for testing (Testing other samples, out of initial dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Subset of specific frequencies to use as input features, If freqs = [] then all frequencies are used\n",
    "\n",
    "freqs = []\n",
    "\n",
    "# Add a group with all frequencies from 100 to 590\n",
    "# freqs.append(list(range(260, 591, 10)))\n",
    "freqs.append(list(range(100, 591, 10)))\n",
    "\n",
    "\n",
    "# Favourite frequencies\n",
    "# freqs.append([250,300,320,330,340,350,360,370,380,390,400,410,420,430,440,450,460,470,480,490,500,510,520,530,540]) # TOP 25\n",
    "# freqs.append([250,310,320,330,340,350,360,370,380,390,400,410,420,430,440,450,460,470,480,490,500,510,520,540]) # TOP 24\n",
    "\n",
    "freqs.append([310,320,330,340,350,360,370,380,390,400,410,420,430,440,450,460,470,480,490,540]) # TOP 20 88% SG(3,2)\n",
    "# freqs.append([310,320,330,340,350,360,370,400,410,420,430,440,450,460,470,480,490,500,510,540]) # TOP 20 88% SG(3,2)\n",
    "# freqs.append([300,310,320,330,340,350,360,370,380,390,400,410,420,430,440,450,460,470,480,490]) # TOP 20 88% SG(3,2)\n",
    "\n",
    "# freqs.append([310,320,330,340,350,360,370,380,390,400,410,420]) # TOP 12 (86%)\n",
    "freqs.append([310,320,330,340,350,360,370,400,410,420]) # TOP 10 (88%)\n",
    "freqs.append([330,340,350,360,410]) # TOP 5 86% SG\n",
    "\n",
    "freqs.append([340,350,360]) # TOP 3\n",
    "freqs.append([350]) # TOP 1\n",
    "\n",
    "\n",
    "# # Count number of frequencies\n",
    "# print(f'freqs number: {len(freqs)}')\n",
    "\n",
    "# Array with subsets of frequencies to use for training\n",
    "# freqs.extend([\n",
    "#     [250, 300, 320],\n",
    "#     [300, 310, 320],\n",
    "#     [330, 340, 350],\n",
    "#     [360, 370, 380],\n",
    "#     [390, 400, 410],\n",
    "#     [420, 430, 440]\n",
    "# ])\n",
    "\n",
    "# Add all frequencies one by one\n",
    "# for i in range(250, 501, 10):\n",
    "#     freqs.append([i])\n",
    "\n",
    "# # Add groups of 2 frequencies\n",
    "# for i in range(300, 411, 20):\n",
    "#     if i + 10 <= 350:\n",
    "#         freqs.append([i, i+10])\n",
    "\n",
    "# # Add groups of 3 frequencies\n",
    "# for i in range(250, 441, 30):\n",
    "#     if i + 20 <= 590 and i + 10 <= 590:\n",
    "#         freqs.append([i, i+10, i+20])\n",
    "\n",
    "\n",
    "\n",
    "# freqs.append([250,320,330,410])\n",
    "\n",
    "# freqs = freqs_fav ## TESTING\n",
    "\n",
    "print(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_sampling(predictions):\n",
    "    \"\"\"\n",
    "    Takes a list of predictions and returns the most common value\n",
    "    Args: predictions: List of prediction values\n",
    "    Returns: Most frequent prediction value\n",
    "    \"\"\"\n",
    "    if not predictions:\n",
    "        return None\n",
    "        \n",
    "    # Count frequency of each prediction\n",
    "    freq_dict = {}\n",
    "    for pred in predictions:\n",
    "        freq_dict[pred] = freq_dict.get(pred, 0) + 1\n",
    "    \n",
    "    # Find value with highest frequency\n",
    "    max_freq = 0\n",
    "    mode = None\n",
    "    for value, freq in freq_dict.items():\n",
    "        if freq > max_freq:\n",
    "            max_freq = freq\n",
    "            mode = value\n",
    "            \n",
    "    return mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_probabilities(model, X):\n",
    "    \"\"\"\n",
    "    Perform inference on the dataset using the trained model and return predictions with probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    model: Trained model object with predict and predict_proba methods.\n",
    "    X: Dataset to perform inference on.\n",
    "\n",
    "    Returns:\n",
    "    predictions: Array of predicted class labels.\n",
    "    probabilities: Array of predicted probabilities for each class.\n",
    "    \"\"\"\n",
    "    # Predict class labels\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    probabilities = model.predict_proba(X)\n",
    "    \n",
    "    return predictions, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_with_sampling(models, X, y_true):\n",
    "    \"\"\"\n",
    "    Evaluate multiple models using inference sampling\n",
    "    \n",
    "    Args:\n",
    "        models: List of trained model objects\n",
    "        X: Features dataset\n",
    "        y_true: True labels\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "        \n",
    "    # Get predictions from each model\n",
    "    all_predictions = []\n",
    "    for model in models:\n",
    "        preds, _ = predict_with_probabilities(model, X)\n",
    "        all_predictions.append(preds)\n",
    "    \n",
    "    # Get final predictions using sampling\n",
    "    final_predictions = []\n",
    "    for i in range(len(X)):\n",
    "        sample_preds = [pred[i] for pred in all_predictions]\n",
    "        final_pred = inference_sampling(sample_preds)\n",
    "        final_predictions.append(final_pred)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, final_predictions),\n",
    "        'precision': precision_score(y_true, final_predictions, average='weighted'),\n",
    "        'recall': recall_score(y_true, final_predictions, average='weighted'),\n",
    "        'f1': f1_score(y_true, final_predictions, average='weighted')\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the data prepared from the directory\n",
    "# input_path_train_data = 'data/experiment_5_plastics/processed/'\n",
    "# input_path_new_sample = 'data/experiment_5_plastics/processed/new_sample/'\n",
    "\n",
    "# window_interval = 0.1\n",
    "# test_percentage = 0.2\n",
    "\n",
    "# df_train, df_test, dummy_labels = prepare_train_test_data(notebook_dir, input_path_train_data, test_percentage=0.2, time_window_s= window_interval, stabilised_time_s=12, split = True, verbose=True)\n",
    "# print(f\"df_train: {df_train['Sample'].unique()}\")\n",
    "\n",
    "# df_new_sample, dummy_labels = prepare_train_test_data(notebook_dir, input_path_new_sample, test_percentage=0.2, time_window_s= window_interval, stabilised_time_s=12, split = False, verbose=True)\n",
    "# print(f\"df_new_sample: {df_new_sample['Sample'].unique()}\")\n",
    "\n",
    "# df_train = df_train.dropna()\n",
    "# df_test = df_test.dropna()\n",
    "# df_new_sample_2 = df_new_sample.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.normpath(os.path.join(notebook_dir, '..', '..', 'results/exp_5/results_WS_05.csv'))\n",
    "\n",
    "input_path_train_data = 'data/experiment_5_plastics/processed/'\n",
    "input_path_new_sample = 'data/experiment_5_plastics/processed/new_sample/'\n",
    "\n",
    "# Training options\n",
    "drop_sample = True\n",
    "eliminate_std_dev = False\n",
    "eliminate_LG = False\n",
    "HG_diff = False\n",
    "LG_diff = False\n",
    "apply_scaling = False\n",
    "apply_savitzky_golay = True\n",
    "apply_pca = False\n",
    "apply_lda = False \n",
    "apply_qda = False\n",
    "apply_ica = False\n",
    "\n",
    "print(f'Training')\n",
    "\n",
    "# window_intervals = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "window_intervals = [0.1]\n",
    "\n",
    "labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'O']\n",
    "test_percentage = 0.2\n",
    "\n",
    "for window_interval in window_intervals:\n",
    "    print(f'Window interval: {window_interval}')\n",
    "    df_train, df_test, labels = prepare_train_test_data(notebook_dir, input_dir=input_path_train_data, test_percentage=test_percentage, time_window_s=window_interval, stabilised_time_s=12, split = True, verbose=True)\n",
    "    df_new_sample, labels = prepare_train_test_data(notebook_dir, input_dir=input_path_new_sample, test_percentage=test_percentage, time_window_s=window_interval, stabilised_time_s=12, split = False, verbose=True)\n",
    "    \n",
    "    # Drop NaN values\n",
    "    df_train = df_train.dropna()\n",
    "    df_test = df_test.dropna()\n",
    "    df_new_sample = df_new_sample.dropna()\n",
    "\n",
    "    for freq in freqs:\n",
    "\n",
    "        subset_freqs = freq\n",
    "        print(f'Frequency: {freq}')\n",
    "        \n",
    "        X_train, y_train = preprocess_data(df_train, labels, subset_freqs, eliminate_std_dev, eliminate_LG, drop_sample=True) \n",
    "        X_test, y_test = preprocess_data(df_test, labels, subset_freqs, eliminate_std_dev, eliminate_LG, drop_sample=True)\n",
    "\n",
    "        X_train = add_features(X_train, y_train, subset_freqs, HG_diff, LG_diff)\n",
    "        X_test = add_features(X_test, y_test, subset_freqs, HG_diff, LG_diff)\n",
    "\n",
    "        # Apply standard scaling\n",
    "        if apply_scaling:\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "        if apply_savitzky_golay:\n",
    "            X_train = savgol_filter(X_train, window_length=3, polyorder=2)\n",
    "            X_test = savgol_filter(X_test, window_length=3, polyorder=2)\n",
    "\n",
    "        if apply_pca:\n",
    "            pca = PCA(n_components=0.95)  \n",
    "            X_train = pca.fit_transform(X_train)\n",
    "            X_test = pca.transform(X_test)\n",
    "\n",
    "        if apply_lda:\n",
    "            lda = LinearDiscriminantAnalysis()\n",
    "            X_train = lda.fit_transform(X_train, y_train)\n",
    "            X_test = lda.transform(X_test)\n",
    "\n",
    "        if apply_qda:\n",
    "            qda = QuadraticDiscriminantAnalysis()\n",
    "            qda.fit(X_train, y_train)\n",
    "            # Get probability estimates\n",
    "            X_train_qda = qda.predict_proba(X_train)\n",
    "            X_test_qda = qda.predict_proba(X_test)\n",
    "            \n",
    "            # To combine with original features\n",
    "            X_train = np.hstack((X_train, X_train_qda))\n",
    "            X_test = np.hstack((X_test, X_test_qda))\n",
    "\n",
    "\n",
    "        if apply_ica:\n",
    "            ica = FastICA(n_components=2)\n",
    "            X_train = ica.fit_transform(X_train)\n",
    "            X_test = ica.transform(X_test)\n",
    "\n",
    "        # Define model names\n",
    "        models = ['RF', 'NB', 'LR', 'GB', 'SVM']\n",
    "\n",
    "        # Train the models\n",
    "        rf_model, nb_model, lr_model, gb_model, svm_model, training_times = train_models(X_train, y_train, seed)\n",
    "\n",
    "        # Feature Importance Extraction \n",
    "        try:\n",
    "            rf_imp, lr_imp, gb_imp, nb_imp, svm_imp = get_feature_importances(\n",
    "                rf_model, lr_model, gb_model, nb_model, svm_model, \n",
    "                X_train, y_train, seed, False, 20  # plot=True, n=10\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while extracting feature importances: {e}\")\n",
    "\n",
    "        # Make Predictions\n",
    "        y_pred = []\n",
    "        inference_times = []\n",
    "\n",
    "        for model in [rf_model, \n",
    "                    nb_model, \n",
    "                    lr_model, \n",
    "                    gb_model, \n",
    "                    svm_model]:\n",
    "            start_time = time.time()\n",
    "            predictions = model.predict(X_test)\n",
    "            total_time = time.time() - start_time\n",
    "            inference_times.append(total_time / len(X_test))  # Time per sample\n",
    "            y_pred.append(predictions)\n",
    "        \n",
    "            \n",
    "        # Evaluate Models\n",
    "        accuracies, precisions, recalls, f1_scores, aic_scores, bic_scores, predictions = evaluate_models(y_test, y_pred, verbose=0)\n",
    "\n",
    "        updated_results = save_model_results(\n",
    "            models=models,\n",
    "            data='test',\n",
    "            time_window_s=window_interval,\n",
    "            accuracies=accuracies,\n",
    "            precisions=precisions,\n",
    "            recalls=recalls,\n",
    "            f1_scores=f1_scores,\n",
    "            aic_scores=aic_scores,\n",
    "            bic_scores=bic_scores,\n",
    "            predictions=predictions,\n",
    "            n_features=X_train.shape[1],\n",
    "            freqs=subset_freqs,\n",
    "            csv_path=csv_path,\n",
    "            training_times=training_times,\n",
    "            inference_times=inference_times\n",
    "        )\n",
    "\n",
    "        print(f'df_new_sample[Sample]: {df_new_sample['Sample'].unique()}')\n",
    "        X_new_sample, y_new_sample = preprocess_data(df_new_sample, labels, subset_freqs, eliminate_std_dev, eliminate_LG, drop_sample=True)\n",
    "        print(f'y_new_sample: {y_new_sample.unique()}')\n",
    "\n",
    "        X_new_sample = add_features(X_new_sample, y_new_sample, subset_freqs, HG_diff, LG_diff)\n",
    "\n",
    "\n",
    "        if apply_scaling:\n",
    "            X_new_sample = scaler.transform(X_new_sample)\n",
    "\n",
    "        if apply_savitzky_golay:\n",
    "            X_new_sample = savgol_filter(X_new_sample, window_length=3, polyorder = 2)\n",
    "\n",
    "        if apply_pca:\n",
    "            X_new_sample = pca.transform(X_new_sample)\n",
    "\n",
    "        if apply_qda:\n",
    "            X_new_sample_qda = qda.predict_proba(X_new_sample)\n",
    "            X_new_sample = np.hstack((X_new_sample, X_new_sample_qda))\n",
    "                                    \n",
    "        if apply_lda:\n",
    "            X_new_sample = lda.transform(X_new_sample)\n",
    "\n",
    "        if apply_ica:\n",
    "            X_new_sample = ica.transform(X_new_sample)\n",
    "\n",
    "        # Make Predictions\n",
    "        y_pred_ns = []\n",
    "        inference_times_ns = []\n",
    "        for model in [rf_model, \n",
    "                        nb_model, \n",
    "                        lr_model, \n",
    "                        gb_model, \n",
    "                        svm_model]:\n",
    "                start_time = time.time()\n",
    "                predictions = model.predict(X_new_sample)\n",
    "                total_time = time.time() - start_time\n",
    "                inference_times_ns.append(total_time / len(X_new_sample))  \n",
    "                y_pred_ns.append(predictions)\n",
    "        \n",
    "\n",
    "        # Evaluate Models\n",
    "        accuracies, precisions, recalls, f1_scores, aic_scores, bic_scores, predictions = evaluate_models(y_new_sample, y_pred_ns, verbose=0)\n",
    "\n",
    "        # Plot Confusion Matrix\n",
    "        save_path = os.path.normpath(os.path.join(notebook_dir, '..', '..', 'results/conf_matrix/'))\n",
    "\n",
    "        for i, accuracy in enumerate(accuracies):\n",
    "            if accuracy >= 0.8:\n",
    "                model_name = f\"{models[i]}_{freq}\"\n",
    "                # plot_confusion_matrix(y_new_sample, y_pred_ns[i], np.unique(y_train))\n",
    "                plot_confusion_matrix_pdf(y_new_sample, y_pred_ns[i], np.unique(y_train), save_path, model_name)\n",
    "\n",
    "        updated_results = save_model_results(\n",
    "            models=models,\n",
    "            data='new_sample',\n",
    "            time_window_s=window_interval,\n",
    "            accuracies=accuracies,\n",
    "            precisions=precisions,\n",
    "            recalls=recalls,\n",
    "            f1_scores=f1_scores,\n",
    "            aic_scores=aic_scores,\n",
    "            bic_scores=bic_scores,\n",
    "            predictions=predictions,\n",
    "            n_features=X_train.shape[1],\n",
    "            freqs=subset_freqs,\n",
    "            csv_path=csv_path,\n",
    "            training_times=training_times,\n",
    "            inference_times=inference_times_ns\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_color_brightness(color, factor=0.6):\n",
    "    return tuple([min(1, max(0, c * factor)) for c in color])\n",
    "\n",
    "def plot_pca_3d(X_train_pca, y_pca, elev=30, azim=-45, show_labels=True):\n",
    "    \"\"\"\n",
    "    Plot PCA results in 3D, 2D, or 1D depending on the number of components.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train_pca : array\n",
    "        PCA-transformed data\n",
    "    y_pca : array\n",
    "        Labels for each point\n",
    "    elev : float, default=30\n",
    "        Elevation angle for 3D plot viewing\n",
    "    azim : float, default=-45\n",
    "        Azimuth angle for 3D plot viewing\n",
    "    show_labels : bool, default=True\n",
    "        Whether to show labels at the centroid of each class\n",
    "    \"\"\"\n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y_pca)\n",
    "    unique_classes = np.unique(y_encoded)\n",
    "    n_classes = len(unique_classes)\n",
    "    \n",
    "    # Define color palette using Seaborn's husl palette\n",
    "    base_palette = sns.color_palette(\"husl\", n_classes)\n",
    "    \n",
    "    # Create a mapping of labels to colors\n",
    "    color_dict = {}\n",
    "    for i, label in enumerate(le.classes_):\n",
    "        color_dict[label] = base_palette[i]\n",
    "    \n",
    "    # Get number of components\n",
    "    n_components = X_train_pca.shape[1]\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    if n_components >= 3:\n",
    "        # 3D plot\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        # Set the viewing angle\n",
    "        ax.view_init(elev=elev, azim=azim)\n",
    "        \n",
    "        # Plot each class separately\n",
    "        for i, class_name in enumerate(le.classes_):\n",
    "            mask = y_pca == class_name\n",
    "            color = base_palette[i]\n",
    "            \n",
    "            # Scatter points\n",
    "            ax.scatter(X_train_pca[mask, 0], \n",
    "                      X_train_pca[mask, 1],\n",
    "                      X_train_pca[mask, 2],\n",
    "                      color=color,\n",
    "                      edgecolor=adjust_color_brightness(color, 0.6),\n",
    "                      label=class_name,\n",
    "                      s=25,\n",
    "                      alpha=0.6)\n",
    "            \n",
    "            # Add labels if requested\n",
    "            if show_labels:\n",
    "                # Calculate centroid\n",
    "                centroid = np.mean(X_train_pca[mask], axis=0)\n",
    "                \n",
    "                # Use a fixed offset based on viewing angle for label positioning\n",
    "                offset_factor = 0.2  # Adjust this factor to increase/decrease offset\n",
    "                \n",
    "                # Calculate offset based on data range\n",
    "                data_range = np.ptp(X_train_pca, axis=0)\n",
    "                \n",
    "                # Compute an offset that considers the viewing angle\n",
    "                sin_azim = np.sin(np.radians(azim))\n",
    "                cos_azim = np.cos(np.radians(azim))\n",
    "                sin_elev = np.sin(np.radians(elev))\n",
    "                cos_elev = np.cos(np.radians(elev))\n",
    "                \n",
    "                # Calculate offset vector\n",
    "                offset_x = offset_factor * data_range[0] * sin_azim\n",
    "                offset_y = offset_factor * data_range[1] * cos_azim\n",
    "                offset_z = offset_factor * data_range[2] * cos_elev\n",
    "                \n",
    "                # Position for label\n",
    "                label_pos = (centroid[0] + offset_x,\n",
    "                             centroid[1] + offset_y,\n",
    "                             centroid[2] + offset_z)\n",
    "                \n",
    "                # Draw an arrow from the label to the centroid\n",
    "                ax.plot([label_pos[0], centroid[0]],\n",
    "                       [label_pos[1], centroid[1]],\n",
    "                       [label_pos[2], centroid[2]],\n",
    "                       'k-', linewidth=1, alpha=0.6)\n",
    "                \n",
    "                # Add the label\n",
    "                ax.text(label_pos[0], label_pos[1], label_pos[2],\n",
    "                       class_name,\n",
    "                       fontsize=10,\n",
    "                       fontweight='bold',\n",
    "                       color='black',\n",
    "                       ha='center',\n",
    "                       va='center',\n",
    "                       bbox=dict(facecolor='white', \n",
    "                                alpha=0.9, \n",
    "                                boxstyle='round,pad=0.3',\n",
    "                                edgecolor='gray'))\n",
    "        \n",
    "        ax.set_xlabel('PC1')\n",
    "        ax.set_ylabel('PC2')\n",
    "        ax.set_zlabel('PC3')\n",
    "        \n",
    "        # Add grid for better depth perception\n",
    "        ax.grid(True)\n",
    "        \n",
    "    elif n_components == 2:\n",
    "        # 2D plot\n",
    "        ax = fig.add_subplot(111)\n",
    "        \n",
    "        # Plot each class separately\n",
    "        for i, class_name in enumerate(le.classes_):\n",
    "            mask = y_pca == class_name\n",
    "            color = base_palette[i]\n",
    "            \n",
    "            # Scatter points\n",
    "            ax.scatter(X_train_pca[mask, 0],\n",
    "                      X_train_pca[mask, 1],\n",
    "                      color=color,\n",
    "                      edgecolor=adjust_color_brightness(color, 0.6),\n",
    "                      label=class_name,\n",
    "                      s=25, \n",
    "                      alpha=0.6)\n",
    "        \n",
    "        # Add labels\n",
    "        if show_labels:\n",
    "            for i, class_name in enumerate(le.classes_):\n",
    "                mask = y_pca == class_name\n",
    "                color = base_palette[i]\n",
    "                \n",
    "                # Calculate centroid of this class\n",
    "                centroid = np.mean(X_train_pca[mask], axis=0)\n",
    "                \n",
    "                # Create a marker at centroid location that will be behind the text\n",
    "                ax.scatter(centroid[0], centroid[1],\n",
    "                          color='white',\n",
    "                          s=250,\n",
    "                          alpha=0.8,\n",
    "                          edgecolor='gray',\n",
    "                          zorder=5)  # Higher zorder to ensure it's drawn on top\n",
    "                \n",
    "                # Add text label at centroid\n",
    "                ax.text(centroid[0], centroid[1], \n",
    "                        class_name, \n",
    "                        fontsize=12, \n",
    "                        fontweight='bold', \n",
    "                        color='black',\n",
    "                        ha='center', va='center',\n",
    "                        bbox=dict(facecolor='white', alpha=0.9, boxstyle='round,pad=0.3', \n",
    "                                  edgecolor='gray'),\n",
    "                        zorder=10)  # Even higher zorder for text\n",
    "            \n",
    "        ax.set_xlabel('PC1')\n",
    "        ax.set_ylabel('PC2')\n",
    "        \n",
    "    else:\n",
    "        # 1D plot\n",
    "        ax = fig.add_subplot(111)\n",
    "        \n",
    "        # Plot each class separately\n",
    "        for i, class_name in enumerate(le.classes_):\n",
    "            mask = y_pca == class_name\n",
    "            color = base_palette[i]\n",
    "            \n",
    "            # Scatter points\n",
    "            ax.scatter(X_train_pca[mask, 0],\n",
    "                      np.zeros_like(X_train_pca[mask, 0]),\n",
    "                      color=color,\n",
    "                      edgecolor=adjust_color_brightness(color, 0.6),\n",
    "                      label=class_name,\n",
    "                      s=25, #dot size\n",
    "                      alpha=0.6)\n",
    "            \n",
    "            # Add label if requested\n",
    "            if show_labels:\n",
    "                # Calculate centroid of this class\n",
    "                centroid = np.mean(X_train_pca[mask], axis=0)\n",
    "                \n",
    "                # Add text label at centroid\n",
    "                ax.text(centroid[0], 0.05, \n",
    "                        class_name, \n",
    "                        fontsize=12, \n",
    "                        fontweight='bold', \n",
    "                        color='black',\n",
    "                        ha='center', va='center',\n",
    "                        bbox=dict(facecolor='white', alpha=0.9, boxstyle='round,pad=0.3',\n",
    "                                 edgecolor='gray'))\n",
    "            \n",
    "        ax.set_xlabel('PC1')\n",
    "        plt.yticks([])\n",
    "    \n",
    "    # Add legend with class names\n",
    "    plt.legend(loc='best', markerscale=1.5)\n",
    "    \n",
    "    # Add explained variance ratio to title if available\n",
    "    if hasattr(pca, 'explained_variance_ratio_'):\n",
    "        if n_components >= 3:\n",
    "            variance_text = f'PC1: {pca.explained_variance_ratio_[0]:.2%}, PC2: {pca.explained_variance_ratio_[1]:.2%}, PC3: {pca.explained_variance_ratio_[2]:.2%}'\n",
    "        elif n_components == 2:\n",
    "            variance_text = f'PC1: {pca.explained_variance_ratio_[0]:.2%}, PC2: {pca.explained_variance_ratio_[1]:.2%}'\n",
    "        else:\n",
    "            variance_text = f'PC1: {pca.explained_variance_ratio_[0]:.2%}'\n",
    "        plt.title(f'PCA Visualization\\n{variance_text}')\n",
    "    else:\n",
    "        plt.title('PCA Visualization')\n",
    "\n",
    "    # Save figure with angle and label information in filename\n",
    "    filepath = os.path.normpath(os.path.join(notebook_dir, '..', '..', 'results', 'pca_models'))\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath, exist_ok=True)\n",
    "        \n",
    "    freqs_code = '_'.join(map(str, subset_freqs))\n",
    "    labels_suffix = \"_labeled\" if show_labels else \"\"\n",
    "\n",
    "    # Use os.path.join for proper path construction\n",
    "    filename = f\"pca_viz_{freqs_code}_elev{elev}_azim{azim}{labels_suffix}.pdf\"\n",
    "    full_path = os.path.join(filepath, filename)\n",
    "\n",
    "    plt.savefig(full_path, bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "# PCA 3D Visualization\n",
    "# Generate multiple views of the same PCA result\n",
    "for freq in freqs:\n",
    "    subset_freqs = freq\n",
    "    print(f'Frequency: {freq}')\n",
    "\n",
    "    X_train, y_train = preprocess_data(df_train, labels, subset_freqs, eliminate_std_dev, eliminate_LG, drop_sample=True) \n",
    "    X_test, y_test = preprocess_data(df_test, labels, subset_freqs, eliminate_std_dev, eliminate_LG, drop_sample=True)\n",
    "\n",
    "    X_train = add_features(X_train, y_train, subset_freqs, HG_diff, LG_diff)\n",
    "    X_test = add_features(X_test, y_test, subset_freqs, HG_diff, LG_diff)\n",
    "\n",
    "    pca = PCA(n_components=0.95)  \n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    # Print all PCA component percentages\n",
    "    print(\"Explained variance by PCA components:\")\n",
    "    for i, var in enumerate(pca.explained_variance_ratio_):\n",
    "        print(f\"PC{i+1}: {var*100:.2f}%\")\n",
    "    print(f\"Total variance explained: {sum(pca.explained_variance_ratio_)*100:.2f}%\")\n",
    "    print(f\"Number of components: {len(pca.explained_variance_ratio_)}\")\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    y_pca = y_train\n",
    "    \n",
    "    # # Generate different views of the same data\n",
    "    # views = [\n",
    "    #     (30, -45),     # Default view\n",
    "    #     # (0, 0),      # Front view\n",
    "    #     # (0, 90),     # Side view\n",
    "    #     # (90, 0),     # Top view\n",
    "    #     (20, -70),   \n",
    "    #     (30, -20),   \n",
    "    #     (25, -25),   \n",
    "    #     (35, -75),   \n",
    "\n",
    "    # ]\n",
    "    \n",
    "    # for elev, azim in views:\n",
    "    #     plot_pca_3d(X_train_pca, y_pca, elev=elev, azim=azim, show_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Visualization (Frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_color_brightness(color, factor=0.6):\n",
    "    return tuple([min(1, max(0, c * factor)) for c in color])\n",
    "\n",
    "def plot_3d_specific_frequencies(df, freq_names, elev=30, azim=-45):\n",
    "    \"\"\"\n",
    "    Plot 3D visualization using 3 selected frequencies directly from the dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        DataFrame containing the frequency data\n",
    "    freq_names : list\n",
    "        List of 3 column names to visualize (e.g., ['340.0 HG (mV)', '350.0 HG (mV)', '360.0 HG (mV)'])\n",
    "    elev : float, default=30\n",
    "        Elevation angle for 3D plot viewing\n",
    "    azim : float, default=-45\n",
    "        Azimuth angle for 3D plot viewing\n",
    "    \"\"\"\n",
    "    # Check if we have 3 frequencies\n",
    "    if len(freq_names) != 3:\n",
    "        raise ValueError(\"Need exactly 3 frequency names for 3D plot\")\n",
    "    \n",
    "    # Check if frequencies exist in dataframe\n",
    "    for freq in freq_names:\n",
    "        if freq not in df.columns:\n",
    "            raise ValueError(f\"Frequency column '{freq}' not found in dataframe\")\n",
    "    \n",
    "    # Extract data for the 3 frequencies and the sample labels\n",
    "    X_3d = df[freq_names].values\n",
    "    y = df['Sample'].values\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    unique_classes = np.unique(y_encoded)\n",
    "    n_classes = len(unique_classes)\n",
    "    \n",
    "    # Define color palette using Seaborn's husl palette\n",
    "    base_palette = sns.color_palette(\"husl\", n_classes)\n",
    "    \n",
    "    # Create a mapping of labels to colors\n",
    "    color_dict = {}\n",
    "    for i, label in enumerate(le.classes_):\n",
    "        color_dict[label] = base_palette[i]\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Set the viewing angle\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    \n",
    "    # Plot each class separately for better legend\n",
    "    for i, class_name in enumerate(le.classes_):\n",
    "        mask = y == class_name\n",
    "        color = base_palette[i]\n",
    "        ax.scatter(X_3d[mask, 0], \n",
    "                  X_3d[mask, 1],\n",
    "                  X_3d[mask, 2],\n",
    "                  color=color,\n",
    "                  edgecolor=adjust_color_brightness(color, 0.6),\n",
    "                  label=class_name,\n",
    "                  s=25,  # dot size\n",
    "                  alpha=0.6)\n",
    "    \n",
    "    # Extract frequency values from column names for axis labels\n",
    "    freq_values = []\n",
    "    for name in freq_names:\n",
    "        # Extract the numeric part from strings like \"340.0 HG (mV)\"\n",
    "        match = re.search(r'(\\d+\\.?\\d*)', name)\n",
    "        if match:\n",
    "            freq_values.append(match.group(1))\n",
    "        else:\n",
    "            freq_values.append(name)\n",
    "    \n",
    "    # Set axis labels with frequency values\n",
    "    ax.set_xlabel(f'{freq_names[0]}')\n",
    "    ax.set_ylabel(f'{freq_names[1]}')\n",
    "    ax.set_zlabel(f'{freq_names[2]}')\n",
    "    \n",
    "    # Add grid for better depth perception\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Add legend with class names\n",
    "    plt.legend(loc='best', markerscale=1.5)\n",
    "    \n",
    "    # Set title\n",
    "    plt.title(f'3D Visualization of Selected Frequencies\\n{freq_values[0]}, {freq_values[1]}, {freq_values[2]} GHz')\n",
    "\n",
    "    # Save figure with angle and frequency information in filename\n",
    "    filepath = os.path.normpath(os.path.join(notebook_dir, '..', '..', 'results/freq_viz/'))\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "        \n",
    "    # Create a cleaner filename\n",
    "    clean_freqs = [re.sub(r'[^\\d.]', '', f) for f in freq_values]\n",
    "    freqs_code = '_'.join(clean_freqs)\n",
    "    plt.savefig(f'{filepath}/freq_viz_{freqs_code}_elev{elev}_azim{azim}.pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return ax\n",
    "\n",
    "import re  # Import regex for extracting frequency values\n",
    "\n",
    "# Define specific target frequencies to visualize\n",
    "target_freqs = [340.0, 350.0, 360.0]  # Adjust to your desired frequencies\n",
    "\n",
    "# Find matching columns in the original dataframe\n",
    "hg_columns = []\n",
    "for target_freq in target_freqs:\n",
    "    # Find columns that match this frequency \n",
    "    for col in df_train.columns:\n",
    "        if f\"{target_freq} HG (mV)\" in col:\n",
    "            hg_columns.append(col)\n",
    "            break\n",
    "\n",
    "if len(hg_columns) == 3:\n",
    "    # Use the original dataframe with the identified columns\n",
    "    df_viz = df_train.copy()\n",
    "    \n",
    "    # Apply preprocessing to these specific columns\n",
    "    if apply_savitzky_golay:\n",
    "        for col in hg_columns:\n",
    "            df_viz[col] = savgol_filter(df_viz[col].values, \n",
    "                                      window_length=5, polyorder=3)\n",
    "        \n",
    "    # Generate different viewing angles\n",
    "    views = [\n",
    "        (30, -45),   # Default view\n",
    "        (20, -70),   \n",
    "        (30, -20),   \n",
    "        (10, -120),  \n",
    "    ]\n",
    "    \n",
    "    for elev, azim in views:\n",
    "        plot_3d_specific_frequencies(df_viz, hg_columns, elev=elev, azim=azim)\n",
    "else:\n",
    "    print(f\"Could not find all required frequency columns. Found: {hg_columns}\")\n",
    "    print(f\"Available columns: {[col for col in df_train.columns if 'HG (mV)' in col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_freqs = list(range(100, 600, 10))\n",
    "\n",
    "X_train, y_train = preprocess_data(df_train, labels, subset_freqs, eliminate_std_dev, eliminate_LG, drop_sample=True)\n",
    "X_train = add_features(X_train, y_train, subset_freqs, HG_diff, LG_diff)\n",
    "\n",
    "## NON PCA VISUALIZATION ##\n",
    "# Choose specific variables for visualization\n",
    "var1 = '410.0 HG (mV)'\n",
    "var2 = '360.0 HG (mV)'\n",
    "\n",
    "try:\n",
    "    X_train[var1].describe()\n",
    "except Exception as e:\n",
    "    var1 = f'{var1} mean'\n",
    "    var2 = f'{var2} mean'\n",
    "\n",
    "def plot_data_visualization(X_train, y_train, var1, var2):\n",
    "    \n",
    "    # Get unique classes and encode\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y_train)\n",
    "    n_classes = len(np.unique(y_encoded))\n",
    "    \n",
    "    # Create custom colormap with only needed colors\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, 20))  # Get all 20 colors\n",
    "    colors = colors[:n_classes]  # Take only needed colors\n",
    "    custom_cmap = plt.cm.colors.ListedColormap(colors)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_train[var1], \n",
    "                         X_train[var2],\n",
    "                         c=y_encoded,\n",
    "                         cmap=custom_cmap,\n",
    "                         edgecolor='k', \n",
    "                         s=25)\n",
    "    \n",
    "    # Create custom legend\n",
    "    legend_elements = [plt.Line2D([0], [0], \n",
    "                                marker='o', \n",
    "                                color='w',\n",
    "                                markerfacecolor=colors[i], \n",
    "                                label=class_name,\n",
    "                                markersize=10) \n",
    "                      for i, class_name in enumerate(le.classes_)]\n",
    "    \n",
    "    plt.legend(handles=legend_elements, title=\"Classes\")\n",
    "    plt.xlabel(var1)\n",
    "    plt.ylabel(var2)\n",
    "    plt.show()\n",
    "\n",
    "plot_data_visualization(X_train, y_train, var1, var2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
